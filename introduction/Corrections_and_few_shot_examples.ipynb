{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b8c61a-94eb-4f13-83ec-0f6db5a4822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Env\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true' # enables tracing \n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"xxx\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-feedback-and-few-shot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839e984-5c01-49da-ad10-80ae00bace2a",
   "metadata": {},
   "source": [
    "### Creating a RAG bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd922eb-0407-4de4-81d7-0e3e789c0b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "### Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1839bd-f291-43ed-8033-0aaa707f6c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ReAct agent, developed by Yao et al. (2023), integrates reasoning and acting within a large language model (LLM) by extending the action space to '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RAG bot\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "class RagBot:\n",
    "\n",
    "    def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question, docs):\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant for question answering.\"\n",
    "                    \" Use the following docs to answer the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{docs}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc.page_content) for doc in docs],\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "rag_bot = RagBot(retriever)\n",
    "response = rag_bot.get_answer(\"How does ReAct agent work?\")\n",
    "response[\"answer\"][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7b382-5e9f-43b3-8040-f8b9eef41618",
   "metadata": {},
   "source": [
    "### Setting an online evaluator\n",
    "\n",
    "Now, we see traces logged to our project.\n",
    "\n",
    "We can add an evaluator to our project.\n",
    "\n",
    "Let's do document grading, which is a very useful check! \n",
    "\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb\n",
    "\n",
    "Grade documents:\n",
    "\n",
    "https://docs.smith.langchain.com/tutorials/Developers/rag#evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5df95a-6570-49f0-b980-904c5dbdad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_bot.get_answer(\"How does ReAct agent work?\")\n",
    "response = rag_bot.get_answer(\"What is the difference between ReAct and Reflexion approaches for self-reflection?\")\n",
    "response = rag_bot.get_answer(\"What is the Memory and Retrieval model in Generative Agents simulation?\")\n",
    "response = rag_bot.get_answer(\"What are the types of LLM memory?\")\n",
    "response = rag_bot.get_answer(\"What is the Memory and Retrieval model in Generative Agents simulation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebeeb00-50a2-4d2d-8894-0fb73e1b91c5",
   "metadata": {},
   "source": [
    "### Attaching evaluators to a dataset\n",
    "\n",
    "Now, let's build a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a6b960-a03d-432b-99ef-fa07774e0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    (\n",
    "        \"How does ReAct agent work? \",\n",
    "        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What are the types of biases that can arise with few-shot prompting?\",\n",
    "        \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What are five types of adversarial attacks?\",\n",
    "        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is the difference between ReAct and Reflexion approaches for self-reflection?\",\n",
    "        \"Reflexion extends ReAct with a standard RL setup: it computed a reward and passes this to a heuristic function, which determines when the trajectory is inefficient or contains hallucination and should be stopped. It reflects on this output and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What are the mappings of human memory into LLMs?\", \n",
    "        \"Sensory memory is learning embedding representations for raw inputs. Short-term memory is in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer. Long-term memory is the external vector store that the agent can attend to at query time, accessible via fast retrieval.\"),\n",
    "]\n",
    "\n",
    "# Save it\n",
    "dataset_name = \"Agent QA\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"question\": text}, {\"answer\": label}) for text, label in examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a55d04-fe89-4343-8d86-aa1a82057681",
   "metadata": {},
   "source": [
    "Here's a function to call our RAG bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae69b7ae-9bc5-41cc-8e9f-4b80f3586b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    # Here, I can re-set the output keys to use in evaluation, if I want\n",
    "    return {\"response\": response[\"answer\"], \"documents\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635e701-3f84-4e78-8a49-7a383b5685d4",
   "metadata": {},
   "source": [
    "Here's an evaluator function for answers relative to our references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1a9d817-bd9b-46ee-b680-b89c0ddef140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the question, the ground truth reference answer, RAG chain answer prediction\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "\n",
    "    # Here, a reference the key set in predict_rag_answer\n",
    "    prediction = run.outputs[\"response\"]\n",
    "\n",
    "    # Define an LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": input_question,\n",
    "            \"correct_answer\": reference,\n",
    "            \"student_answer\": prediction,\n",
    "        }\n",
    "    )\n",
    "    score = score[\"Score\"]\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27729f57-79de-498d-a999-23e956da986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAG-agentQA-gpt-4o-answer-45f862b3' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/08941386-706a-4a9f-9567-b1f1d7242101/compare?selectedSessions=3b412956-df7c-461c-b2df-d0ed04c57ddc\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9d33d017eb4795af58100d9126dae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "model_tested = \"gpt-4o\"\n",
    "experiment_prefix = f\"RAG-agentQA-{model_tested}\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=experiment_prefix + \"-answer\",\n",
    "    num_repetitions=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b6845-437f-4681-985d-20f73c212b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea60ec-fab6-4a43-865f-46971d4c66c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb69149-71f4-47ac-adbd-91b6a3cf205e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85738f-f1bd-4b9a-bc95-cd46e099dae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
