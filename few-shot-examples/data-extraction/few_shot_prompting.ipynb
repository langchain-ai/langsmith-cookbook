{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08787e1-142d-4f63-be4f-f74e2c832da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Few-Shot Prompting for Data Extraction\n",
    "\n",
    "Few-shot prompting refers to a set of techniques to control what an LLM produces by giving examples\n",
    "of what it should produce. It's typically used to adapt an LLM to a domain, coerce it to follow a certain\n",
    "style, or to improve the format of what it produces.\n",
    "\n",
    "In this notebook, we'll use LangSmith to evaluate a couple few-shot prompting techniques.\n",
    "\n",
    "The basic steps are:\n",
    "- Create a \"training\" and evaluation dataset\n",
    "- Benchmark a baseline instruction model on the eval dataset\n",
    "- Create a few-shot prompt and benchmark\n",
    "- Iterate and improve\n",
    "\n",
    "Tl;dr - we find random sampling to be the best naive method without meta-prompting or example selection conditioned\n",
    "on the result.\n",
    "\n",
    "There are some great prompt tuning resources out there. [Lilian Weng's blog](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/) is always a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc19dd8-c467-4dc0-a2b1-16346ed4864d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Datasets\n",
    "\n",
    "We've got some samples from the [REBEL](https://github.com/Babelscape/rebel/tree/main) relationship extraction dataset we will use to develop our model. First, upload them to your langsmith organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80990f3-10e3-4a40-91d7-99b9dd52d8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c06ac76-71cb-453f-af20-75926e05edbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    name = f\"Rebel-linearized-{split}\"\n",
    "    client.upload_csv(\n",
    "        f\"data/{name}.csv\",\n",
    "          input_keys=[\"context\"],\n",
    "          output_keys=[\"triplets\"],\n",
    "          name=name + \"foo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a699d7f1-3b29-411e-84ce-bec19de4957e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'The feature appears in U . S . Navy aerial photographs taken in the 1960s and in imagery obtained by the NASA Earth Resources Technology Satellite ( ERTS-1 ) , 1973â€“74 . '}\n",
      "{'triplets': '<triplet> Earth Resources Technology Satellite <subj> NASA <obj> operator'}\n"
     ]
    }
   ],
   "source": [
    "example_format = next(client.list_examples(dataset_name=\"Rebel-linearized-train\"))\n",
    "print(example_format.inputs)\n",
    "print(example_format.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af16e4e-c357-4429-becc-31b023f8b972",
   "metadata": {},
   "source": [
    "## Define an evaluator\n",
    "\n",
    "Next we'll define a custom evaluator. We want to be permutation invariant and not be super strict on the exact format of each of the tail entity values, since it's hard to get completely open information extraction systems (and humans) to agree on the \"ground truth\".\n",
    "\n",
    "We'll write a custom evaluator here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc458876-a557-4cd3-ba3f-dd5e18c93be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.evaluation import StringEvaluator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import openai_functions\n",
    "\n",
    "import json\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an impartial grader tasked with measuring the accuracy of extracted entity relations.\"),\n",
    "        (\"human\", \"Please evaluate the following data:\\n\\n\"\n",
    "         \"<INPUT>\\n{input}</INPUT>\\n\"\n",
    "         \"<PREDICTED>\\n{prediction}</PREDICTED>\\n\"\n",
    "         \"<GROUND_TRUTH>\\n{reference}</GROUND_TRUTH>\\n\\n\"\n",
    "         \"Please save your reasoning and grading by calling the commit_grade function.\"\n",
    "         \" First, enumerate all factual discrepancies in the predicted triplets relative to the ground truth.\"\n",
    "         \" Finally, score the prediction on a scale out of 100, taking into account factuality and\"\n",
    "         \" correctness according to the ground truth.\"),\n",
    "         \n",
    "    ]\n",
    ")\n",
    "\n",
    "commit_grade_schema = {\n",
    "    \"name\": \"commit_grade\",\n",
    "    \"description\": \"Commits a grade with reasoning.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"commit_grade_parameters\",\n",
    "        \"description\": \"Parameters for the commit_grade function.\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"mistakes\": {\n",
    "                \"title\": \"discrepancies\",\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any discrepencies between the predicted and ground truth.\"\n",
    "            },\n",
    "            \"reasoning\": {\n",
    "                \"title\": \"reasoning\",\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The explanation or logic behind the final grade.\"\n",
    "            },\n",
    "            \"grade\": {\n",
    "                \"title\": \"grade\",\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The numerical value representing the grade.\",\n",
    "                \"minimum\": 0,\n",
    "                \"maximum\": 100\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"reasoning\", \"grade\", \"mistakes\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_grade(func_args: str) -> dict:\n",
    "    args = json.loads(func_args)\n",
    "    return {\n",
    "        \"reasoning\": (args.get(\"reasoning\", \"\") + \"\\n\\n\" + args.get(\"discrepancies\", \"\")).strip(),\n",
    "        \"score\": args.get(\"grade\", 0) / 100,\n",
    "    }\n",
    "\n",
    "eval_chain = (\n",
    "    eval_prompt \n",
    "    | ChatOpenAI(model=\"gpt-4\", temperature=0).bind(functions=[commit_grade_schema])\n",
    "    | openai_functions.OutputFunctionsParser() \n",
    "    | normalize_grade                             \n",
    ")\n",
    "\n",
    "class EvaluateTriplets(StringEvaluator):\n",
    "    \"\"\"Evaluate the triplets of a predicted string.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        *,\n",
    "        prediction: str,\n",
    "        reference: Optional[str] = None,\n",
    "        input: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        callbacks = kwargs.pop(\"callbacks\", None)\n",
    "        return eval_chain.invoke(\n",
    "            {\"prediction\": prediction, \"reference\": reference, \"input\": input}, \n",
    "            {\"callbacks\": callbacks},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd17ef6-62a5-46f7-b591-4b507070ff23",
   "metadata": {},
   "source": [
    "#### Define Baseline Chain\n",
    "\n",
    "We first want to get a baseline measurement of an extractor that only relies on instructions.\n",
    "We do so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8228fbb-775a-491e-bfa9-1b8807720e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ebb788-4506-4496-b0d8-458f2266d11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will focus on an instructional prompt based on the format\n",
    "# description of the dataset.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an autoregressive information extraction agent.\"),\n",
    "        (\"human\", \"Extract knowledge triplets from the following text:\\n\"\n",
    "        \"<TEXT>\\n{context}\\n<\\/TEXT>\"),\n",
    "        (\"system\", \"Output should be in linearized format.<triplet> marks the start of a new triplet with\"\n",
    "        \"a new head entity, followed by the surface form\"\n",
    "        \"of that entity in the input text. <subj> marks\"\n",
    "        \"the end of the head entity and the start of the tail\"\n",
    "        \"entity surface form. <obj> marks the end of the\"\n",
    "        \"tail entity and the start of the relation between the\"\n",
    "        \"head and tail entity, in its surface form\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b9ddda-0e5b-4239-8144-c04641469a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_dataset_name = \"Rebel-linearized-validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df551c3e-d808-4fe3-8cd3-6a3dafed190d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "config = RunEvalConfig(\n",
    "    custom_evaluators=[EvaluateTriplets()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dece101b-2f8b-4d10-b35b-653a094d4f92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'e9369e73b2e94d218aecf06a51af866a-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/a0d94242-89d1-42af-b851-4fa6e8a3795b?eval=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} <CIMultiDictProxy('Date': 'Fri, 18 Aug 2023 21:45:08 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7f8d48edbd8dceb5-SJC', 'alt-svc': 'h3=\":443\"; ma=86400')>.\n"
     ]
    }
   ],
   "source": [
    "_ = await client.arun_on_dataset(validation_dataset_name, chain, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10683ab1-aa5a-4cee-95c4-60830786ea9a",
   "metadata": {},
   "source": [
    "## Few-Shot Examples\n",
    "\n",
    "We'll first try with some static examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e69a4ef2-b52c-4358-9f56-cee48d8ae6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "def create_few_shot_prompt(examples):\n",
    "    formatted_examples = [{**ex.inputs, **ex.outputs} for ex in examples]\n",
    "    # This is a prompt template used to format each individual example.\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"Extract knowledge triplets from the following text:\"\n",
    "            \"\\n<TEXT>\\n{context}</TEXT>\"),\n",
    "            (\"ai\", \"{triplets}\"),\n",
    "        ]\n",
    "    )\n",
    "    return FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=formatted_examples,\n",
    "    )\n",
    "\n",
    "def create_chain_from_examples(examples):\n",
    "    few_shot_prompt = create_few_shot_prompt(examples)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an autoregressive information extraction agent.\"),\n",
    "            few_shot_prompt,\n",
    "            (\"human\", \"Extract knowledge triplets from the following text:\\n\"\n",
    "            \"<TEXT>\\n{context}\\n<\\/TEXT>\"),\n",
    "            (\"system\", \"Output should be in linearized format.<triplet> marks the start of a new triplet with\"\n",
    "        \"a new head entity, followed by the surface form\"\n",
    "        \"of that entity in the input text. <subj> marks\"\n",
    "        \"the end of the head entity and the start of the tail\"\n",
    "        \"entity surface form. <obj> marks the end of the\"\n",
    "        \"tail entity and the start of the relation between the\"\n",
    "        \"head and tail entity, in its surface form\"),\n",
    "        ]\n",
    "    )\n",
    "    return prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "402eff8c-38a2-4fc8-8b41-4b9a74795868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "train_dataset_name = \"Rebel-linearized-train\"\n",
    "\n",
    "K = 5\n",
    "examples = list(client.list_examples(dataset_name=train_dataset_name))\n",
    "\n",
    "# Randomly select K examples\n",
    "random.shuffle(examples)\n",
    "examples_head = examples[:K]\n",
    "chain_2 = create_chain_from_examples(examples_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c86aa0-8444-4e90-9777-8ac16f3c83f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'f091482618c54db4a0726b98067e9661-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/441695f7-5a8a-4011-b4ec-297a6e6fd13a?eval=true\n"
     ]
    }
   ],
   "source": [
    "chain_2_results = await client.arun_on_dataset(validation_dataset_name, chain_2, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31530d-a6b6-48ba-a347-7e8211cc1eb7",
   "metadata": {},
   "source": [
    "That increases the score by a bit. It seems to help! Would other static techniques help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61d749-74b7-49da-bf95-08e1770fd967",
   "metadata": {},
   "source": [
    "### Select hardest examples\n",
    "\n",
    "You may hypothesize that selecting the \"hardest\" examples may be a good idea, since it will help the model \"learn more\" from them.\n",
    "Hard examples carry more information, right?\n",
    "\n",
    "We don't want to unfairly skew our results by selecting examples from the dev set, so we'll first go and score the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0149bda2-ef76-4ff2-a3ac-d0a96ebc9acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '10a8199763ed4212ad05376cb90a6373-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/6a28a8bf-733f-43ab-b400-5194a87a9722?eval=true\n"
     ]
    }
   ],
   "source": [
    "# We will run this only on the training set just to score the outputs\n",
    "training_results = await client.arun_on_dataset(train_dataset_name, chain_2, evaluation=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c445ef91-d18a-45c0-aaf9-109dc0c1b3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = list(client.list_runs(\n",
    "    filter='and(eq(feedback_key, \"EvaluateTriplets\"), lt(feedback_score, 0.1))',\n",
    "    project_name=training_results[\"project_name\"]\n",
    "))\n",
    "example_ids = {r.reference_example_id for r in runs}\n",
    "# examples = [client.list_examples(example_ids=[r.reference_example_id for r in runs])]\n",
    "examples = [e for e in client.list_examples(dataset_id=client.read_example(example_id=runs[0].reference_example_id).dataset_id)\n",
    " if e.id in example_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd9c1383-a464-439b-a3ad-0a8a5397ca97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_3 = create_chain_from_examples(examples[:K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f150de81-55cb-49fb-9b26-3427821ec3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'a37aafc4a00a49c0bbf3386a2834c029-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/5c121572-a15b-48b0-8ae4-9b637fb9bd66?eval=true\n"
     ]
    }
   ],
   "source": [
    "results = await client.arun_on_dataset(validation_dataset_name, chain_3, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c9ae5-db22-4d81-8095-c348b07e4e75",
   "metadata": {},
   "source": [
    "This actually **decreased** the score. You can look at the selected examples to see why that may be the case.\n",
    "In our case, they are all labels that seem to leave out information. It's poorly labeled!\n",
    "\n",
    "### Select the \"easy\" examples\n",
    "\n",
    "You may then think to filter the other way around: select the \"easy\" examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ceb4480-c9a1-46ca-9527-fe49f8b0fbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = list(client.list_runs(\n",
    "    filter='and(eq(feedback_key, \"EvaluateTriplets\"), gte(feedback_score, 0.7))',\n",
    "    project_name=training_results[\"project_name\"]\n",
    "))\n",
    "example_ids = {r.reference_example_id for r in runs}\n",
    "# examples = [client.list_examples(example_ids=[r.reference_example_id for r in runs])]\n",
    "examples = [e for e in client.list_examples(dataset_id=client.read_example(example_id=runs[0].reference_example_id).dataset_id)\n",
    " if e.id in example_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e4384acd-eae5-4011-ba1d-eabb9918fefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_4 = create_chain_from_examples(examples[:K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76723200-e4e0-4100-a641-ca3ab4b86f79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '567387171ca943e2bce7d6f08889e768-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/d482c156-2d80-4baf-b37c-fdcdefa98755?eval=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    }
   ],
   "source": [
    "results = await client.arun_on_dataset(validation_dataset_name, chain_3, evaluation=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399002a-0efe-4f58-9458-bc46b27c0428",
   "metadata": {},
   "source": [
    "This performs even worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07f58c-f4b1-4be2-9c19-f66fc6f78124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
