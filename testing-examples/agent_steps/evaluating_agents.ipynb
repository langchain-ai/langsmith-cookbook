{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734ab45a-eac6-46bb-bed7-2eaac6ab701a",
   "metadata": {},
   "source": [
    "# Evaluating Agents' Intermediate Steps\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/testing-examples/agent_steps/evaluating_agents.ipynb)\n",
    "\n",
    "In many scenarios, evaluating an agent isn't merely about the final outcome, but about understanding the steps it took to arrive at that decision. This notebook provides an introductory walkthrough on configuring an evaluator to assess an agent based on its \"decision-making process,\" scoring based on the sequence of selected tools. \n",
    "\n",
    "[![Example Agent Trace](./img/agent_trace.png)](https://smith.langchain.com/public/6d50f517-115f-4c14-97b2-2e19b15efca7/r)\n",
    "\n",
    "We'll [create a custom run evaluator](https://docs.smith.langchain.com/evaluation/custom-evaluators#custom-evaluators-outside-langchain) that captures and compares the intermediate steps of the agent against a pre-defined sequence. This ensures that the agent isn't just providing the correct answers but is also being efficient about how it is using external resources.\n",
    "\n",
    "The basic steps are:\n",
    "\n",
    "- Prepare a dataset with input queries and expected agent actions\n",
    "- Define the agent with specific tools and behavior\n",
    "- Construct custom evaluators that check the actions taken\n",
    "- Running the evaluation\n",
    "\n",
    "Once the evaluation is completed, you can review the results in LangSmith. By the end of this guide, you'll have a better sense of how to apply an evaluator to more complex inputs like an agent's trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fdc4da-d656-47eb-9298-198a5a7a3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd1303f-a7bb-4098-bf13-598d9893d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526e117",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset\n",
    "\n",
    "Define a new dataset. At minimum, the dataset should have input queries the agent is tasked to solve.\n",
    "We will also store expected steps in our dataset to demonstrate the sequence of actions the agent is expected to\n",
    "take in order to resolve the query.\n",
    "\n",
    "Optionally, you can store reference labels to evaluate the agent's \"correctness\" in an end-to-end fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129154f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "questions = [\n",
    "    (\n",
    "        \"Why was was a $10 calculator app one of the best-rated Nintendo Switch games?\",\n",
    "        {\n",
    "            \"reference\": \"It became an internet meme due to its high price point.\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"hi\",\n",
    "        {\n",
    "            \"reference\": \"Hello, how can I assist you?\",\n",
    "            \"expected_steps\": [],  # Expect a direct response\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who is Dejan Trajkov?\",\n",
    "        {\n",
    "            \"reference\": \"Macedonian Professor, Immunologist and Physician\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who won the 2023 U23 world wresting champs (men's freestyle 92 kg)\",\n",
    "        {\n",
    "            \"reference\": \"Muhammed Gimri from turkey\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What's my first meeting on Friday?\",\n",
    "        {\n",
    "            \"reference\": 'Your first meeting is 8:30 AM for \"Team Standup\"',\n",
    "            \"expected_steps\": [\"check_calendar\"],  # Only expect calendar tool\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "uid = uuid.uuid4()\n",
    "dataset_name = f\"Agent Eval Example {uid}\"\n",
    "ds = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"An example agent evals dataset using search and calendar checks.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q[0]} for q in questions],\n",
    "    outputs=[q[1] for q in questions],\n",
    "    dataset_id=ds.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc0a4c-e9a9-490c-91af-c702e35fad08",
   "metadata": {},
   "source": [
    "## 2. Define agent\n",
    "\n",
    "The main components of an agentic program are:\n",
    "- The agent (or runnable) that accepts the query and intermediate and responds with the next action to take\n",
    "- The tools the agent has access to\n",
    "- The executor, which controls the looping behavior when choosing subsequent actions\n",
    "\n",
    "In this example, we will create an agent with access to a DuckDuckGo search client (for informational search) and a mock tool to check a user's calendar for a given date.\n",
    "\n",
    "Our agent will use OpenAI function calling to ensure it generates arguments that conform to the tool's expected input schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools import DuckDuckGoSearchResults, tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_calendar(date: str) -> list:\n",
    "    \"\"\"Check the user's calendar for a meetings on the specified datetime (in iso format).\"\"\"\n",
    "    date_time = parse(date)\n",
    "    # A placeholder to demonstrate with multiple tools.\n",
    "    # It's easy to mock tools when testing.\n",
    "    if date_time.weekday() == 4:\n",
    "        return [\n",
    "            \"8:30 : Team Standup\",\n",
    "            \"9:00 : 1 on 1\",\n",
    "            \"9:45 design review\",\n",
    "        ]\n",
    "    return [\"Focus time\"]  # If only...\n",
    "\n",
    "\n",
    "def agent_factory():\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    tools = [\n",
    "        DuckDuckGoSearchResults(\n",
    "            name=\"duck_duck_go\"\n",
    "        ),  # General internet search using DuckDuckGo\n",
    "        check_calendar,\n",
    "    ]\n",
    "    llm_with_tools = llm.bind(\n",
    "        functions=[format_tool_to_openai_function(t) for t in tools]\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant.\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    runnable_agent = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"question\"],\n",
    "            \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "                x[\"intermediate_steps\"]\n",
    "            ),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm_with_tools\n",
    "        | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "\n",
    "    return AgentExecutor(\n",
    "        agent=runnable_agent,\n",
    "        tools=tools,\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d318",
   "metadata": {},
   "source": [
    "## 3. Define evaluators\n",
    "\n",
    "We will create a custom run evaluator to check the agent trajectory.\n",
    "It compares the run's intermediate steps against the \"ground truth\" we saved in the dataset above.\n",
    "\n",
    "Review the code below. Note that this evaluator expects the agent's response to contain the \"intermediate_steps\" key containing the list of agent actions. This is done by setting `return_intermediate_steps=True` above.\n",
    "\n",
    "This also expects your dataset to have the \"expected_steps\" key in each example outputs, as done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "class AgentTrajectoryEvaluator(RunEvaluator):\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        # This is the output of each run\n",
    "        intermediate_steps = run.outputs[\"intermediate_steps\"]\n",
    "        # Since we are comparing to the tool names, we now need to get that\n",
    "        # Intermediate steps is a Tuple[AgentAction, Any]\n",
    "        # The first element is the action taken\n",
    "        # The second element is the observation from taking that action\n",
    "        trajectory = [action.tool for action, _ in intermediate_steps]\n",
    "        # This is what we uploaded to the dataset\n",
    "        expected_trajectory = example.outputs[\"expected_steps\"]\n",
    "        # Just score it based on whether it is correct or not\n",
    "        score = int(trajectory == expected_trajectory)\n",
    "        return EvaluationResult(key=\"Intermediate steps correctness\", score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568f90",
   "metadata": {},
   "source": [
    "## 4. Evaluate\n",
    "\n",
    "Add your custom evaluator to the `custom_evaluators` list in the evaluation configuration below.\n",
    "\n",
    "Since our dataset has multiple output keys, we have to instruct the `run_on_dataset` function on which key to use as the ground truth for the QA evaluator by setting `reference_key=\"reference\"` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5201d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-formal-candle-44' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/ed045f5e-61ce-45a5-b489-d3b0c695ce5f\n",
      "[------------------------------------------------->] 5/5"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[AgentTrajectoryEvaluator()],\n",
    "    # We now need to specify this because we have multiple outputs in our dataset\n",
    "    reference_key=\"reference\",\n",
    ")\n",
    "\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    tags=[\"agent-eval-example\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe01d96-bfde-4979-a1e3-c6c70759d9b7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've succesfully performed a simple evaluation of the agent's trajectory by comparing it to an expected sequence of actions. This is useful when you know the expected steps to take. \n",
    "\n",
    "Once you've configured a custom evaluator for this type of evaluation, it's easy to apply other techniques using off-the-shelf evaluators like LangChain's [TrajectoryEvalChain](https://python.langchain.com/docs/guides/evaluation/trajectory/trajectory_eval#evaluate-trajectory), which can instruct an LLM to grade the efficacy of the agent's actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
