{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e537e271-4b08-491f-8cf7-c9be1f3fcf15",
   "metadata": {},
   "source": [
    "# Exact Match Evaluation\n",
    "\n",
    "The simplest evaluation type is direct string comparison. LangChain has a prebuilt [\"exact match\" evaluator](https://python.langchain.com/docs/guides/productionization/evaluation/string/exact_match) you can use, or you can do the same with a custom evaluator.\n",
    "\n",
    "You can check out the example results [here](https://smith.langchain.com/public/454c80b5-9809-4f4f-95ee-1f71d8e3ef53/d).\n",
    "\n",
    "[![Test graph](./img/result_example.png)](https://smith.langchain.com/public/454c80b5-9809-4f4f-95ee-1f71d8e3ef53/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121dcc53-70ec-48df-adac-cbd424c66adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U --quiet langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f610c6e-144b-47c8-9791-eaf4f42a8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your openai api key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62061d-0fb9-4ba9-b185-ff8c9746fb72",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "First you create a simple dataset of input and expected output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8ca802-e306-4632-afec-e9d655c84982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "client = langsmith.Client()\n",
    "dataset_name = \"Oracle of Exactness\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    ds = client.create_dataset(dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[\n",
    "            {\n",
    "                \"prompt_template\": \"State the year of the declaration of independence.\"\n",
    "                \"Respond with just the year in digits, nothign else\"\n",
    "            },\n",
    "            {\"prompt_template\": \"What's the average speed of an unladen swallow?\"},\n",
    "        ],\n",
    "        outputs=[{\"output\": \"1776\"}, {\"output\": \"5\"}],\n",
    "        dataset_id=ds.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ea231-7901-44b8-9d66-761d3aca140a",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad3e9fc-72ac-4854-a67a-378ae0c8c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'impressionable-crew-29' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/4f23ec54-3cf8-44fc-a729-ce08ad855bfd/compare?selectedSessions=a0672ba4-e513-4fef-84b8-bab439581721\n",
      "\n",
      "View all tests for Dataset Oracle of Exactness at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/4f23ec54-3cf8-44fc-a729-ce08ad855bfd\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.exact_match</th>\n",
       "      <th>feedback.matches_label</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2b4532af-445e-46aa-8170-d34c3af724a8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.265404</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.451211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.638880</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.732714</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.exact_match feedback.matches_label error  execution_time  \\\n",
       "count               2.000000                      2     0        2.000000   \n",
       "unique                   NaN                      2     0             NaN   \n",
       "top                      NaN                  False   NaN             NaN   \n",
       "freq                     NaN                      1   NaN             NaN   \n",
       "mean                0.500000                    NaN   NaN        0.545045   \n",
       "std                 0.707107                    NaN   NaN        0.265404   \n",
       "min                 0.000000                    NaN   NaN        0.357376   \n",
       "25%                 0.250000                    NaN   NaN        0.451211   \n",
       "50%                 0.500000                    NaN   NaN        0.545045   \n",
       "75%                 0.750000                    NaN   NaN        0.638880   \n",
       "max                 1.000000                    NaN   NaN        0.732714   \n",
       "\n",
       "                                      run_id  \n",
       "count                                      2  \n",
       "unique                                     2  \n",
       "top     2b4532af-445e-46aa-8170-d34c3af724a8  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'impressionable-crew-29',\n",
       " 'results': {'893730f0-393d-4c40-92f9-16ce24aaec1f': {'input': {'prompt_template': \"What's the average speed of an unladen swallow?\"},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=0, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('089a016a-d847-4a26-850c-afc0e78879d5'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='matches_label', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.732714,\n",
       "   'run_id': '2b4532af-445e-46aa-8170-d34c3af724a8',\n",
       "   'output': {'output': 'The average speed of an unladen European swallow is approximately 20.1 miles per hour (32.4 km/h).'},\n",
       "   'reference': {'output': '5'}},\n",
       "  'ec9d8754-d264-4cec-802e-0c33513843d8': {'input': {'prompt_template': 'State the year of the declaration of independence.Respond with just the year in digits, nothign else'},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=1, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cd4c7ede-f367-4d9c-b424-577bf054bf21'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='matches_label', score=True, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.357376,\n",
       "   'run_id': '82b65c5c-bfbf-4d2b-9c05-3bbd1cd4e711',\n",
       "   'output': {'output': '1776'},\n",
       "   'reference': {'output': '1776'}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "# This is your model/system that you want to evaluate\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    response = ChatOpenAI(model=model).invoke(input_[\"prompt_template\"])\n",
    "    return {\"output\": response.content}\n",
    "\n",
    "\n",
    "@run_evaluator\n",
    "def compare_label(run, example) -> EvaluationResult:\n",
    "    # Custom evaluators let you define how \"exact\" the match ought to be\n",
    "    # It also lets you flexibly pick the fields to compare\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    target = example.outputs.get(\"output\") or \"\"\n",
    "    match = prediction and prediction == target\n",
    "    return EvaluationResult(key=\"matches_label\", score=match)\n",
    "\n",
    "\n",
    "# This defines how you generate metrics about the model's performance\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"exact_match\"],  # equivalent prebuilt evaluator\n",
    "    custom_evaluators=[compare_label],\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=predict_result,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_metadata={\"version\": \"1.0.0\", \"model\": model},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442400b-903d-441b-a6d7-58fe2abd253c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
