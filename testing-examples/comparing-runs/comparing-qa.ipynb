{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Comparing Q&A System Outputs\n",
    "\n",
    "The most common way to compare two models is to benchmark them both on a dataset and compare the aggregate metrics.\n",
    "\n",
    "This approach is useful but it may filter out helpful information about the quality of the two system variants. In this case, it can be helpful to directly perform pairwise comparisons on the responses and take the resulting preference scores into consideration.\n",
    "\n",
    "In this tutorial, we will share one way to do this in code. We will use a retrieval Q&A system over LangSmith's docs as a motivating example.\n",
    "\n",
    "The main steps are:\n",
    "\n",
    "1. Create a dataset of questions and answers.\n",
    "2. Define different versions of your chains to evaluate.\n",
    "3. Evaluate chains directly on a dataset using regular metrics (e.g. correctness).\n",
    "4. Evaluate the pairwise preferences over that dataset\n",
    "\n",
    "In this case, we will test the impact of chunk sizes on our result quality. Let's begin!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial uses OpenAI for the model, ChromaDB to store documents, and LangChain to compose the chain. To make sure the tracing and evals are set up for [LangSmith](https://smith.langchain.com), please configure your API Key appropriately.\n",
    "\n",
    "We will also use pandas to render the results in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "Install the required packages. `lxml` and `html2text` are used by the document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \"langchain[openai]\" --quiet\n",
    "# %pip install chromadb --quiet\n",
    "# %pip install lxml --quiet\n",
    "# %pip install html2text --quiet\n",
    "# %pip install pandas --quiet\n",
    "# %pip install nest_asyncio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bae821d-1861-4b43-9495-7285326c6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for running in jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## 1. Create a dataset\n",
    "\n",
    "No evaluation process is complete without a development dataset. We've hard-coded a few examples below to demonstrate the process. In general, you'll want a lot more (>100) pairs for statistically significant results. Drawing from actual user queries can be helpful to ensure better representation of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\"),\n",
    "    (\"How might I query for all runs in a project?\", \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\"),\n",
    "    (\"What's a langsmith dataset?\", \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\"),\n",
    "    (\"How do I use a traceable decorator?\", \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\"),\n",
    "    (\"Can I trace my Llama V2 llm?\", \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\"),\n",
    "    (\"Why do I have to set environment variables?\", \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "     \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\"),\n",
    "    (\"How do I move my project between organizations?\", \"LangSmith doesn't directly support moving projects between organizations.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edb7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Retrieval QA Questions\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "for q, a in examples:\n",
    "    client.create_example(inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "## 2. Define RAG Q&A system\n",
    "\n",
    "Our Q&A system uses a simple retriever and LLM response generator. To break that down further, the chain will be composed of:\n",
    "\n",
    "1. A [VectorStoreRetriever](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.base.VectorStoreRetriever.html#langchain.vectorstores.base.VectorStoreRetriever) to retrieve documents. This uses:\n",
    "   - An embedding model to vectorize documents and user queries for retrieval. In this case, the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) model.\n",
    "   - A vectorstore, in this case we will use [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma).\n",
    "2. A response generator. This uses:\n",
    "   - A [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html#langchain.prompts.chat.ChatPromptTemplate) to combine the query and documents. \n",
    "   - An LLM, in this case, the 16k token context window version of `gpt-3.5-turbo` via [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI).\n",
    "\n",
    "We will combine them using LangChain's [expression syntax](https://python.langchain.com/docs/guides/expression_language/cookbook).\n",
    "\n",
    "First, load the documents to populate the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fab721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "\n",
    "def create_retriever(transformed_documents, text_splitter):\n",
    "    documents = text_splitter.split_documents(transformed_documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f884-bb93-427d-a0ad-3858c449a1ea",
   "metadata": {},
   "source": [
    "Next up, we'll define the chain. Since we are going to vary the retriever parameters, our constructor will\n",
    "take the retriever as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "                \" questions from LangSmith's documentation.\"\n",
    "                \" LangChain is a framework for building applications using large language models.\"\n",
    "                \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "                (\"system\", \"{context}\"),\n",
    "                (\"human\",\"{question}\")\n",
    "            ]\n",
    "        ).partial(time=str(datetime.now()))\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    response_generator = (\n",
    "        prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    chain = (\n",
    "        # The runnable map here routes the original inputs to a context and a question dictionary to pass to the response generator\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | response_generator\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d02a25-2863-4c5c-81e8-00887247516d",
   "metadata": {},
   "source": [
    "With the documents prepared, and the chain constructor ready, it's time to create and evaluate our chains.\n",
    "We will vary the split size and overlap to evaluate its impact on the response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1028d963-d84b-4759-a5ca-087b27065485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "retriever = create_retriever(transformed, text_splitter)\n",
    "\n",
    "chain_1 = create_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5765c938-c5f6-4ee4-be6b-90b7b341b683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will shrink both the chunk size and overlap\n",
    "text_splitter_2 = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "retriever_2 = create_retriever(transformed, text_splitter_2)\n",
    "\n",
    "chain_2 = create_chain(retriever_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "## 3. Evaluate the chains\n",
    "\n",
    "At this point, we are still going through the regular development -> evaluation process. We have two candidates and will evaluate them with a correctness evaluator from LangChain. By running `run_on_dataset`, we will generate predicted answers to each question in the dataset and log feedback from the evaluator for that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62de3aea-638e-4add-998a-22eb21f6feaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    # We will use the chain-of-thought Q&A correctness evaluator\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae51a83-6ae1-4fa6-a9d8-4e27bf03047f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '883bc88d0e994280aee27945d7c65496-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/ab79529c-e05a-4fe3-b290-1b62523b4572?eval=true\n"
     ]
    }
   ],
   "source": [
    "results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain_1,\n",
    "    evaluation=eval_config\n",
    ")\n",
    "project_name = results[\"project_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec375e76-7261-433b-9f02-f4750acf6f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '476151a4fdcd468b8f84b3312f61345a-RunnableSequence' at:\n",
      "https://smith.langchain.com/projects/p/abddba69-614f-4d2c-8dd3-96dbd8942b67?eval=true\n"
     ]
    }
   ],
   "source": [
    "results_2 = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain_2,\n",
    "    evaluation=eval_config\n",
    ")\n",
    "project_name_2 = results_2[\"project_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d2019-d076-4f18-832e-634dbd31091c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you should have two test run projects over the same dataset. If you click on one, it should look something like the following:\n",
    "    \n",
    "![Original Feedback](img/original_eval.png)\n",
    "\n",
    "You can look at the aggregate results here and for the other project to compare them, but let's move on to the pairwise comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb19f4-1855-4e9c-a447-491625c66646",
   "metadata": {},
   "source": [
    "## 4. Pairwise Evaluation\n",
    "\n",
    "Suppose both approaches return similar scores when evaluated in isolation.\n",
    "\n",
    "We can run a pairwise evaluator to see how try to predict preferred outputs. We will first define a couple helper functions to run the evaluator\n",
    "on each prediction pair. Let's break this function down:\n",
    "\n",
    "- The function accepts a dataset example and loads each model's predictions on that data point.\n",
    "- It then randomizes the order of the predictions and calls the evaluator. This is done to aveage out the impact of any ordering bias in the evaluator LLM.\n",
    "- Once the evaluation result is returned, we check it to make sure it is valid and then log feedback for both models.\n",
    "\n",
    "Once this is complete, the values are all returned so we can display them in a table in the notebook below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7c70f3-9c8f-436d-85ca-5a398c3b1369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "\n",
    "def _get_run_and_prediction(example_id, project_name):\n",
    "    run = next(client.list_runs(reference_example_id=example_id, project_name=project_name))\n",
    "    prediction = next(iter(run.outputs.values()))\n",
    "    return run, prediction\n",
    "\n",
    "def _log_feedback(run_ids):\n",
    "    for score, run_id in enumerate(run_ids):\n",
    "        client.create_feedback(run_id, key=\"preference\", score=score)\n",
    "\n",
    "def predict_preference(example, project_a, project_b, eval_chain):\n",
    "    example_id = example.id\n",
    "    run_a, pred_a = _get_run_and_prediction(example_id, project_a)\n",
    "    run_b, pred_b = _get_run_and_prediction(example_id, project_b)\n",
    "    input_, answer = example.inputs['question'], example.outputs['answer']\n",
    "    result = {\"input\": input_, \"answer\": answer, \"A\": pred_a, \"B\": pred_b}\n",
    "\n",
    "    # Flip a coin to average out persistent positional bias\n",
    "    if random.random() < 0.5:\n",
    "        result['A'], result['B'] = result['B'], result['A']\n",
    "        run_a, run_b = run_b, run_a\n",
    "    try:\n",
    "        eval_res = eval_chain.evaluate_string_pairs(\n",
    "            prediction=result['A'],\n",
    "            prediction_b=result['B'],\n",
    "            input=input_, \n",
    "            reference=answer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        return result\n",
    "\n",
    "    if eval_res[\"value\"] is None:\n",
    "        return result\n",
    "\n",
    "    preferred_run = (run_a.id, \"A\") if eval_res[\"value\"] == \"A\" else (run_b.id, \"B\")\n",
    "    runner_up_run = (run_b.id, \"B\") if eval_res[\"value\"] == \"A\" else (run_a.id, \"A\")\n",
    "    _log_feedback((runner_up_run[0], preferred_run[0]))\n",
    "    result[\"Preferred\"] = preferred_run[1]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a51df-616f-4f3e-9106-a5f7669b9090",
   "metadata": {},
   "source": [
    "For this example, we will use the `labeled_pairwise_string` evaluator from LangChain off-the-shelf. By default, instructs the evaluation llm to choose the preference based on helpfulness, relevance, correctness, and depth of thought. In your case, you will likely want to customize the criteria used!\n",
    "\n",
    "For more information on how to configure it, check out the [Labeled Pairwise String Evaluator](https://python.langchain.com/docs/guides/evaluation/comparison/labeled_pairwise_string) documentation and inspect the resulting traces when running this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e1f12e-ed28-46ce-b09b-37088c475bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "pairwise_evaluator = load_evaluator(\"labeled_pairwise_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f65e8572-18a3-46b3-936a-38e566b77922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "eval_func = functools.partial(\n",
    "    predict_preference,\n",
    "    project_a=project_name,\n",
    "    project_b=project_name_2,\n",
    "    eval_chain=pairwise_evaluator,\n",
    ")\n",
    "\n",
    "\n",
    "# We will wrap in a lambda to take advantage of its default `batch` convenience method\n",
    "runnable = RunnableLambda(eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16969ec1-d27c-4f8e-9624-8c3f25c51f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Invalid verdict: Final decision: [[A. Verdict must be one of 'A', 'B', or 'C'.\n"
     ]
    }
   ],
   "source": [
    "examples = list(client.list_examples(dataset_name=\"Retrieval QA Questions\"))\n",
    "values = runnable.batch(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be2783-6c15-4fb9-8fdc-11eaa91197fa",
   "metadata": {},
   "source": [
    "By running the function above, the \"preference\" feedback was automatically logged to the test projects you created in step 3. Below is a view of the same test run as before with the preference scores added. This model seems to be less preferred than the other! \n",
    "\n",
    "![Preference Tags](img/with_preferences.png)\n",
    "\n",
    "The `predict_preference` function we wrote above is set up to not log feedback in the case of a tie, meaning some of the examples do not have a corresponding preference score. You can adjust this behavior as you see fit. \n",
    "\n",
    "You can also view the feedback results for the other test run in the app to see how well the evaluator's results match your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea9a80d8-0672-44a5-b858-49e83427bdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46c7d7f7-9e26-4439-8a2c-93f6d89741ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .hover_table td {\n",
       "            max-width: 200px; /* You can adjust this value */\n",
       "            overflow: hidden;\n",
       "            text-overflow: ellipsis;\n",
       "            white-space: nowrap;\n",
       "        }\n",
       "        .hover_table td:hover {\n",
       "            white-space: normal;\n",
       "            word-wrap: break-word;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe hover_table\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Preferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I move my project between organizations?</td>\n",
       "      <td>LangSmith doesn't directly support moving projects between organizations.</td>\n",
       "      <td>Currently, LangSmith does not support project migration between organizations. If you want to move your project to a different organization, the recommended approach is to create a new project within the desired organization and manually transfer the necessary data, such as runs and datasets, to the new project.\\n\\nYou can achieve this by using the LangSmith SDK or REST API to export the data from the original project and import it into the new project. Here are the general steps you can follow:\\n\\n1. Export the runs and datasets from the original project using the LangSmith API or SDK. You can filter the runs and datasets based on your requirements.\\n\\n2. Save the exported data to a local file or storage location.\\n\\n3. Create a new project in the desired organization using the LangSmith web app or API.\\n\\n4. Import the exported runs and datasets into the new project using the LangSmith API or SDK. Make sure to map the data correctly to the new project's structure.\\n\\nBy following these steps, you can effectively move your project from one organization to another.</td>\n",
       "      <td>Currently, LangSmith does not support project migration between organizations. However, you can manually export and import runs and datasets using the SDK to imitate project migration. Here's how you can do it:\\n\\n1. Export Runs: Use the LangSmith SDK to export the runs from your current organization. You can find examples of exporting runs in the LangSmith SDK documentation.\\n\\n2. Export Datasets: Similarly, use the LangSmith SDK to export the datasets associated with your project.\\n\\n3. Create a New Project: In the new organization, create a new project where you want to move your project to.\\n\\n4. Import Runs: Use the LangSmith SDK to import the exported runs into the new project. Again, you can find examples of importing runs in the LangSmith SDK documentation.\\n\\n5. Import Datasets: Finally, use the LangSmith SDK to import the exported datasets into the new project.\\n\\nBy following these steps, you can effectively move your project from one organization to another.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why do I have to set environment variables?</td>\n",
       "      <td>Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith. While there are other ways to connect, environment variables tend to be the simplest way to configure your application.</td>\n",
       "      <td>Setting environment variables is a common practice in software development for configuring and customizing the behavior of applications. In the context of LangSmith and LangChain, environment variables are used to enable and configure tracing, which allows you to log and monitor the execution of your language models.\\n\\nBy setting the `LANGCHAIN_TRACING_V2` environment variable, you enable tracing for your LangChain applications. This allows you to log the input, output, and other relevant information about each run of your language model. Tracing can be useful for debugging, understanding model behavior, and monitoring the performance of your applications.\\n\\nSetting environment variables provides a flexible and configurable way to enable tracing without modifying your code. It allows you to control tracing behavior across different environments and deployments without the need for code changes.</td>\n",
       "      <td>Setting environment variables is a common practice in software development to configure the behavior of applications or libraries. In the case of LangSmith, setting environment variables is necessary to enable tracing and logging of run traces to LangSmith.\\n\\nBy setting the `LANGCHAIN_TRACING_V2` environment variable, LangChain components such as LLMs, chains, agents, tools, and retrievers will automatically log their calls and responses to LangSmith. This allows you to easily debug and monitor the behavior of your application, track inputs and outputs, analyze latency and token usage, and more.\\n\\nUsing environment variables provides a flexible and consistent way to configure tracing across different programming languages and environments. It allows you to enable or disable tracing without modifying your code, making it easier to switch between development, testing, and production environments.\\n\\nWhile setting environment variables may require some initial setup, it provides a powerful and convenient way to leverage the capabilities of LangSmith for debugging and monitoring your LangChain applications.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can I trace my Llama V2 llm?</td>\n",
       "      <td>So long as you are using one of LangChain's LLM implementations, all your calls can be traced</td>\n",
       "      <td>Yes, you can trace your Llama V2 LLM using LangSmith's tracing feature. Tracing allows you to log the runs of your LLM application and inspect the inputs and outputs of each component in the chain. To enable tracing for your Llama V2 LLM, you need to set the `LANGCHAIN_TRACING_V2` environment variable to `true` before running your application. Here's an example of how to enable tracing in Python:\\n\\n```python\\nimport os\\nfrom langchain.chat_models import LlamaV2\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n\\nllm = LlamaV2()\\nllm.invoke(\"Hello, world!\")\\n```\\n\\nMake sure you have also set the `LANGCHAIN_ENDPOINT` and `LANGCHAIN_API_KEY` environment variables to the appropriate values for your LangSmith account.</td>\n",
       "      <td>Yes, you can trace your Llama V2 LLM using LangSmith. Tracing allows you to log the inputs, outputs, and other important information of your LLM runs to LangSmith for visualization and debugging purposes.\\n\\nTo enable tracing for your Llama V2 LLM, you need to set the `LANGCHAIN_TRACING_V2` environment variable to `true` before running your LLM code. Here's an example of how to enable tracing in Python:\\n\\n```python\\nimport os\\nfrom langchain.chat_models import LlamaV2\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n\\nllm = LlamaV2()\\nllm.invoke(\"Hello, world!\")\\n```\\n\\nMake sure you have the LangSmith SDK installed and your API key and project name configured correctly. The traces from your Llama V2 LLM runs will be logged to LangSmith and can be visualized in the LangSmith web app.\\n\\nNote that tracing is asynchronous, so you may need to wait for the traces to be submitted before exiting your application. You can use the `wait_for_all_tracers()` function from the `langsmith.callbacks.tracers.langchain` module to ensure all traces are logged before exiting.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I use a traceable decorator?</td>\n",
       "      <td>The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,import the required function, decorate your function, and then call the function. Below is an example:\\n```python\\nfrom langsmith.run_helpers import traceable\\n@traceable(run_type=\"chain\") # or \"llm\", etc.\\ndef my_function(input_param):\\n    # Function logic goes here\\n    return output\\nresult = my_function(input_param)\\n```</td>\n",
       "      <td>To use the `traceable` decorator, you need to import it from the `langsmith.run_helpers` module. The `traceable` decorator allows you to easily log the execution of a function as a run in LangSmith.\\n\\nHere's an example of how to use the `traceable` decorator:\\n\\n```python\\nfrom langsmith.run_helpers import traceable\\n\\n@traceable(run_type=\"llm\", name=\"openai.ChatCompletion.create\")\\ndef my_llm(*args, **kwargs):\\n    # Your function logic here\\n    pass\\n```\\n\\nIn the example above, the `traceable` decorator is applied to the `my_llm` function. The `run_type` parameter specifies the type of the run (e.g., \"llm\", \"tool\", \"chain\"), and the `name` parameter specifies the name of the run. You can customize these parameters based on your specific use case.\\n\\nOnce the `traceable` decorator is applied, every time the `my_llm` function is called, a run will be logged to LangSmith with the specified run type and name.\\n\\nNote that the `traceable` decorator works for both synchronous and asynchronous functions. If you're using an asynchronous function, make sure to use the `await` keyword when calling the function.\\n\\nYou can find more information about the `traceable` decorator in the LangSmith documentation under the \"Tracing\" section.</td>\n",
       "      <td>To use the `traceable` decorator in LangSmith, you can follow these steps:\\n\\n1. Import the necessary modules:\\n```python\\nfrom langsmith.run_helpers import traceable\\n```\\n\\n2. Define your function and decorate it with the `traceable` decorator:\\n```python\\n@traceable(run_type=\"llm\")\\ndef my_function(arg1, arg2):\\n    # Function logic here\\n    return result\\n```\\nThe `run_type` argument specifies the type of the run, such as \"llm\", \"chain\", or \"tool\". You can choose the appropriate run type based on your use case.\\n\\n3. Call your function as usual:\\n```python\\nresult = my_function(arg1, arg2)\\n```\\n\\nWhen you call the decorated function, it will automatically log a trace of the function execution to LangSmith. The trace will include the inputs, outputs, and other relevant information.\\n\\nNote that the `traceable` decorator works for both synchronous and asynchronous functions. If you're using an asynchronous function, make sure to use the `await` keyword when calling it.\\n\\nYou can find more examples and details in the [traceable notebook](https://github.com/langchain-ai/langsmith-cookbook/blob/main/tracing-examples/traceable/traceable.ipynb) in the LangSmith Cookbook.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What's a langsmith dataset?</td>\n",
       "      <td>A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.</td>\n",
       "      <td>A LangSmith dataset is a collection of input-output examples that are used for testing, evaluation, and training purposes in the LangSmith framework. It consists of pairs of input prompts and their corresponding model outputs. These examples can be created and curated during the development and debugging process of building applications using large language models (LLMs).\\n\\nLangSmith datasets are valuable for several reasons:\\n\\n1. **Testing and Evaluation:** Datasets can be used to test the performance of prompts, chains, agents, and other components of LLM applications. By running the examples in the dataset through the application, developers can observe the outputs and evaluate the effectiveness of their models.\\n\\n2. **Debugging and Troubleshooting:** When encountering unexpected or incorrect outputs, developers can add these examples to a dataset for further analysis and debugging. By examining the inputs and outputs in the dataset, they can identify issues and improve the application.\\n\\n3. **Benchmarking and Comparison:** Datasets provide a benchmark for comparing different versions of prompts, chains, or models. Developers can use the dataset to measure the performance of different configurations and make informed decisions about which version to use.\\n\\n4. **Training and Fine-tuning:** LangSmith datasets can also be exported for use in training or fine-tuning language models. These datasets can be used to improve the performance of the models by incorporating real-world examples and specific use cases.\\n\\nLangSmith provides features to easily create, edit, and manage datasets within the LangSmith framework. Developers can add examples to datasets during the debugging process, and these datasets can be exported for use in other contexts, such as OpenAI Evals or fine-tuning with external tools like FireworksAI.</td>\n",
       "      <td>A LangSmith dataset is a collection of examples that can be used to evaluate or improve a chain, agent, or model. It consists of rows, where each row represents an example and contains the inputs and (optionally) the expected outputs for a given interaction. Datasets in LangSmith can be created from existing runs, uploaded as CSV files, or manually created using the LangSmith client. Datasets are useful for testing and evaluating models, monitoring performance, and exporting data for use in other contexts.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How might I query for all runs in a project?</td>\n",
       "      <td>client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})</td>\n",
       "      <td>To query for all runs in a project, you can use the LangSmith SDK or the web app. Here's how you can do it using the SDK in Python:\\n\\n```python\\nfrom langsmith import Client\\n\\nclient = Client()\\nruns = list(client.list_runs(project_name=\"&lt;your_project&gt;\"))\\n```\\n\\nIn this example, replace `&lt;your_project&gt;` with the name of your project. The `list_runs` method retrieves all runs in the specified project.\\n\\nIf you prefer to use the web app, you can navigate to the project and view all the runs in the runs table. You can also use the search bar in the runs table to filter and search for specific runs based on various criteria.\\n\\nRemember to authenticate with your LangSmith credentials when using the SDK to access your projects and runs.</td>\n",
       "      <td>To query for all runs in a project, you can use the LangSmith SDK or the LangSmith REST API. Here's an example of how to do it using the LangSmith SDK in Python:\\n\\n```python\\nfrom langsmith import Client\\n\\nclient = Client()\\nruns = list(client.list_runs(project_name=\"Your Project Name\"))\\n```\\n\\nIn this example, replace \"Your Project Name\" with the name of your project. The `list_runs` method returns a generator that you can iterate over to get all the runs in the project.\\n\\nIf you prefer to use the LangSmith REST API directly, you can make a GET request to the `/runs` endpoint with the appropriate project name parameter.\\n\\nKeep in mind that you may need to paginate through the results if there are a large number of runs in the project. The LangSmith SDK provides convenient methods for pagination, such as `client.list_runs_paginated()`, which returns a paginated generator that you can iterate over.\\n\\nRemember to authenticate your requests with your API key.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is LangChain?</td>\n",
       "      <td>LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.</td>\n",
       "      <td>LangChain is a framework for building applications using large language models (LLMs). It aims to simplify the process of building reliable LLM applications by providing tools and features to enhance development, testing, evaluation, and monitoring.\\n\\nLangChain helps bridge the gap between the initial setup of LLMs and their performance in production. It offers a tracing feature that logs all calls to LLMs, chains, agents, tools, and retrievers, providing visibility into the behavior of the application. This tracing feature can be used for debugging purposes, such as identifying unexpected end results, understanding why an agent is looping, or investigating the latency of a chain.\\n\\nThe framework also includes a playground feature that allows developers to modify prompts and observe the resulting changes to the output. This helps in experimenting with different inputs and understanding the impact on the application's behavior.\\n\\nLangChain provides a visualization of the sequence of events in complicated chains and agents, making it easier to understand the flow and interactions between different components. It also tracks the token usage of each step in the chain, helping developers identify potentially costly parts.\\n\\nAdditionally, LangChain supports collaborative debugging by allowing users to share faulty chains and LLM runs with colleagues for troubleshooting. It also enables the collection of examples to create datasets for testing and evaluation. Developers can add input/output examples to datasets at different steps of the chain, facilitating benchmarking and testing against known issues.\\n\\nFor evaluation purposes, LangChain allows the uploading of datasets and running chains over the data points. The results can be visualized and evaluated using built-in evaluators or custom evaluators. Feedback can be assigned to runs and marked as correct or incorrect, providing aggregate statistics for each test project.\\n\\nFinally, LangChain can be used for monitoring applications in production. It provides the ability to log traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\\n\\nOverall, LangChain aims to simplify the development, testing, evaluation, and monitoring of LLM applications, making it easier to build reliable and performant language-based applications.</td>\n",
       "      <td>LangChain is a framework for building applications using large language models (LLMs). It aims to simplify the process of developing reliable LLM applications by providing tools and features to enhance debugging, testing, evaluation, and monitoring.\\n\\nLangChain helps bridge the gap between the initial setup of LLMs and their performance in production. It offers a tracing feature that logs all calls to LLMs, chains, agents, tools, and retrievers, providing visibility into the inputs and outputs of each call. This tracing feature is enabled by default in LangChain.\\n\\nDebugging LLMs, chains, and agents can be challenging, but LangChain provides tools to address common pain points. It offers a visualization of the exact inputs and outputs of LLM calls, allowing developers to understand the formatting logic, transformations to user input, and missing input. LangChain also provides a playground where developers can modify prompts and observe the resulting changes to the output.\\n\\nIn complex chains and agents, it can be difficult to understand the sequence of events and the interactions between different calls. LangChain's tracing feature includes a visualization of the sequence of calls, helping developers gain insights into the inner workings of their applications.\\n\\nLangChain also helps identify performance issues by tracking the latency of each step in a chain. Developers can identify and optimize the slowest components of their applications. Additionally, LangChain tracks the token usage of each step, making it easy to identify potentially costly parts of the chain.\\n\\nCollecting examples of failures and unexpected outcomes is crucial for testing and improving LLM applications. LangChain allows developers to add input/output examples to datasets, which can be used for testing changes to prompts or chains. These datasets can be evaluated using LangChain's evaluators, providing guidance on examples that require further investigation.\\n\\nLangChain can also be used for monitoring LLM applications in production. It allows developers to log traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Feedback can be associated with runs, enabling performance tracking over time.\\n\\nOverall, LangChain aims to simplify the development, testing, evaluation, and monitoring of LLM applications, providing tools and features to enhance reliability and performance.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def word_wrap_on_hover(df):\n",
    "    \"\"\"\n",
    "    Takes a Pandas DataFrame and returns an HTML table with word wrap on hover.\n",
    "    \"\"\"\n",
    "    styles = \"\"\"\n",
    "    <style>\n",
    "        .hover_table td {\n",
    "            max-width: 200px; /* You can adjust this value */\n",
    "            overflow: hidden;\n",
    "            text-overflow: ellipsis;\n",
    "            white-space: nowrap;\n",
    "        }\n",
    "        .hover_table td:hover {\n",
    "            white-space: normal;\n",
    "            word-wrap: break-word;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    html_table = df.to_html(classes='hover_table')\n",
    "    return HTML(styles + html_table)\n",
    "\n",
    "word_wrap_on_hover(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec77a-67b3-48e8-9cde-c9022641f245",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "In this walkthrough, you compared two versions of a RAG Q&A chain by predicting preference scores for each pair of predictions.\n",
    "This approach is one way to automatically compare two versions of a chain that can give additional context beyond regular benchmarking.\n",
    "\n",
    "There are many related ways to evaluate preferences! Here, we used binary choices to compare the two models and only evaluated once, but you may get better results by trying one of the following approaches:\n",
    "\n",
    "- Evaluate multiple times in each position and returning a win rate\n",
    "- Ensemble evaluators\n",
    "- Instruct the model to output continuous scores\n",
    "- Instruct the model to use a different prompt strategy than chain of thought\n",
    "\n",
    "For more information on measuring the reliability of this and other approaches, you can check out the [evaluations examples](https://python.langchain.com/docs/guides/evaluation/examples) in the LangChain repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b5ac9-1fd1-4eaa-bc6d-53263cd575bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
