[{"inputs": {"question": "How can I create a simple chat model using my locally saved huggingface model"}, "outputs": {"answer": "Certainly! To create a simple chat model using your locally saved Hugging Face model, follow these steps:\n\n1. Install the necessary Python libraries:\n\n- Install the Hugging Face Hub client library with 'pip install huggingface_hub'.\n- Install 'transformers' library for working with models and tokenizers using 'pip install transformers'.\n\n2. Create a Hugging Face account (if not done already) and obtain an access token.\n\n3. Set the access token as an environment variable named 'HUGGINGFACEHUB_API_TOKEN'.\n\n4. Choose the appropriate Hugging Face wrapper for your use case:\n\n- If working with a local pipeline, use the 'HuggingFacePipeline' wrapper from 'langchain.llms'.\n- If using a model hosted on Hugging Face Hub, utilize the 'HuggingFaceHub' wrapper from 'langchain.llms'.\n\n5. Import the chosen wrapper module and create an instance of the wrapper.\n\n6. Start interacting with your chat model by passing input text to the wrapper's methods.\n\nFor instance, using the 'HuggingFacePipeline' wrapper:\n\nfrom langchain.llms import HuggingFacePipeline\n\n# Create an instance of the wrapper\npipeline = HuggingFacePipeline(...)\n\n# Interact with the chat model\nresponse = pipeline('User: Hello, how can I help?')\n\n7. You can also use tokenizers from the 'transformers' package to count tokens, which can be useful when working with LLMs and splitting documents.\n\nThese steps should help you set up and use a simple chat model with your locally saved Hugging Face model."}}, {"inputs": {"question": "Can you show a me an example of how to create a vector store with Azure Cognitive Search?"}, "outputs": {"answer": "Certainly! To create a vector store with Azure Cognitive Search (ACS), follow these steps:\n\n1. Set up your ACS service, index, and obtain your API key (either Admin or Query key).\n\n2. Use the AzureCognitiveSearchRetriever from LangChain to interact with ACS.\n\n3. Import the necessary modules:\n\nimport os\nfrom langchain.retrievers import AzureCognitiveSearchRetriever\n\n4. Set your ACS service name, index name, and API key as environment variables:\n\nos.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = \"<YOUR_ACS_SERVICE_NAME>\"\nos.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"] = \"<YOUR_ACS_INDEX_NAME>\"\nos.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = \"<YOUR_API_KEY>\"\n\n5. Create the AzureCognitiveSearchRetriever:\n\nretriever = AzureCognitiveSearchRetriever(content_key=\"content\", top_k=10)\n\n6. Retrieve documents from Azure Cognitive Search:\n\nYou can use the 'get_relevant_documents' method to retrieve relevant documents from ACS. For example:\n\nretriever.get_relevant_documents(\"what is langchain\")\n\nYou can also adjust the number of results returned using the 'top_k' parameter. The default value is None, which returns all results.\n\nAzure Cognitive Search provides powerful search capabilities to enhance your application's user experience by retrieving relevant documents from your indexed content."}}, {"inputs": {"question": "Help me debug: TypeError: Pinecone.similarity_search_with_score() got an unexpected keyword argument 'min_score'"}, "outputs": {"answer": "Your error is likely due to an incorrect parameter being passed to the 'similarity_search_with_score' function in the Pinecone vector store. The correct function signature does not include a 'min_score' argument. This function retrieves documents most similar to a given query text while providing both the documents and their corresponding similarity scores. To use 'similarity_search_with_score', ensure you provide the right arguments: 'query' for the text you want to find similar documents for, 'k' for the number of documents to retrieve (default: 4), 'filter' as an optional metadata-based filtering dictionary, and 'namespace' to specify the search namespace (default is the empty namespace ''). The function returns a list of tuples, each containing a Document and a similarity score (float). To resolve the 'TypeError' issue, ensure proper argument usage as per the documentation.  "}}, {"inputs": {"question": "whats the code to load text file into a vector store"}, "outputs": {"answer": "To load a text file into a vector store using LangChain's document loaders, you can use the following code snippet:\n\n```python\nfrom langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./index.txt\")\ndocs = loader.load()\nprint(docs)\n```\n\nThis code imports the `TextLoader` class from the document_loaders module, creates an instance of the loader with the path to the text file as an argument, and then uses the `load()` method to read the file's content and create a Document. The resulting Document will contain the text content of the file."}}, {"inputs": {"question": "what is RAG"}, "outputs": {"answer": "RAG stands for Retrieval Augmented Generation. It's a powerful framework that combines LLMs (Language Models) with retrieval techniques to transform how users interact with search engines and information retrieval. RAG enables generative search, chat interactions with documents, and enhanced customer service chatbots.\n\nGenerative Search: RAG revolutionizes search by integrating LLMs and retrieval techniques, powering chat-search applications like Bing Chat, You.com, and Perplexity.\n\nChat with your Data: Using RAG, static content turns into dynamic knowledge sources, making information retrieval engaging and interactive.\n\nCustomer Service Chatbots: RAG enables new-generation chatbots that access vast knowledge bases, delivering accurate and context-aware assistance, enhancing brand loyalty, and providing exceptional customer service.\n\nAugmented Generation: RAG's core idea is Augmented Generation. It involves adding external context to input prompts for LLMs. This influences the generated response. In Langchain, this is achieved using prompt templates and chains like base QAChain.\n\nRetrieval + Augmented Generation: When working with longer content, Retrieval is used to index and retrieve relevant parts from a knowledge base. Combining retrieval and generation overcomes context limits and provides accurate results.\n\nConversational Retrieval Augmented Generation: Conversational RAG takes engagement further. It creates intuitive AI interactions, enhancing user satisfaction and establishing Conversational AI as a new UI paradigm."}}, {"inputs": {"question": "I want to return the source documents of my Weaviate retriever. Show me how"}, "outputs": {"answer": "To retrieve the source documents from your Weaviate vectorstore, you need to configure your client to include the 'sources' attribute. Here's an example of how you can achieve this:\n\n```python\nweaviate_client = Weaviate(\n    client=client,\n    index_name='index_name',\n    text_key='text',\n    embedding=OpenAIEmbeddings(),\n    by_text=False,\n    attributes=['source'],\n)\n```\n\nBy including the 'source' attribute in the attributes list, you ensure that the source documents are returned when you interact with the Weaviate client."}}, {"inputs": {"question": "What is html2texttransformer? Does it omit urls?"}, "outputs": {"answer": "The `Html2TextTransformer` is a class provided by LangChain's `document_transformers` module. It is used to transform HTML content into clean, easy-to-read plain ASCII text. The resulting ASCII text also happens to be valid Markdown, which is a text-to-HTML format.\n\nThis transformer is designed to convert HTML pages into human-readable text, making it suitable for scenarios where the goal is to extract content without the need to manipulate specific HTML elements.\n\nThe class provides methods such as `__init__()` for initialization and both `transform_documents()` and `atransform_documents()` for transforming a list of documents. The transformation process involves converting HTML content to plain text with markdown-like formatting.\n\nAs for whether the `Html2TextTransformer` omits URLs, the provided context does not explicitly mention URL omission. It appears to focus on the transformation of HTML content into plain text, but specific details about URL handling are not present in this context."}}, {"inputs": {"question": "Explain how a runnable lambda works"}, "outputs": {"answer": "A runnable lambda in LangChain allows you to incorporate custom functions or operations into a pipeline. These functions can process specific inputs and produce outputs for further pipeline usage. Key points:\n\n1. Arbitrary Functions: You can use custom functions within the pipeline to perform specific tasks on the data.\n2. Input Requirement: Functions should take a single argument. If they require multiple arguments, a wrapper function should be used.\n3. RunnableLambda Usage: The RunnableLambda component integrates custom functions. It's applied at various pipeline stages.\n\nIn the provided code example:\n\n1. `length_function` calculates the length of text.\n2. `_multiple_length_function` calculates a product of lengths.\n3. `multiple_length_function` wraps `_multiple_length_function` and takes dictionary input.\n\nThe 'chain' section demonstrates the usage:\n\n- 'a' extracts 'foo' value and applies `length_function` using RunnableLambda.\n- 'b' extracts 'foo' and 'bar', then applies `multiple_length_function` using RunnableLambda.\n\nUsing runnable lambdas, LangChain enables the seamless integration of custom logic and processing steps into pipelines."}}, {"inputs": {"question": "what are the main methods supported by Runnables"}, "outputs": {"answer": "The `runnable` interface in LangChain supports several main methods for interacting with components:\n\n1. `stream`: This method streams back response chunks in real-time, useful for processing large computations or text generation as they become available.\n\n2. `invoke`: Calls the runnable chain on a single input, providing a single response output.\n\n3. `batch`: Calls the runnable chain on a list of inputs, returning a list of corresponding responses.\n\n4. `astream`: Asynchronously streams response chunks back, suitable for handling concurrent tasks.\n\n5. `ainvoke`: Asynchronously calls the runnable chain on a single input and returns the response.\n\n6. `abatch`: Asynchronously calls the runnable chain on a list of inputs, returning a list of corresponding responses."}}, {"inputs": {"question": "Tell me about the stream() method"}, "outputs": {"answer": "The stream() method is part of the LangChain framework and is used to process and handle responses in a streaming manner. It's particularly useful for dealing with large amounts of data or lengthy computations where you want to process results as they become available, rather than waiting for the entire computation to finish before accessing the results. From the code snippet:\n\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\n\nmodel = ChatOpenAI()\n\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n\nchain = prompt | model\n\n# Using the stream() method to process the response in chunks\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)"}}, {"inputs": {"question": "What's the purpose of the transform method?"}, "outputs": {"answer": "The purpose of the transform method, in a broader context, is to modify or manipulate data in a specific and structured way to achieve desired outcomes."}}, {"inputs": {"question": "Show me how to use RecursiveURLLoader"}, "outputs": {"answer": "The RecursiveURLLoader comes from the langchain.document_loaders.recursive_url_loader module. Here's an example of how to use it:\n\n```python\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n\n# Create an instance of RecursiveUrlLoader with the URL you want to load\nloader = RecursiveUrlLoader(url=\"https://example.com\")\n\n# Load all child links from the URL page\nchild_links = loader.load()\n\n# Print the child links\nfor link in child_links:\n    print(link)\n```\n\nMake sure to replace \"https://example.com\" with the actual URL you want to load. The load() method returns a list of child links found on the URL page. You can iterate over this list to access each child link."}}, {"inputs": {"question": "What are Runnables?"}, "outputs": {"answer": "A Runnable is a generic unit of work that can be invoked, batched, streamed, and/or transformed."}}, {"inputs": {"question": "How can I fine tune with LangChain?"}, "outputs": {"answer": "LangChain makes it easy to fine-tune models like GPT-3 and LLaMA using your own data. The key steps are: Collect and prepare your training data. LangSmith helps with this by allowing you to log model outputs, filter and tag them, and export clean datasets. Quantize your model to fit on available GPU memory if needed. Tools like BitsandBytes make this easy. Use parameter efficient fine-tuning approaches like LoRA or qLoRA to fine-tune within GPU constraints. Fine-tune your model on your data using HuggingFace or services like Anthropic Claude. LangChain's CoLab notebooks provide examples. Evaluate your fine-tuned model using LangSmith - you can set up custom evaluators and chains and inspect generations. Analyze the results to understand where your model is succeeding vs failing using LangSmith's filters and analytics. Improve fine-tuning through steps like collecting more data, using a bigger model, adding instructions, and scaling up compute. So in summary, LangChain and LangSmith help you efficiently collect data, fine-tune models like GPT-3 and LLaMA, evaluate results, and iterate to improve performance on your specialized tasks. The documentation provides end-to-end examples and recipes to follow."}}, {"inputs": {"question": "What is BabyAGI and why is it useful for LangChain"}, "outputs": {"answer": "BabyAGI is a system of autonomous AI agents that can generate and simulate the execution of tasks based on a given objective."}}, {"inputs": {"question": "Show me an example using Weaviate, but customizing the vectorStoreRetriever to return the top 10 k nearest neighbors. "}, "outputs": {"answer": "To customize the Weaviate client and return the top 10 k nearest neighbors, you can utilize the `as_retriever` method with the appropriate parameters. Here's how you can achieve this:\n\n```python\n# Assuming you have imported the necessary modules and classes\n\n# Create the Weaviate client\nclient = weaviate.Client(url=os.environ[\"WEAVIATE_URL\"], ...)\n\n# Initialize the Weaviate wrapper\nweaviate = Weaviate(client, index_name, text_key)\n\n# Customize the client to return top 10 k nearest neighbors using as_retriever\ncustom_retriever = weaviate.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\n        'k': 10  # Customize the value of k as needed\n    }\n)\n\n# Now you can use the custom_retriever to perform searches\nresults = custom_retriever.search(query, ...)\n```"}}, {"inputs": {"question": "Does LangChain support local LLMs? If so, show me which ones. "}, "outputs": {"answer": "Yes, LangChain supports a number of local LLMs via several providers. For instance:\n\n```python\nfrom langchain.llms import LlamaCpp\nfrom langchain.llms import GPT4All\nfrom langchain.llms import HuggingFacePipeline\n```\n"}}, {"inputs": {"question": "Let's say I wanted to chat with a Github repo. Write me code to do that."}, "outputs": {"answer": "To chat with a GitHub repository using LangChain, you can follow these steps:\n\nFirst, define a function, for instance, `ingest_repo()`, that ingests the contents of the GitHub repository into LangChain:\n\n```python\nimport os\nimport shutil\nfrom git import Repo\nfrom langchain.loaders import GenericLoader\nfrom langchain.parsers import LanguageParser, Language\nfrom langchain.textsplitters import RecursiveCharacterTextSplitter\n\n\ndef ingest_repo():\n    repo_path = \"/Users/mollycantillon/Desktop/test_repo\"\n    if os.path.exists(repo_path):\n        shutil.rmtree(repo_path)\n\n    repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)\n\n    loader = GenericLoader.from_filesystem(\n        repo_path+\"/libs/langchain/langchain\",\n        glob=\"**/*\",\n        suffixes=[\".py\"],\n        parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n    )\n    documents_repo = loader.load()\n    len(documents_repo)\n\n    python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n                                                                    chunk_size=2000,\n                                                                    chunk_overlap=200)\n    texts = python_splitter.split_documents(documents_repo)\n\n    return texts\n```\n\nThis function, `ingest_repo()`, clones the repository, loads and parses Python files, and then splits the documents into chunks for chat-based interaction. You can use the returned `texts` to chat with the GitHub repository contents using LangChain."}}, {"inputs": {"question": "How do I deal with openai rate limiting by having a backup model? Show me with code."}, "outputs": {"answer": "A straightforward way to do this is to specify fallback models using LangChain Expression Language. Example below:\n```python     \nfrom langchain.chat_models import ChatOpenAI\n\nbad_llm = ChatOpenAI(model_name=\"gpt-fake\")\ngood_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\nllm = bad_llm.with_fallbacks([good_llm])\nllm.invoke(\"Why did the the chicken cross the road?\")\n```"}}, {"inputs": {"question": "I want to save the configuration for a given LLM. Show me how to do that."}, "outputs": {"answer": "To save the configuration for a given LLM, you can follow these steps:\n\nFirst, let's go over loading an LLM configuration from disk. LLM configurations can be saved in two formats: JSON or YAML, but they are loaded in the same way.\n\nFor a JSON-formatted LLM configuration (llm.json):\n\n```json\n{\n    \"model_name\": \"text-davinci-003\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 256,\n    \"top_p\": 1.0,\n    \"frequency_penalty\": 0.0,\n    \"presence_penalty\": 0.0,\n    \"n\": 1,\n    \"best_of\": 1,\n    \"request_timeout\": null,\n    \"_type\": \"openai\"\n}\n```\n\nLoad the configuration like this:\n\n```python\nfrom langchain.llms.loading import load_llm\n\nllm = load_llm(\"llm.json\")\n```\n\nFor a YAML-formatted LLM configuration (llm.yaml):\n\n```yaml\n_type: openai\nbest_of: 1\nfrequency_penalty: 0.0\nmax_tokens: 256\nmodel_name: text-davinci-003\nn: 1\npresence_penalty: 0.0\nrequest_timeout: null\ntemperature: 0.7\ntop_p: 1.0\n```\n\nLoad the configuration like this:\n\n```python\nfrom langchain.llms.loading import load_llm\n\nllm = load_llm(\"llm.yaml\")\n```\n\nTo save an LLM configuration from memory to a serialized version, you can use the `.save` method. This method supports both JSON and YAML formats:\n\n```python\nllm.save(\"llm.json\")\nllm.save(\"llm.yaml\")\n```\n\nThis way, you can easily save and load LLM configurations to and from disk."}}, {"inputs": {"question": "in code, how can i add a system message at the end of the conversation history to influence the output of the llm"}, "outputs": {"answer": "\n```python\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{user_input}\"),\n        (\"system\", \"New System Instruction here\"),\n    ]\n)\n```\nWhen formatted, it will generate 1 human/user message and 1 system message.\n```python\nprompt_template.format_messages(user_input=\"Hello\")\n```\n[HumanMessage(content='Hello', additional_kwargs={}, example=False),\n SystemMessage(content='New System Instruction here', additional_kwargs={})]\n"}}, {"inputs": {"question": "How do I cache a Chat model? What caches can I use?"}, "outputs": {"answer": "You can use LangChain's caching layer for Chat Models. This can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times, and speed up your application.\n\n```python\n\nfrom langchain.cache import InMemoryCache\nlangchain.llm_cache = InMemoryCache()\n\n# The first time, it is not yet in cache, so it should take longer\nllm.predict('Tell me a joke')\n\n```\n\nYou can also use SQLite Cache which uses a SQLite database:\n\n```python\n  rm .langchain.db\n\nfrom langchain.cache import SQLiteCache\nlangchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n\n# The first time, it is not yet in cache, so it should take longer\nllm.predict('Tell me a joke') \n```\n"}}, {"inputs": {"question": "Help me rewrite this using chat completion gpt-3.5-turbo-16k model\nimport os\n\nfrom langchain.llms import OpenAI\nfrom langchain.agents import initialize_agent\nfrom langchain.agents.agent_toolkits import ZapierToolkit\nfrom langchain.utilities.zapier import ZapierNLAWrapper\n\nfrom config import OPEN_AI_API_KEY\n\nllm = OpenAI(temperature=0, openai_api_key=OPEN_AI_API_KEY, model=\"gpt-3.5-turbo-16k\")\nzapier = ZapierNLAWrapper(zapier_nla_api_key='key')\ntoolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)\nagent = initialize_agent(toolkit.get_tools(), llm, agent=\"zero-shot-react-description\", verbose=True)\n\nagent.run(\"do i have any important emails\")\n"}, "outputs": {"answer": "Sure, here's how you can rewrite your code to use the ChatOpenAI model instead of OpenAI:\n\n```python\nimport os\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.agents.agent_toolkits import ZapierToolkit\nfrom langchain.utilities.zapier import ZapierNLAWrapper\n\nfrom config import OPEN_AI_API_KEY\n\nllm = ChatOpenAI(temperature=0, openai_api_key=OPEN_AI_API_KEY, model=\"gpt-3.5-turbo-16k\")\nzapier = ZapierNLAWrapper(zapier_nla_api_key='key')\ntoolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)\nagent = initialize_agent(toolkit.get_tools(), llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\nagent.run(\"do i have any important emails\")\n```\n"}}, {"inputs": {"question": "Crawl Langchain Docs and Define Agent"}, "outputs": {"answer": "The core idea of agents is to use an LLM to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order."}}, {"inputs": {"question": "What is the difference between ChatPromptTemplate and PromptTemplate?"}, "outputs": {"answer": "ChatPromptTemplate and PromptTemplate are similar in their functions. PromptTemplate takes a prompt as a string that can contain variables. ChatPromptTemplate separates the prompt into three different kinds. Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role. "}}, {"inputs": {"question": "I am making three requests to the OpenAI API in my Python code, but it is taking 4 times as long to run as compared to last week. Is there something I could be doing wrong?"}, "outputs": {"answer": "There could be several reasons why your requests to the OpenAI API are taking longer than before. However, based on the information provided in the extracts, one possible solution could be to use concurrent execution instead of serial execution. You could use asyncio to execute tasks concurrently. This can significantly speed up the execution time as compared to running tasks serially. Here's the relevant code snippet: \nimport asyncio\nimport time\n\nfrom langchain.agents import initialize_agent, load_tools\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\nfrom langchain.callbacks.stdout import StdOutCallbackHandler\nfrom langchain.callbacks.tracers import LangChainTracer\nfrom aiohttp import ClientSession\n\nquestions = [\n \"Who won the US Open men's final in 2019? What is his age raised to the 0.334 power?\" llm = OpenAI(temperature=0)\ntools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n\ns = time.perf_counter()\n# If running this outside of Jupyter, use asyncio.run or loop.run_until_complete\ntasks = [agent.arun(q) for q in questions]\nawait asyncio.gather(*tasks)\nelapsed = time.perf_counter() - s\nprint(f\"Concurrent executed in {elapsed:0.2f} seconds.\")\nThis code concurrently executes multiple tasks, which can significantly reduce the total execution time. You can adapt this code to your specific use case."}}, {"inputs": {"question": "How can I check the token length of a prompt and memory object"}, "outputs": {"answer": "To check the token length of a prompt and memory object in the context of LangChain, you can utilize the `get_openai_callback` function for token usage tracking. This tracking is currently available for the OpenAI API.\n\nIn an example where you want to track token usage for a single LLM call, you can follow the code snippet below:\n\n```python\nfrom langchain.llms import OpenAI\nfrom langchain.callbacks import get_openai_callback\n\nllm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n\nwith get_openai_callback() as cb:\n    result = llm(\"Tell me a joke\")\n    print(cb)\n\n    Tokens Used: 42\n        Prompt Tokens: 4\n        Completion Tokens: 38\n    Successful Requests: 1\n    Total Cost (USD): $0.00084\n```\n\nInside the context manager, the token usage will be automatically tracked. You can extend this tracking to multiple calls in sequence as shown below:\n\n```python\nwith get_openai_callback() as cb:\n    result = llm(\"Tell me a joke\")\n    result2 = llm(\"Tell me a joke\")\n    print(cb.total_tokens)\n\n    91\n```\n\nIt's worth noting that if you use a chain or agent with multiple steps, the token tracking will cover all those steps. An example involving an agent with multiple steps is provided below:\n\n```python\nfrom langchain.agents import load_tools\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n)\n\nwith get_openai_callback() as cb:\n    response = agent.run(\n        \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n    )\n    print(f\"Total Tokens: {cb.total_tokens}\")\n    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n    print(f\"Completion Tokens: {cb.completion_tokens}\")\n    print(f\"Total Cost (USD): ${cb.total_cost}\")\n```\n\nThis code demonstrates how to track token usage within an agent's execution chain, giving you insights into prompt and completion tokens, as well as the total cost."}}, {"inputs": {"question": "Can I train my model on a specific website?"}, "outputs": {"answer": "Good Question. In LangChain there are many ways of doing so. The easiest would be to use a RecursiveURLLoader, which can scrape all URLs under a root directory. You would chunk the documents you get from the loader and embed them into a Vector Database. When you want to query your trained model, you can retrieve the relevant embeddings from the Vector Database and use them as context for your Chat Model."}}, {"inputs": {"question": "what does llm.run method do?"}, "outputs": {"answer": "LLMs in LangChain do not have a `.run` method"}}, {"inputs": {"question": "whats the best way to use an Anthropic model in an agent?"}, "outputs": {"answer": "You'd want to use the XML agent. Here's some code for doing so:\n\n```python\nfrom langchain.agents import XMLAgent, tool, AgentExecutor\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.chains import LLMChain\nmodel = ChatAnthropic(model='claude-2')\nchain = LLMChain(\nllm=model,\nprompt=XMLAgent.get_default_prompt(),\noutput_parser=XMLAgent.get_default_output_parser()\n)\nagent = XMLAgent(tools=tool_list, llm_chain=chain)\nagent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)\n```"}}, {"inputs": {"question": "What is ConversationSummaryBufferMemory?"}, "outputs": {"answer": "ConversationSummaryBufferMemory is a type of memory module in Langchain that keeps a buffer of recent interactions in memory and compiles them into a summary. It uses token length to determine when to flush interactions."}}, {"inputs": {"question": "What serialization format is used to serialize chains to and from disk?"}, "outputs": {"answer": "The serialization format used is JSON or YAML."}}, {"inputs": {"question": "What is the challenge with retrieval in document storage systems?"}, "outputs": {"answer": "One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses."}}, {"inputs": {"question": "What is the main purpose of the time-weighted vector store retriever?"}, "outputs": {"answer": "The main purpose of the time-weighted vector store retriever is to use a combination of semantic similarity and a time decay to retrieve relevant vectors."}}, {"inputs": {"question": "What are prompt templates?"}, "outputs": {"answer": "Prompt templates are pre-defined recipes for generating prompts for language models."}}, {"inputs": {"question": "What does ConversationBufferWindowMemory do?"}, "outputs": {"answer": "ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."}}, {"inputs": {"question": "What is the purpose of the Self Ask With Search chain?"}, "outputs": {"answer": "The Self Ask With Search chain is designed to facilitate self-questioning by searching for answers within the available documents."}}, {"inputs": {"question": "What is the purpose of the Retry parser?"}, "outputs": {"answer": "The Retry parser is used when the output is not just in the incorrect format, but is partially complete. It is designed to handle cases where parsing mistakes cannot be fixed by only looking at the output."}}, {"inputs": {"question": "What is the purpose of caching embeddings?"}, "outputs": {"answer": "Caching embeddings allows for storing or temporarily caching embeddings to avoid the need for recomputing them."}}, {"inputs": {"question": "What does LangChain offer for token counting?"}, "outputs": {"answer": "LangChain offers a context manager that allows you to count tokens."}}, {"inputs": {"question": "What is ConversationSummaryBufferMemory?"}, "outputs": {"answer": "ConversationSummaryBufferMemory is a memory type that combines the ideas of keeping a buffer of recent interactions and compiling them into a summary. It uses token length to determine when to flush interactions."}}, {"inputs": {"question": "What's the difference between a prompt template and a chat prompt template?"}, "outputs": {"answer": "A prompt template wraps instructions, examples, and other formatting information for the llm's prompts. A chat prompt template provides similar functionality for chat models. The largest difference is that chat prompt templates expose a `format_messages` method to convert the  prompt template into a list of structured chat messages."}}, {"inputs": {"question": "What's an LLMChain"}, "outputs": {"answer": "An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output."}}, {"inputs": {"question": "What's a runnable lambda?"}, "outputs": {"answer": "A runnable lambda is an object that implements LangChain's `Runnable` interface and runs a callbale (i.e., a function). Note the function must accept a single argument."}}, {"inputs": {"question": "I'm runing my own model using vllm. How do I connect it to LangChain?"}, "outputs": {"answer": "LangChain has a VLLM integration:\"\n```python\nfrom langchain.llms import VLLM\n\nllm = VLLM(model=\"mosaicml/mpt-7b\",\n           trust_remote_code=True,  # mandatory for hf models\n           max_new_tokens=128,\n           top_k=10,\n           top_p=0.95,\n           temperature=0.8,\n)\n\nprint(llm(\"What is the capital of France ?\"))"}}, {"inputs": {"question": "how to use enums in Tools that will be used by functions agent?"}, "outputs": {"answer": "If you're defining a dynamic tool (either using the @tool decorator or the `.from_function(...) classmethod), simply using the enum typehint is enough. If you are manually specifying an `args_schema` as a pydantic basemodel, you can specify the enum as the argument typehint there."}}]