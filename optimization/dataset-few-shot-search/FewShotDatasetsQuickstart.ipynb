{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14404e6-59c6-496b-b047-bf9d4f2b72a1",
   "metadata": {},
   "source": [
    "# Few Shot Datasets Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc93bd5-426e-42a5-87cc-ceeca273dc44",
   "metadata": {},
   "source": [
    "Thank you for agreeing to beta test our new product around few shot dynamic example selection!\n",
    "\n",
    "This notebook is designed to help you get started. As a prereq, please do the following:\n",
    "* Set `LANGCHAIN_API_KEY` in your environment\n",
    "* Set `LANGCHAIN_TRACING_V2` to true in your environment\n",
    "* Set `LANGCHAIN_PROJECT` to a unique project for debugging. It's helpful to view langsmith traces to understand how the few shot examples are getting retrieved\n",
    "* Set an `OPENAI_API_KEY` in your environment (or replace the openai calls with another chat model)\n",
    "* Confirm with Jake Rachleff (via email with jake@langchain.dev or via slack) that you have been allowlisted for this feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939e92d-abfa-44d0-8b2c-bec132ad0282",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "When writing your few shot prompt, you have to take into account the input/output schemas of your past data that flowed through your application. In the following example code, our application answers questions about langchain. Our dataset is formulated as follows:\n",
    "\n",
    "### Inputs\n",
    "```\n",
    "{ \"question\": <a question about langchain> }\n",
    "```\n",
    "\n",
    "### Outputs\n",
    "```\n",
    "{ \"answer\": <an answer to the question> }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee475300-025a-49a2-9240-558e9b1725a2",
   "metadata": {},
   "source": [
    "## Before Few Shot Prompting\n",
    "We'll ask a question below that the LLM does not know the answer to. This will show how past examples can increase the LLM's ability to answer questions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dda5a0-982b-4df8-9edf-62561d42c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import Client\n",
    "\n",
    "openai = Client()\n",
    "\n",
    "incoming_question = {\n",
    "    \"question\": \"Why would I use a runnable lambda in LangChain? What does it do?\"\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert on the LLM framework LangChain. Your job is to answer questions provided by users of the framework \n",
    "as succinctly and accurately as possible.\"\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": incoming_question[\"question\"]},\n",
    "]\n",
    "result = openai.chat.completions.create(\n",
    "    messages=messages, model=\"gpt-3.5-turbo\", temperature=0\n",
    ")\n",
    "result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262af6a-c2d1-4587-9ca5-17c80b1bea85",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Make sure you have set up the data.json file that was distributed with this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c36f4-44a4-4ec3-b02b-210f8c2f2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import json \n",
    "\n",
    "langsmith_client = Client()\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "test_dataset_name = \"LangSmith Few Shot Datasets Notebook\"\n",
    "\n",
    "has_dataset = langsmith_client.has_dataset(dataset_name=test_dataset_name)\n",
    "print(has_dataset)\n",
    "if not has_dataset:\n",
    "    dataset_id = langsmith_client.create_dataset(dataset_name=test_dataset_name).id\n",
    "    langsmith_client.create_examples(\n",
    "        dataset_name = test_dataset_name,\n",
    "        inputs=[e['inputs'] for e in examples],\n",
    "        outputs=[e['outputs'] for e in examples],\n",
    "    )\n",
    "else:\n",
    "    dataset_id = list(langsmith_client.list_datasets(dataset_name=test_dataset_name))[0].id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a276f8b9-7cdf-4ed1-b378-3513367e8810",
   "metadata": {},
   "source": [
    "## Indexing your Dataset for Dynamic Example Selection\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ You'll need to access the LangSmith UI to do the following steps! They are available via API\n",
    "    as well of course, but this walk through assumes you do them via UI.\n",
    "</div>\n",
    "\n",
    "1. **Navigate to the Datasets page in LangSmith**\n",
    "2. **Click on the Few-Shot Index button on the top navigation**\n",
    "3. **Wait until the indexing process completes**\n",
    "   Note: you can click on/off the button to make the current indexing state auto-reload. For small datasets this should happen almost instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7378d-610b-4356-b6d4-1b388842a506",
   "metadata": {},
   "source": [
    "## Helper code - feel free to skip\n",
    "This section sets up code that can be used to query a langsmith for few shot examples. A polished version of this will end up in the LangSmith SDK so you can skip to the next section. It will:\n",
    "1. creates a langsmith api client for our REST API\n",
    "2. creates a small helpers for searching our new indexed dataset via REST API. We decorate this to be traced into LangSmith so you can see the example selection process.\n",
    "3. creates a new langchain example selector using this REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0ec8c-0ee8-40b3-9268-2fa594861d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "langsmith_api_client = httpx.Client(\n",
    "    transport=httpx.HTTPTransport(\n",
    "        retries=5,  # this applies only to ConnectError, ConnectTimeout\n",
    "        limits=httpx.Limits(\n",
    "            max_keepalive_connections=20,\n",
    "            keepalive_expiry=120.0,\n",
    "        ),\n",
    "    ),\n",
    "    timeout=httpx.Timeout(\n",
    "        connect=5.0,\n",
    "        read=10.0,\n",
    "        write=30.0,\n",
    "        pool=30.0,\n",
    "    ),\n",
    "    headers={\n",
    "        \"x-api-key\": os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "    },\n",
    "    base_url=\"https://api.smith.langchain.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71df79-74ba-4e5b-9a3d-dd9350cdc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def search_similar_examples(\n",
    "    dataset_id: str, \n",
    "    inputs_dict: dict[str, Any], \n",
    "    limit: int = 5) -> list[dict[str, Any]]:\n",
    "    few_shot_resp = langsmith_api_client.post(\n",
    "        f\"datasets/{dataset_id}/search\",\n",
    "        json={\n",
    "            \"inputs\": inputs_dict,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "    )\n",
    "    few_shot_resp.raise_for_status()\n",
    "    return few_shot_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f31a3-942a-47b9-a207-e40716a45640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import BaseExampleSelector\n",
    "from typing import Dict, List\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "class LangSmithDatasetExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_id,\n",
    "        num_examples: int = 5):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.num_examples = num_examples\n",
    "\n",
    "    def add_example(self, example: Dict[str, str]) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @traceable\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "        return search_similar_examples(\n",
    "            dataset_id=self.dataset_id,\n",
    "            inputs_dict=input_variables,\n",
    "            limit=self.num_examples\n",
    "        )[\"examples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff6067-fae5-425b-b46b-4664f5273ba6",
   "metadata": {},
   "source": [
    "## Setup Shared Parameters\n",
    "Below, we will offer two ways to use few shot examples:\n",
    "1. with LangChain\n",
    "2. without LangChain\n",
    "\n",
    "You can opt to use whichever path you prefer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08892ff-7e43-4769-8562-7c7c4bb3029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 3\n",
    "example_selector = LangSmithDatasetExampleSelector(dataset_id, num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976c4e1-8123-4ae9-8911-b3551ea88414",
   "metadata": {},
   "source": [
    "## Few Shot Examples with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a7a76-2917-4b51-8da9-028515537fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        <example>\n",
    "            <question> {{inputs.question}} </question>\n",
    "            <answer> {{outputs.answer}} </answer>\n",
    "        </example>\n",
    "    \"\"\",\n",
    "    template_format=\"mustache\"\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are an expert on LangChain, an LLM development framework. Answer the question succinctly and correctly.\",\n",
    "    suffix=\"\",\n",
    "    input_variables=[\"question\"]\n",
    "\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate(prompt=prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | ChatOpenAI() | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435b953-e7d9-4bce-8d76-634b5e7da04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(incoming_question):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079888f5-734b-4920-877b-ebed32ec8144",
   "metadata": {},
   "source": [
    "## Few Shot Examples without LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12a618-746b-4aaf-b5ea-c6297814aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please overwrite the prompts as you see fit\n",
    "\n",
    "from langsmith import traceable, wrappers\n",
    "\n",
    "@traceable\n",
    "def generate_example_prompt(example: Dict[str, Any]):\n",
    "    # TODO: FILL IN WITH YOUR FEW SHOT PROMPT\n",
    "    return f\"\"\"\n",
    "        <example>\n",
    "            <question> {example['inputs']['question']}</question>\n",
    "            <answer> {example['outputs']['answer']} </answer>\n",
    "        </example>\n",
    "        \"\"\"\n",
    "\n",
    "# TODO: FILL IN WITH YOUR BASE PROMPT\n",
    "base_prompt = \"\"\"\n",
    "    You are an expert on the LLM framework LangChain. Your job is to answer questions provided by users of the \n",
    "    framework as succinctly and accurately as possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc5c61-3028-49d3-a198-81dbe34ae09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import Client\n",
    "from langsmith import traceable, wrappers\n",
    "\n",
    "openai = wrappers.wrap_openai(Client())\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_few_shot_prompt(inputs: Dict[str, Any]):\n",
    "    examples = search_similar_examples(\n",
    "        dataset_id=dataset_id,\n",
    "        inputs_dict=inputs,\n",
    "        limit=num_examples\n",
    "    )[\"examples\"]\n",
    "\n",
    "    example_prompt = \"\\n\".join(generate_example_prompt(example) for example in examples)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "                You are an expert on LangChain, an LLM development framework. Answer the question succinctly and correctly.\n",
    "\n",
    "                {example_prompt}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": inputs[\"question\"]},\n",
    "    ]\n",
    "    result = openai.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-3.5-turbo\", temperature=0\n",
    "    )\n",
    "    return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c2cad-7214-49da-acb2-74ea7f9dc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_few_shot_prompt(incoming_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc25ef0-64bc-44f6-b5bf-213f27a9a35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
