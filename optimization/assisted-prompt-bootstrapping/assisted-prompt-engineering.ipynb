{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56f6b50-d708-43c5-acd2-ad948cdc1797",
   "metadata": {},
   "source": [
    "# Prompt Bootstrapping with Human Feedback\n",
    "\n",
    "Prompt engineering isn't always the most fun, especially when it comes to tasks where metrics are hard to defined. Crafting a prompt is often an iterative process, and it can be hard to get over the initial \"cold start problem\" of creating good prompts and datasets.\n",
    "\n",
    "Turns out LLMs can do a [decent job at prompt engineering](https://arxiv.org/abs/2211.01910), especially when incorporating human feedback on representative data. For lack of a better term, I'll call this form of prompt optimization \"Prompt Bootstrapping\", since it iteratively refines a prompt via instruction tuning distilled from human feedback. Below is an overview of the process.\n",
    "\n",
    "![Prompt Bootstrapping Diagram](./img/prompt-bootstrapping.png)\n",
    "\n",
    "LangSmith makes this this whole flow very easy. Let's give it a whirl!\n",
    "\n",
    "This example is based on [@alexalbert's example Claude workflow](https://twitter.com/alexalbert__/status/1767258557039378511?s=20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191baa94-41b2-4aaf-b621-aaf8171566d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langsmith langchain_anthropic langchain arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20f9596-dcd5-4928-a6f0-e4f75e1cf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key\n",
    "# We are using Anthropic here as well\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ac85d1-d72f-482e-8488-6a848ae98c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb8d0a-7618-4d93-ae82-ae150a3acc28",
   "metadata": {},
   "source": [
    "# 1. Pick a task\n",
    "\n",
    "Let's say I want to write a tweet generator about academic papers, one that is catchy but not laden with too many buzzwords\n",
    "or impersonal. Let's see if we can \"optimize\" a prompt without having to engineer it ourselves.\n",
    "\n",
    "We will use the meta-prompt ([wfh/metaprompt](https://smith.langchain.com/hub/wfh/metaprompt)) from the Hub to generate our first prompt candidate to solve this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d087d0-4a76-4046-82d7-dddf8361bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "task = (\n",
    "    \"Generate a tweet to market an academic paper or open source project. It should be\"\n",
    "    \" well crafted but avoid gimicks or over-reliance on buzzwords.\"\n",
    ")\n",
    "\n",
    "\n",
    "# See: https://smith.langchain.com/hub/wfh/metaprompt\n",
    "prompt = hub.pull(\"wfh/metaprompt\")\n",
    "llm = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "\n",
    "def get_instructions(gen: str):\n",
    "    return gen.split(\"<Instructions>\")[1].split(\"</Instructions>\")[0]\n",
    "\n",
    "\n",
    "meta_prompter = prompt | llm | StrOutputParser() | get_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b6e6a7b-f3f7-4fe6-87f5-7e78340a8489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is the text of the academic paper or open source project to craft a marketing tweet for:\n",
      "\n",
      "<paper>\n",
      "{paper}\n",
      "</paper>\n",
      "\n",
      "Please follow these steps to generate the tweet:\n",
      "\n",
      "1. Carefully read through the paper/project description. In 1-2 sentences, summarize the key point, main contribution or central finding of the work. Write this summary inside <key_point> tags.\n",
      "\n",
      "2. Next, think about why this work matters. How is it significant to the field? What are the key implications or potential applications? Describe the importance concisely in 1-2 sentences inside <significance> tags.\n",
      "\n",
      "3. Review the paper/project again and pick out 1-2 interesting statistics, impactful quotes, or illustrative examples that help convey the key ideas. Include these inside <highlight> tags.\n",
      "\n",
      "4. Consider relevant hashtags to include that are topical to the subject area (e.g. #MachineLearning, #ClimateChange, #Genomics). Aim for 1-3 hashtags. Also consider any Twitter handles to @mention, such as the authors, lab/organization, or publisher. Include hashtags and mentions inside <hashtags> tags.\n",
      "\n",
      "5. Now, craft the full tweet using the key point, significance, highlights, and hashtags you noted above. Aim for around 280 characters or less. Ensure the writing is clear, concise, and avoids gimmicky marketing language or an over-reliance on buzzwords. Focus on accurately and compellingly conveying the substance of the work.\n",
      "\n",
      "Write the final generated tweet inside <tweet> tags.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommended_prompt = meta_prompter.invoke(\n",
    "    {\n",
    "        \"task\": task,\n",
    "        \"input_variables\": \"\"\"\n",
    "{paper}\n",
    "\"\"\",\n",
    "    }\n",
    ")\n",
    "print(recommended_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac4c64-98f8-43e4-ad63-a7797a9b6396",
   "metadata": {},
   "source": [
    "OK so it's a fine-not-great prompt. Let's see how it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64f33f-328b-4609-b248-fa52214b5b76",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "For some tasks you can generate them yourselves. For our notebook, we have created a 10-datapoint dataset of some scraped ArXiv papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e9ef2ef-d7be-4f41-b947-73407319611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "\n",
    "wrapper = ArxivAPIWrapper(doc_content_chars_max=200_000)\n",
    "docs = list(islice(wrapper.lazy_load(\"Self-Replicating Language model Agents\"), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc80f9da-db1f-41c9-8db1-0f92b3acb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"Tweet Generator\"\n",
    "ds = client.create_dataset(dataset_name=ds_name)\n",
    "client.create_examples(\n",
    "    inputs=[{\"paper\": doc.page_content} for doc in docs], dataset_id=ds.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dda2b0-e4f7-4394-9b65-6f4316399392",
   "metadata": {},
   "source": [
    "## 3. Predict\n",
    "\n",
    "We will refrain from defining metrics for now (it's quite subjective). Instead we will run the first version of the generator against the dataset and manually review + provide feedback on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54cd01c1-8859-4fe7-a8ed-1ef430c475a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def parse_tweet(response: str):\n",
    "    try:\n",
    "        return response.split(\"<tweet>\")[1].split(\"</tweet>\")[0].strip()\n",
    "    except:\n",
    "        return response.strip()\n",
    "\n",
    "\n",
    "def create_tweet_generator(prompt_str: str):\n",
    "    prompt = PromptTemplate.from_template(prompt_str)\n",
    "    return prompt | llm | StrOutputParser() | parse_tweet\n",
    "\n",
    "\n",
    "tweet_generator = create_tweet_generator(recommended_prompt)\n",
    "\n",
    "# Example\n",
    "prediction = tweet_generator.invoke({\"paper\": docs[0].page_content})\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a87a9b2d-a2fe-4e55-bc50-6e7bef4f98ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'ample-price-67' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/096f2d40-c661-404f-8cb1-a384619cc262/compare?selectedSessions=962fc9fc-4f7d-4ccb-acd3-667c37763c47\n",
      "\n",
      "View all tests for Dataset Tweet Generator 2 at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/096f2d40-c661-404f-8cb1-a384619cc262\n",
      "[------------------------------------------------->] 10/10"
     ]
    }
   ],
   "source": [
    "res = client.run_on_dataset(\n",
    "    dataset_name=ds_name,\n",
    "    llm_or_chain_factory=tweet_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9a554-7504-4070-9a2a-ed0c09e656fc",
   "metadata": {},
   "source": [
    "## 4. Label\n",
    "\n",
    "Now, we will use an annotation queue to score + add notes to the results. We will use this to iterate on our prompt!\n",
    "\n",
    "For this notebook, I will be logging two types of feedback:\n",
    "\n",
    "`note`- freeform comments on the runs\n",
    "\n",
    "`tweet_quality` - a 0-4 score of the generated tweet based on my subjective preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94a24156-a2a9-49ee-98e9-f2f3d8f9ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = client.create_annotation_queue(name=\"Tweet Generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6c2406-eb48-4abb-99f0-b8993ce607d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_runs_to_annotation_queue(\n",
    "    q.id,\n",
    "    run_ids=[\n",
    "        r.id\n",
    "        for r in client.list_runs(project_name=res[\"project_name\"], execution_order=1)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae068c97-ced9-4496-b7a5-57213e493325",
   "metadata": {},
   "source": [
    "Now, go through the runs to label them. Return to this notebook when you are finished.\n",
    "\n",
    "![Queue](./img/queue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ed075-38de-4a09-b073-63807039550e",
   "metadata": {},
   "source": [
    "## 4. Update\n",
    "\n",
    "With the human feedback in place, let's update the prompt and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75f8f65e-40af-4c40-8ca4-ca07831b28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def format_feedback(single_feedback, max_score=4):\n",
    "    if single_feedback.score is None:\n",
    "        score = \"\"\n",
    "    else:\n",
    "        score = f\"\\nScore:[{single_feedback.score}/{max_score}]\"\n",
    "    comment = f\"\\n{single_feedback.comment}\".strip()\n",
    "    return f\"\"\"<feedback key={single_feedback.key}>{score}{comment}\n",
    "</feedback>\"\"\"\n",
    "\n",
    "\n",
    "def format_run_with_feedback(run, feedback):\n",
    "    all_feedback = \"\\n\".join([format_feedback(f) for f in feedback])\n",
    "    return f\"\"\"<example>\n",
    "<tweet>\n",
    "{run.outputs[\"output\"]}\n",
    "</tweet>\n",
    "<annotations>\n",
    "{all_feedback}\n",
    "</annotations>\n",
    "</example>\"\"\"\n",
    "\n",
    "\n",
    "def get_formatted_feedback(project_name: str):\n",
    "    traces = list(client.list_runs(project_name=project_name, execution_order=1))\n",
    "    feedbacks = defaultdict(list)\n",
    "    for f in client.list_feedback(run_ids=[r.id for r in traces]):\n",
    "        feedbacks[f.run_id].append(f)\n",
    "    return [\n",
    "        format_run_with_feedback(r, feedbacks[r.id])\n",
    "        for r in traces\n",
    "        if r.id in feedbacks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fd4c807-ef3f-4520-a77f-ad813252f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_feedback = get_formatted_feedback(res[\"project_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c907a-73d3-45c5-812f-9631bb63c21c",
   "metadata": {},
   "source": [
    "LLMs are especially good at 2 things:\n",
    "1. Generating grammatical text\n",
    "2. Summarization\n",
    "\n",
    "Now that we've left a mixture of scores and free-form comments, we can use an \"optimizer prompt\" ([wfh/optimizerprompt](https://smith.langchain.com/hub/wfh/optimizerprompt)) to incorporate the feedback into an updated prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b41aa18b-a99e-4927-a155-83b8cdb4c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://smith.langchain.com/hub/wfh/optimizerprompt\n",
    "optimizer_prompt = hub.pull(\"wfh/optimizerprompt\")\n",
    "\n",
    "\n",
    "def extract_new_prompt(gen: str):\n",
    "    return gen.split(\"<improved_prompt>\")[1].split(\"</improved_prompt>\")[0].strip()\n",
    "\n",
    "\n",
    "optimizer = optimizer_prompt | llm | StrOutputParser() | extract_new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8f42771-a135-44c9-9534-e6f572089d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prompt = recommended_prompt\n",
    "new_prompt = optimizer.invoke(\n",
    "    {\n",
    "        \"current_prompt\": current_prompt,\n",
    "        \"annotated_predictions\": \"\\n\\n\".join(formatted_feedback).strip(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eac1918-1c85-420a-9519-a924a11a8f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prompt\n",
      "\n",
      "\n",
      "\n",
      "Here is the text of the academic paper or open source project to craft a marketing tweet for:\n",
      "\n",
      "<paper>\n",
      "{paper}\n",
      "</paper>\n",
      "\n",
      "Please follow these steps to generate the tweet:\n",
      "\n",
      "1. Carefully read through the paper/project description. In 1-2 sentences, summarize the key point, main contribution or central finding of the work. Write this summary inside <key_point> tags.\n",
      "\n",
      "2. Next, think about why this work matters. How is it significant to the field? What are the key implications or potential applications? Describe the importance concisely in 1-2 sentences inside <significance> tags.\n",
      "\n",
      "3. Review the paper/project again and pick out 1-2 interesting statistics, impactful quotes, or illustrative examples that help convey the key ideas. Include these inside <highlight> tags.\n",
      "\n",
      "4. Consider relevant hashtags to include that are topical to the subject area (e.g. #MachineLearning, #ClimateChange, #Genomics). Aim for 1-3 hashtags. Also consider any Twitter handles to @mention, such as the authors, lab/organization, or publisher. Include hashtags and mentions inside <hashtags> tags.\n",
      "\n",
      "5. Now, craft the full tweet using the key point, significance, highlights, and hashtags you noted above. Aim for around 280 characters or less. Ensure the writing is clear, concise, and avoids gimmicky marketing language or an over-reliance on buzzwords. Focus on accurately and compellingly conveying the substance of the work.\n",
      "\n",
      "Write the final generated tweet inside <tweet> tags.\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "New Prompt\n",
      "\n",
      "Here is the text of the academic paper or open source project to craft a marketing tweet for:\n",
      "\n",
      "<paper>\n",
      "{paper}\n",
      "</paper>\n",
      "\n",
      "Please follow these steps to generate an engaging, informative tweet:\n",
      "\n",
      "1. Carefully read through the paper/project description. Identify the central thesis, key innovations, and main takeaways of the work. \n",
      "\n",
      "2. Craft a compelling opening line to grab the reader's attention and make them want to learn more. Avoid generic phrases like \"New study\" or \"Introducing X\". Instead, lead with an intriguing question, provocative statement, or interesting statistic related to the work. \n",
      "\n",
      "3. Briefly summarize the key details of what makes this work novel - whether that's the methods used, experiments run, results achieved etc. Go beyond just stating the topic and convey specifically what about the approach or findings is unique and noteworthy.\n",
      "\n",
      "4. Explain the significance of the work and implications of the results. Why does this matter? How does it advance the field or open up new possibilities? Clearly convey the \"so what\" to the reader.\n",
      "\n",
      "5. If relevant, include 1-2 key statistics, quotes or examples to illustrate the main ideas. Choose ones that are especially attention-grabbing or memorable.\n",
      "\n",
      "6. If appropiate, include 1-2 highly relevant hashtags (e.g. #MachineLearning, #ClimateChange) and/or @mentions of the authors, lab, or publisher. Only include if they are directly topical and useful for categorizing the content or highlighting sources. Do not force hashtags/mentions if they feel unnatural.\n",
      "\n",
      "7. Aim for a total tweet length of around 200-280 characters. Format the tweet in 2-3 short paragraphs for easy readability and logical flow.\n",
      "\n",
      "8. Close with a direct, concise assessment of how impactful or potentially paradigm-shifting you believe this work to be based on your analysis. Avoid exaggeration or hype, but candidly convey if you think this is an especially groundbreaking or important paper.\n",
      "\n",
      "Write the final generated tweet inside <tweet> tags. Remember to focus on really grabbing the reader's attention, clearly conveying the key details and significance, and offering an authentic assessment of the importance of the work. Let your enthusiasm shine through while sticking to the facts.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Prompt\\n\\n\" + current_prompt)\n",
    "print(\"*\" * 80 + \"\\nNew Prompt\\n\\n\" + new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316756d0-30d9-45bd-a3f4-e737bf4430de",
   "metadata": {},
   "source": [
    "## 5. Repeat!\n",
    "\n",
    "Now that we have an \"upgraded\" prompt, we can test it out again and repeat until we are satisfied with the result.\n",
    "\n",
    "If you find the prompt isn't converging to something you want, you can manually update the prompt (you are the optimizer in this case) and/or be more explicit in your free-form note feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2aef764-ef56-4e21-83ac-f51f7f0e7112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'drab-crate-45' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/096f2d40-c661-404f-8cb1-a384619cc262/compare?selectedSessions=8b1bde1d-47d7-4085-92c8-274193cd1855\n",
      "\n",
      "View all tests for Dataset Tweet Generator 2 at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/096f2d40-c661-404f-8cb1-a384619cc262\n",
      "[------------------------------------------------->] 10/10"
     ]
    }
   ],
   "source": [
    "tweet_generator = create_tweet_generator(new_prompt)\n",
    "\n",
    "updated_results = client.run_on_dataset(\n",
    "    dataset_name=ds_name,\n",
    "    llm_or_chain_factory=tweet_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58ac7137-a6c1-4c5a-aacb-cc1bd66b0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_runs_to_annotation_queue(\n",
    "    q.id,\n",
    "    run_ids=[\n",
    "        r.id\n",
    "        for r in client.list_runs(\n",
    "            project_name=updated_results[\"project_name\"], execution_order=1\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ed160-28d4-493e-8e10-6420ded71e1a",
   "metadata": {},
   "source": [
    "Then review/provide feedback/repeat.\n",
    "\n",
    "Once you've provided feedback, you can continue here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f403a5f8-00b2-4d20-8d3b-b5f1cf050718",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_feedback = get_formatted_feedback(updated_results[\"project_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5103da25-8b28-44dc-b816-a972cff3c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap them out\n",
    "current_prompt = new_prompt\n",
    "new_prompt = optimizer.invoke(\n",
    "    {\n",
    "        \"current_prompt\": current_prompt,\n",
    "        \"annotated_predictions\": \"\\n\\n\".join(formatted_feedback).strip(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5e46e96-ad32-4251-8e51-72d9bf582a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Prompt\n",
      "\n",
      "Your task is to write an engaging tweet to market the following academic paper or open source project:\n",
      "\n",
      "<paper>\n",
      "{paper}\n",
      "</paper>\n",
      "\n",
      "Here are the key elements to include in your tweet:\n",
      "\n",
      "1. Concisely summarize the most important new findings from the paper. Focus on the key advances rather than the minutiae of the methodology. \n",
      "\n",
      "2. Explain why the findings are important or how they could lead to new applications. Avoid overhyped terms like \"breakthrough\" and focus on clear, grounded explanations using concrete examples where possible.\n",
      "\n",
      "3. Briefly define any essential technical jargon that appears in your summary (e.g. what are \"spintronic devices\"?). Aim to make the significance of the work accessible to a broad audience.\n",
      "\n",
      "4. Credit the authors and/or their institutions, and include a link to the paper/project.\n",
      "\n",
      "Feel free to use more than 280 characters to achieve a good balance of being punchy and interesting while also including key context and explanations. But don't let it become too dense or long-winded.\n",
      "\n",
      "Here's an example of the kind of tone and content to aim for:\n",
      "\n",
      "<tweet>\n",
      "New record: 29.5% efficiency solar cells achieved by stacking perovskite-silicon tandem cell. That's 4% higher than standard Si cells! Promising path to ultra-high efficiency at lower costs. Perovskites are semiconductors that can be \"tuned\" to absorb different light colors.\n",
      "https://doi.org/10.1038/s41586-023-46736-2\n",
      "By M. Jeong et al. @SomaiyaVidyavihar \n",
      "</tweet>\n",
      "\n",
      "Don't copy this example directly, but use it as a guide for the style and level of detail to try to emulate for the paper provided.\n",
      "\n",
      "Write out the full text of your proposed tweet inside <tweet> tags.\n",
      "********************************************************************************\n",
      "New Prompt\n",
      "\n",
      "Your task is to write an engaging tweet to market the following academic paper or open source project:\n",
      "\n",
      "<paper>\n",
      "{paper}\n",
      "</paper>\n",
      "\n",
      "Structure your tweet in the following way:\n",
      "\n",
      "1. Identify the key problem or opportunity that the new work addresses. This should be the attention-grabbing \"hook\" that leads your tweet.\n",
      "\n",
      "2. Concisely explain what the researchers did and how they did it. Focus on the most important and novel aspects of their work. Avoid getting bogged down in methodological details.\n",
      "\n",
      "3. Briefly explain any essential technical concepts to make the significance of the work more accessible to a broad audience. Use plain language rather than jargon where possible.\n",
      "\n",
      "4. Discuss the potential impact and applications of the work moving forward. Be specific and grounded rather than making hyperbolic claims. Explain how this work could lead to advances in the field.\n",
      "\n",
      "5. Provide a link to the paper or project, and credit the authors and their institutions. \n",
      "\n",
      "Aim for a total tweet length of 250-350 characters. Focus on being punchy and compelling while still including key details and context.\n",
      "\n",
      "Here's an example of a strong tweet following this structure:\n",
      "\n",
      "<tweet>\n",
      "Room-temp ferromagnetic semiconductor could revolutionize spintronics and quantum computing. Zhang & Sui grew graphene on nickel, opening a bandgap & splitting electron spins. Harnesses graphene's unique properties in new ways for nonvolatile memory & more.\n",
      "https://doi.org/10.1038/s41563-023-01587-y\n",
      "@BeijingNormalU @TsinghuaUni\n",
      "</tweet>\n",
      "\n",
      "Use this as a guide for the style and content to aim for. Write your tweet inside <tweet> tags.\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous Prompt\\n\\n\" + current_prompt)\n",
    "print(\"*\" * 80 + \"\\nNew Prompt\\n\\n\" + new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b952f0d-a3e2-4e09-99eb-af75c03416f2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats! You've \"optimized\" a prompt on a subjective task using human feedback and an automatic prompt engineer flow. LangSmith makes it easy to score and improve LLM systems even when it is hard to craft a hard metric.\n",
    "\n",
    "You can push the optimized version of your prompt to the hub (here and in future iterations) to version each change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "817d82e4-98b4-4902-9f29-05df7a21621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/hub/wfh/academic-tweet-generator/03670db0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub.push(\"wfh/academic-tweet-generator\", PromptTemplate.from_template(new_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adabff-26ff-48ec-8faf-d0d43031f2c0",
   "metadata": {},
   "source": [
    "#### Extensions:\n",
    "\n",
    "We haven't optimized the meta-prompts above - feel free to make them your own by forking and updating them!\n",
    "Some easy extensions you could try out include:\n",
    "1. Including the full history of previous prompts and annotations (or most recent N prompts with feedback) in the \"optimizer prompt\" step. This may help it better converge (especially if you're using a small dataset)\n",
    "2. Updating the optimizer prompt to encourage usage of few-shot examples, or to encourage other prompting tricks.\n",
    "3. Incorporating an LLM judge by including the annotation few-shot examples and instructing it to critique the generated outputs: this could help speed-up the human annotation process.\n",
    "4. Generating and including a validation set (to avoid over-fitting this training dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
