{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56f6b50-d708-43c5-acd2-ad948cdc1797",
   "metadata": {},
   "source": [
    "# Bootstrap Few-shot Prompting with LangSmith\n",
    "\n",
    "Prompt engineering is a pain. You can use _examples_ to optimize the prompt for you with the help of tools like LangSmith. Instead of guessing which examples will be the most impactful, you can use tried-and-true evaluation practices to curate and compile the right examples for your pipeline. The main steps are:\n",
    "\n",
    "1. Create a dataset\n",
    "2. Pick a metric to improve\n",
    "3. Create an initial system\n",
    "4. Decide the update logic (few-shot examples vs. instruction teaching vs. other methods, how to format the examples, etc.)\n",
    "5. Train!\n",
    "\n",
    "\n",
    "Below is an example bootstrapping a gpt-3.5-turbo model on an entailment task using few-shot examples. This example inspired by Christopher Potts' [example](https://github.com/stanfordnlp/dspy/blob/main/examples/nli/scone/scone.ipynb) on the SCONE dataset.\n",
    "\n",
    "The task is natural language inference, where the LLM is required to predict whether the a statement can be logically concluded from a premise / grounding statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191baa94-41b2-4aaf-b621-aaf8171566d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langsmith langchain langchain_openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20f9596-dcd5-4928-a6f0-e4f75e1cf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR API KEY\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4cf23c-c99f-4206-99cc-2b2020fa5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461b3958-1b1a-47aa-a2f8-02c6119eb2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs {'context': 'A man who does not walk confidently dropping produce.', 'question': 'Can we logically conclude for sure that a man who does not walk confidently dropping kale?'}\n",
      "outputs {'answer': 'No', 'category': 'one_not_scoped'}\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "public_datasets = [\n",
    "    \"https://smith.langchain.com/public/1d065de2-56c1-496e-bc66-bdce308e6537/d\",  # train\n",
    "    \"https://smith.langchain.com/public/3205fa05-bd78-4eaf-924f-96df0f577b1f/d\",  # train2\n",
    "    \"https://smith.langchain.com/public/fdf16166-1edd-418f-b777-3af82034931d/d\",  # dev\n",
    "    \"https://smith.langchain.com/public/aee61506-3c60-4ca8-95c4-0314c9719ca8/d\",  # dev2\n",
    "    \"https://smith.langchain.com/public/8d40d210-f8e6-4def-a206-78c5080c5d53/d\",  # test\n",
    "]\n",
    "for ds in public_datasets:\n",
    "    client.clone_public_dataset(ds)\n",
    "train_name = \"scone-train2\"\n",
    "dev_name = \"scone-dev2\"\n",
    "test_name = \"scone-test-one-scoped\"\n",
    "full_test_name = \"scone-test\"\n",
    "\n",
    "example = next(client.list_examples(dataset_name=train_name))\n",
    "print(\"inputs\", example.inputs)\n",
    "print(\"outputs\", example.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc278868-1fe8-4a24-845c-7095607f3a88",
   "metadata": {},
   "source": [
    "Reviewing the values above, these examples can be tricky! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810e70c-9518-4bf4-a50f-bd52296b145d",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Since we have ground-truth clasification labels, we can use an exact-match criterion as our evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33084e53-ba37-4274-892c-f4d02ce3c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import langsmith as ls\n",
    "\n",
    "\n",
    "def exact_match(run, example):\n",
    "    # Evaluate the exact match correctness of the NLI result\n",
    "    try:\n",
    "        predicted = run.outputs[\"is_entailed\"]\n",
    "        expected = example.outputs[\"answer\"]\n",
    "        score = expected.lower() == predicted.lower()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            expected = example.outputs[\"answer\"]\n",
    "            expected_bool = {\"no\": False, \"yes\": True}.get(expected.strip().lower())\n",
    "            score = run.outputs[\"output\"].is_entailed == expected_bool\n",
    "        except Exception as e2:\n",
    "            score = 0\n",
    "    return {\n",
    "        \"key\": \"exact_match\",\n",
    "        \"score\": int(score),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "012b34ab-e6eb-4d76-a55f-24bbdcd97d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# And we will create a placeholder in the template to add few-shot examples\n",
    "original_prompt_str = \"\"\"You are given some context (a premise) and a question (a hypothesis). You must indicate with Yes/No answer whether we can logically conclude the hypothesis from the premise.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Context: ${{context}}\n",
    "\n",
    "Question: ${{question}}\n",
    "\n",
    "Reasoning: Let's think step by step in order to ${{produce the answer}}. We ...\n",
    "\n",
    "Answer: Yes or No\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Reasoning: Let's think step by step in order to\"\"\"\n",
    "base_prompt = PromptTemplate.from_template(original_prompt_str)\n",
    "\n",
    "\n",
    "def parse(pred: str):\n",
    "    fnd = \"\\nAnswer:\"\n",
    "    idx = pred.find(fnd)\n",
    "    answer = pred[idx + len(fnd) :].strip()\n",
    "    return {\"is_entailed\": answer, \"reasoning\": pred[:idx].strip()}\n",
    "\n",
    "\n",
    "chain = base_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser() | parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073c51af-1a53-4530-885d-099f56bf27fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_entailed': 'No',\n",
       " 'reasoning': 'produce the answer. We are given that the man does not walk confidently and drops produce. We are not specifically told that he drops kale. It could be any type of produce. Therefore, we cannot logically conclude for sure that he drops kale.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = chain.invoke(example.inputs)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dda2b0-e4f7-4394-9b65-6f4316399392",
   "metadata": {},
   "source": [
    "## Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a9b2d-a2fe-4e55-bc50-6e7bef4f98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ls.evaluate(\n",
    "    chain.invoke,\n",
    "    data=\"scone-test2\",\n",
    "    evaluators=[exact_match],\n",
    "    metadata={\"optimizer\": None},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9236d-a213-4cf6-815e-fd05a248105a",
   "metadata": {},
   "source": [
    "Got about ~55% on it. Definitely room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192913d-e9c1-4f00-aa41-badc64c6b21d",
   "metadata": {},
   "source": [
    "## ✨ Optimize ✨\n",
    "\n",
    "\n",
    "This just means to \"use data to update the system\". At present, LangChain runnables don't natively support a \"backwards\" method (a la pytorch), but you can pretty easily define updates/mutations for key important components you'd want to update, (such as prompts or LLMs).\n",
    "\n",
    "For instance, component-wise, you could apply:\n",
    "- Few shot prompting: add an additional string input or MessagesPlaceholder in the prompt template\n",
    "- Updating the instructions: update the prompt template directly (likely the system prompt)\n",
    "- LLM: do a backwards pass.\n",
    "\n",
    "We will focus on few-shot prompting to limit the search space. We will then apply a genetic/evolutionary algorithm to compare performance of different few-shot examples and pick the ones that provide the most \"lift\" of the provided metric.\n",
    "\n",
    "We'll first create a constructor for our chain that accepts the few-shot examples, letting us re-create the chain with each updated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d50bf47b-0c46-422f-818e-a9d58bac3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define how we want our few-shot examples to be formatted\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def format_example(example: dict):\n",
    "    inputs = example[\"input\"]\n",
    "    outputs = example[\"output\"]\n",
    "    return f\"\"\"\n",
    "\n",
    "Context: {inputs['context']}\n",
    "\n",
    "Question: {inputs['question']}\n",
    "\n",
    "Reasoning: {outputs['reasoning']}\n",
    "\n",
    "Answer: {outputs['is_entailed']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_chain(prompt=None, llm=None):\n",
    "    if prompt is None:\n",
    "        prompt = base_prompt\n",
    "    else:\n",
    "        prompt = PromptTemplate.from_template(prompt)\n",
    "    llm = llm or ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    chain = (prompt | llm | StrOutputParser() | parse).with_config(tags=[\"to_train\"])\n",
    "    return chain.invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1c37c-6f85-465f-adf9-3f0a8548be81",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Next, we'll define the training utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c00b2973-0ec2-468a-abf5-b7f481d85eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt = \"\"\"You are an expert prompt engineer tasked with improving prompts for various AI tasks. You will be provided with the current prompt and a set of annotated predictions made by an AI using this prompt. Your goal is to analyze the results and create an improved version of the prompt.\n",
    "\n",
    "## Current Prompt\n",
    "<current_prompt>\n",
    "{current_prompt}\n",
    "</current_prompt>\n",
    "## Analysis\n",
    "Carefully analyze the current prompt and the annotated predictions. Identify elements of the prompt that seem to lead to high-quality responses versus low-quality responses. Pay close attention to any user-provided scores, feedback, or notes.\n",
    "\n",
    "## Brainstorming\n",
    "<task>\n",
    "First, state the task the original prompt is trying to perform.\n",
    "</task>\n",
    "<brainstorm>\n",
    "Then, brainstorm what should be im proved. Some example aspects you can address (depending on the task)\n",
    "1. Clarity: How can you make the instructions clearer (without loss of generality)?\n",
    "2. Context: What additional context is the current prompt missing that would help it avoid errors identified by the annotated feedback above?\n",
    "3. Constraints: Are there any constraints that need to be added or modified?\n",
    "4. Structure: How can you improve the prompt's structure or formatting?\n",
    "5. Focus: What changes could help the AI focus on the most critical aspects of the task?\n",
    "6. Task-specific considerations: What unique elements of this task need to be addressed?\n",
    "</brainstorm>\n",
    "<plan>\n",
    "After brainstorming, create a plan of proposed edits. Cite specific annotated scores or notes that would be improved using the fixed instructions.\n",
    "</plan>\n",
    "<improved_prompt>\n",
    "Finally, write the improved prompt.\n",
    "</improved_prompt>\n",
    "\n",
    "## Important Notes\n",
    "1. All variables in double curly braces {{variable_name}} are placeholders for input that MUST be retained in the new version of the prompt. These represent task-critical information that the AI needs access to.\n",
    "2. Ensure that your improved prompt is clear, concise, and tailored to the specific task at hand.\n",
    "3. Consider the appropriate length for the expected output and provide guidance on this in your prompt if necessary.\n",
    "4. If the task involves sensitive or controversial topics, include appropriate guidelines for handling such content responsibly.\n",
    "\n",
    "Remember, your goal is to create a prompt that will consistently guide the AI to generate high-quality responses across the entire dataset (and when generalizing beyond the dataset).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c3fc234-9404-4196-8238-3b4cdc89da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def format_feedback(single_feedback, max_score=1):\n",
    "    \"\"\"\n",
    "    Formats a single feedback item into a structured string.\n",
    "\n",
    "    This function takes a feedback object and an optional maximum score,\n",
    "    then formats the feedback's score (if present) and comment into a structured\n",
    "    string representation. The feedback's key is used as an identifier in the\n",
    "    output string.\n",
    "\n",
    "    Parameters:\n",
    "    - single_feedback (object): An object representing a single piece of feedback.\n",
    "                                It must have `score`, `comment`, and `key` attributes.\n",
    "    - max_score (int, optional): The maximum possible score that can be assigned to\n",
    "                                 feedback. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "    - str: A structured string representation of the feedback, including the key,\n",
    "           score (if available), and comment.\n",
    "    \"\"\"\n",
    "    if single_feedback.score is None:\n",
    "        if single_feedback.value is not None:\n",
    "            val = f\"Value: {single_feedback.value}\"\n",
    "        else:\n",
    "            val = \"\"\n",
    "    else:\n",
    "        val = f\"\\nScore:[{single_feedback.score}/{max_score}]\"\n",
    "    comment = f\"{single_feedback.comment}\".strip()\n",
    "    if comment:\n",
    "        comment = f\"\\n{comment}\"\n",
    "    return f\"\"\"<feedback key={single_feedback.key}>{val}{comment}\n",
    "</feedback>\"\"\"\n",
    "\n",
    "\n",
    "def format_run_with_feedback(run, feedback, id):\n",
    "    \"\"\"\n",
    "    Formats the output of a run along with its associated feedback into a structured string.\n",
    "\n",
    "    This function takes a run object and a list of feedback objects associated with that run,\n",
    "    then formats the run's output and the feedback into a structured string representation\n",
    "    suitable for display or further processing.\n",
    "\n",
    "    Parameters:\n",
    "    - run (object): An object representing a single run. Must have an `outputs` attribute\n",
    "                    that contains a dictionary with an `\"output\"` key.\n",
    "    - feedback (list): A list of feedback objects to be formatted and included with the run's output.\n",
    "\n",
    "    Returns:\n",
    "    - str: A structured string representation of the run's output and associated feedback.\n",
    "    \"\"\"\n",
    "    all_feedback = \"\\n\".join([format_feedback(f) for f in feedback])\n",
    "    return f\"\"\"<example id={id}>\n",
    "<input>\n",
    "{run.inputs}\n",
    "</input>\n",
    "<prediction>\n",
    "{run.outputs}\n",
    "</prediction>\n",
    "<annotations>\n",
    "{all_feedback}\n",
    "</annotations>\n",
    "</example>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dd7dec10-8c36-4e0b-aacd-dae0d70f51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic.v1 import BaseModel, Field, root_validator\n",
    "from trustcall import create_extractor\n",
    "\n",
    "\n",
    "def optimize(current_prompt, annotated_predictions):\n",
    "    input_variables = list(PromptTemplate.from_template(current_prompt).input_variables)\n",
    "\n",
    "    class OptimizerOutput(BaseModel):\n",
    "        \"\"\"Think step-by-step, then write the optimized prompt.\"\"\"\n",
    "\n",
    "        task_objective: str = Field(\n",
    "            description=\"What task is this prompt seeking to solve? What defines success here?\"\n",
    "        )\n",
    "        brainstorm: str = Field(\n",
    "            description=\"At least 3 bullet points brainstorming how to improve the prompt. Can focus on logical/correctness, style, or any other qualities that are salient, given the provided annotations.\"\n",
    "        )\n",
    "        plan: str = Field(\n",
    "            description=\"Proposed edits and citations on which feedback will be improved.\"\n",
    "        )\n",
    "        improved_prompt: str = Field(\n",
    "            description=f\"The full text of the optimized prompt. Ensure that all the curly bracket {{variable_name}}'s are retained in the new prompt. These are: {input_variables}.\"\n",
    "        )\n",
    "\n",
    "        @root_validator\n",
    "        def check_prompt_strings(cls, values):\n",
    "            predicted_input_variables = list(\n",
    "                PromptTemplate.from_template(\n",
    "                    values.get(\"improved_prompt\") or \"\"\n",
    "                ).input_variables\n",
    "            )\n",
    "            missing = set(input_variables) - set(predicted_input_variables)\n",
    "            extra = set(predicted_input_variables) - set(input_variables)\n",
    "            if missing or extra:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected variables included in output prompt. Expected {input_variables}. Got: {predicted_input_variables}.\\nMissing: {missing}\\nExtra: {extra}\"\n",
    "                )\n",
    "            return values\n",
    "\n",
    "    optim_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", meta_prompt),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"\"\"Given the following annotated/evaluated predictions, optimize the provided prompt.\n",
    "    <annotated_predictions>\n",
    "    {annotated_predictions}\n",
    "    </annotated_predictions>\n",
    "    \n",
    "    Remember to first brainstorm, then plan, and finally generate the optimized prompt. Remember to retain all bracketed variable placeholders.\"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    meta_optimizer = optim_prompt | create_extractor(\n",
    "        ChatAnthropic(model=\"claude-3-5-sonnet-20240620\"),\n",
    "        tools=[OptimizerOutput],\n",
    "        tool_choice=OptimizerOutput.__name__,\n",
    "    )\n",
    "    results = meta_optimizer.invoke(\n",
    "        {\n",
    "            \"annotated_predictions\": annotated_predictions,\n",
    "            \"current_prompt\": current_prompt,\n",
    "        }\n",
    "    )\n",
    "    return results[\"responses\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2333f9b5-ab0b-48f5-ad2c-8a81c79c2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.jupyter import print as richprint\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "\n",
    "\n",
    "def colorize_diff(diff):\n",
    "    for op, i1, i2, j1, j2 in diff.get_opcodes():\n",
    "        if op == \"equal\":\n",
    "            yield diff.a[i1:i2]\n",
    "        elif op == \"insert\":\n",
    "            yield f\"[green]{diff.b[j1:j2]}[/green]\"\n",
    "        elif op == \"delete\":\n",
    "            yield f\"[red]{diff.a[i1:i2]}[/red]\"\n",
    "        elif op == \"replace\":\n",
    "            yield f\"[red]{diff.a[i1:i2]}[/red][green]{diff.b[j1:j2]}[/green]\"\n",
    "\n",
    "\n",
    "def print_rich_diff(original, updated, title: str = \"\"):\n",
    "    diff = SequenceMatcher(None, original, updated)\n",
    "    colorized_diff = \"\".join(colorize_diff(diff))\n",
    "    panel = Panel(\n",
    "        colorized_diff, title=title or \"Prompt Diff\", expand=False, border_style=\"bold\"\n",
    "    )\n",
    "\n",
    "    richprint(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "709d4bdf-e452-45d7-a97d-e2e8e1dd595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def step(\n",
    "    construct_chain,\n",
    "    prompt: str,\n",
    "    train_examples,\n",
    "    evaluators,\n",
    "    step_idx,\n",
    ") -> str:\n",
    "    # TODO: Batching to speed it up\n",
    "    chain = construct_chain(prompt)\n",
    "    results = ls.evaluate(\n",
    "        chain, data=train_examples[:15], evaluators=evaluators, blocking=False\n",
    "    )\n",
    "    formatted = []\n",
    "    for idx, res in enumerate(results):\n",
    "        formatted.append(\n",
    "            format_run_with_feedback(\n",
    "                res[\"run\"], res.get(\"evaluation_results\", {}).get(\"results\") or [], idx\n",
    "            )\n",
    "        )\n",
    "    updated = optimize(\n",
    "        current_prompt=prompt, annotated_predictions=\"\\n\".join(formatted)\n",
    "    )\n",
    "    new_prompt = updated.improved_prompt\n",
    "    print_rich_diff(prompt or \"\", new_prompt, f\"Prompt diff at step {step_idx}\")\n",
    "    # Now prin\n",
    "    return new_prompt\n",
    "\n",
    "\n",
    "def eval(eval_dataset, system, evaluators, step_n) -> float:\n",
    "    \"\"\"Compute the metrics on the validation dataset.\"\"\"\n",
    "    dev_results = ls.evaluate(\n",
    "        system,\n",
    "        # TODO: do whole\n",
    "        data=itertools.islice(\n",
    "            ls.Client().list_examples(dataset_name=eval_dataset), 0, 15\n",
    "        ),\n",
    "        evaluators=evaluators,\n",
    "        metadata={\n",
    "            \"step\": step_n,\n",
    "        },\n",
    "    )\n",
    "    scores = []\n",
    "    for res in dev_results:\n",
    "        scores.append(res[\"evaluation_results\"][\"results\"][0].score)\n",
    "    # Assume single metric rn ha\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def train(\n",
    "    chain_constructor,\n",
    "    original,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    evaluators,\n",
    "    steps: int = 5,\n",
    "):\n",
    "    \"\"\"Run the full training loop\"\"\"\n",
    "    best_score = eval(eval_dataset, chain_constructor(original), evaluators, 0)\n",
    "    best_step = 0\n",
    "    scores = [(best_score, [])]\n",
    "    train_examples = list(ls.Client().list_examples(dataset_name=train_dataset))\n",
    "    updated = original\n",
    "    for step_number in range(steps):\n",
    "        updated = step(\n",
    "            chain_constructor, updated, train_examples, evaluators, step_idx=step_number\n",
    "        )\n",
    "        updated_chain = chain_constructor(updated)\n",
    "        updated_score = eval(eval_dataset, updated_chain, evaluators, step_number + 1)\n",
    "        scores.append((updated_score, updated))\n",
    "\n",
    "        if updated_score > best_score:\n",
    "            print(\n",
    "                f\"New best score {updated_score} > {best_score}. Updating selected examples.\"\n",
    "            )\n",
    "            best_score = updated_score\n",
    "            best_step = step_number + 1\n",
    "        else:\n",
    "            print(f\"Underperformed ({updated_score} < {best_score}). Continuing\")\n",
    "    print(\"Best overall score: \", best_score)\n",
    "    print(\"Best step: \", best_step)\n",
    "    return sorted(scores, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7afc25-7fb5-47a6-b233-013b2e02bef9",
   "metadata": {},
   "source": [
    "#### Train\n",
    "\n",
    "Now we can finally run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb42676-8fb1-4eca-bf50-29f9d2225ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'elderly-payment-30' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/0360d4ae-63e0-4e58-b4a7-97f8ef466aaa/compare?selectedSessions=7ad95300-319a-4f8a-8a56-bb9f2ace11aa\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dda891c3616414183e2d79fcd0660d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ample-nail-98' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a1613752-8df2-4e05-be08-9af1a2b55f60/compare?selectedSessions=13926443-ff28-4627-8e0b-ebbc7da41011\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f07e768dcc405abad9ef9b867541d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">╭───────────────────────────────────────────── Prompt diff at step 0 ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"font-weight: bold\">│</span> You are <span style=\"color: #008000; text-decoration-color: #008000\">a logical reasoning expert. Your task is to determine whether a </span>given <span style=\"color: #800000; text-decoration-color: #800000\">some </span><span style=\"color: #008000; text-decoration-color: #008000\">hypothesis (question) can be</span> <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">logically concluded from a given premise (</span>context<span style=\"color: #008000; text-decoration-color: #008000\">). Answer with Yes or No and provide step-by-step reasoning.</span>   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Instructions:</span>                                                                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">1. Carefully analyze the context and question, identifying key terms and their relationships.</span>                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">2. Consider only the information explicitly provided in the context. Do not use general knowledge or make </span>      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">assumptions beyond what is given.</span>                                                                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">3. Pay special attention to:</span>                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">   - Negations</span> (<span style=\"color: #800000; text-decoration-color: #800000\">a premise</span><span style=\"color: #008000; text-decoration-color: #008000\">e.g., \"not\", \"isn't\"</span>)<span style=\"color: #800000; text-decoration-color: #800000\"> and a </span>                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">   - Generalizations (e.g., \"all\", \"every\", \"any\")</span>                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">   - Specific vs. general terms (e.g., \"fedora\" is a specific type of \"hat\")</span>                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">4. A logical conclusion must be 100% certain based on the given information.</span>                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">5. Provide clear, step-by-step reasoning for your answer.</span>                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Example of valid conclusion:</span>                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Context: All dogs are animals. Fido is a dog.</span>                                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Question: Can we logically conclude that Fido is an animal?</span>                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Answer: Yes</span>                                                                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Example of invalid conclusion:</span>                                                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Context: Some birds can fly.</span>                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Question: Can we logically conclude that all birds can fly?</span>                                                     <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Answer: No</span>                                                                                                      <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Input Format:</span>                                                                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Context: {context}</span>                                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Question: {</span>question<span style=\"color: #800000; text-decoration-color: #800000\"> (a hypothesis). You must indicate</span><span style=\"color: #008000; text-decoration-color: #008000\">}</span>                                                          <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Output Format:</span>                                                                                                  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Reasoning: Let's think step by step to determine if</span> w<span style=\"color: #800000; text-decoration-color: #800000\">ith </span><span style=\"color: #008000; text-decoration-color: #008000\">e can logically conclude the hypothesis from the given</span> <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">premise:</span>                                                                                                        <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">1. [First step in reasoning]</span>                                                                                    <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">2. [Second step in reasoning]</span>                                                                                   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">...</span>                                                                                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">[Final step in reasoning]</span>                                                                                       <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">Answer: [</span>Yes/No<span style=\"color: #800000; text-decoration-color: #800000\"> ans</span><span style=\"color: #008000; text-decoration-color: #008000\">]</span>                                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #008000; text-decoration-color: #008000\">No</span>w<span style=\"color: #800000; text-decoration-color: #800000\">er </span><span style=\"color: #008000; text-decoration-color: #008000\">, please provide your reasoning and ans</span>w<span style=\"color: #800000; text-decoration-color: #800000\">hether </span><span style=\"color: #008000; text-decoration-color: #008000\">er for the follo</span>w<span style=\"color: #800000; text-decoration-color: #800000\">e can logically conclude the hypothesis </span>  <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">from the premise.</span>                                                                                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">---</span>                                                                                                             <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">Follow the following format.</span><span style=\"color: #008000; text-decoration-color: #008000\">ing:</span>                                                                                <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Context: <span style=\"color: #800000; text-decoration-color: #800000\">$</span>{<span style=\"color: #800000; text-decoration-color: #800000\">{</span>context}<span style=\"color: #800000; text-decoration-color: #800000\">}</span>                                                                                           <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Question: <span style=\"color: #800000; text-decoration-color: #800000\">${</span>{question}<span style=\"color: #800000; text-decoration-color: #800000\">}</span>                                                                                         <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> Reasoning: Let's think step by step <span style=\"color: #800000; text-decoration-color: #800000\">in order to ${{produce the ans</span><span style=\"color: #008000; text-decoration-color: #008000\">to determine if </span>we<span style=\"color: #800000; text-decoration-color: #800000\">r}}. We ...</span>                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">Answer: Yes or No</span>                                                                                               <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">Context: {context}</span>                                                                                              <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">Question: {question}</span>                                                                                            <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span>                                                                                                                 <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">Reasoning: Let's think step by step in order to</span><span style=\"color: #008000; text-decoration-color: #008000\"> can logically conclude the hypothesis from the given premise:</span>   <span style=\"font-weight: bold\">│</span>\n",
       "<span style=\"font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m╭─\u001b[0m\u001b[1m────────────────────────────────────────────\u001b[0m\u001b[1m Prompt diff at step 0 \u001b[0m\u001b[1m────────────────────────────────────────────\u001b[0m\u001b[1m─╮\u001b[0m\n",
       "\u001b[1m│\u001b[0m You are \u001b[32ma logical reasoning expert. Your task is to determine whether a \u001b[0mgiven \u001b[31msome \u001b[0m\u001b[32mhypothesis (question) can be\u001b[0m \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mlogically concluded from a given premise (\u001b[0mcontext\u001b[32m). Answer with Yes or No and provide step-by-step reasoning.\u001b[0m   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mInstructions:\u001b[0m                                                                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m1. Carefully analyze the context and question, identifying key terms and their relationships.\u001b[0m                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m2. Consider only the information explicitly provided in the context. Do not use general knowledge or make \u001b[0m      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32massumptions beyond what is given.\u001b[0m                                                                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m3. Pay special attention to:\u001b[0m                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m   - Negations\u001b[0m (\u001b[31ma premise\u001b[0m\u001b[32me.g., \"not\", \"isn't\"\u001b[0m)\u001b[31m and a \u001b[0m                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m   - Generalizations (e.g., \"all\", \"every\", \"any\")\u001b[0m                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m   - Specific vs. general terms (e.g., \"fedora\" is a specific type of \"hat\")\u001b[0m                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m4. A logical conclusion must be 100% certain based on the given information.\u001b[0m                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m5. Provide clear, step-by-step reasoning for your answer.\u001b[0m                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mExample of valid conclusion:\u001b[0m                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mContext: All dogs are animals. Fido is a dog.\u001b[0m                                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mQuestion: Can we logically conclude that Fido is an animal?\u001b[0m                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mAnswer: Yes\u001b[0m                                                                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mExample of invalid conclusion:\u001b[0m                                                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mContext: Some birds can fly.\u001b[0m                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mQuestion: Can we logically conclude that all birds can fly?\u001b[0m                                                     \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mAnswer: No\u001b[0m                                                                                                      \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mInput Format:\u001b[0m                                                                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mContext: {context}\u001b[0m                                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mQuestion: {\u001b[0mquestion\u001b[31m (a hypothesis). You must indicate\u001b[0m\u001b[32m}\u001b[0m                                                          \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mOutput Format:\u001b[0m                                                                                                  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mReasoning: Let's think step by step to determine if\u001b[0m w\u001b[31mith \u001b[0m\u001b[32me can logically conclude the hypothesis from the given\u001b[0m \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mpremise:\u001b[0m                                                                                                        \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m1. [First step in reasoning]\u001b[0m                                                                                    \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m2. [Second step in reasoning]\u001b[0m                                                                                   \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m...\u001b[0m                                                                                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32m[Final step in reasoning]\u001b[0m                                                                                       \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mAnswer: [\u001b[0mYes/No\u001b[31m ans\u001b[0m\u001b[32m]\u001b[0m                                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[32mNo\u001b[0mw\u001b[31mer \u001b[0m\u001b[32m, please provide your reasoning and ans\u001b[0mw\u001b[31mhether \u001b[0m\u001b[32mer for the follo\u001b[0mw\u001b[31me can logically conclude the hypothesis \u001b[0m  \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31mfrom the premise.\u001b[0m                                                                                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31m---\u001b[0m                                                                                                             \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31mFollow the following format.\u001b[0m\u001b[32ming:\u001b[0m                                                                                \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Context: \u001b[31m$\u001b[0m{\u001b[31m{\u001b[0mcontext}\u001b[31m}\u001b[0m                                                                                           \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Question: \u001b[31m${\u001b[0m{question}\u001b[31m}\u001b[0m                                                                                         \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m Reasoning: Let's think step by step \u001b[31min order to ${{produce the ans\u001b[0m\u001b[32mto determine if \u001b[0mwe\u001b[31mr}}. We ...\u001b[0m                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31mAnswer: Yes or No\u001b[0m                                                                                               \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31mContext: {context}\u001b[0m                                                                                              \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31mQuestion: {question}\u001b[0m                                                                                            \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m                                                                                                                 \u001b[1m│\u001b[0m\n",
       "\u001b[1m│\u001b[0m \u001b[31mReasoning: Let's think step by step in order to\u001b[0m\u001b[32m can logically conclude the hypothesis from the given premise:\u001b[0m   \u001b[1m│\u001b[0m\n",
       "\u001b[1m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'puzzled-knife-28' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/0360d4ae-63e0-4e58-b4a7-97f8ef466aaa/compare?selectedSessions=a9ba2210-4a71-4629-8ede-bad8566dfdd7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dee85dfa464c10b2d62022d10cbc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best score <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span> &gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9333333333333333</span>. Updating selected examples.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best score \u001b[1;36m1.0\u001b[0m > \u001b[1;36m0.9333333333333333\u001b[0m. Updating selected examples.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'tart-science-50' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a1613752-8df2-4e05-be08-9af1a2b55f60/compare?selectedSessions=7329ddf9-d492-4bdc-8068-bb77c0063d21\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b39af877254ef499e511a930a9c8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "# We will train with gpt-4-turbo\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "all_scores = train(\n",
    "    functools.partial(create_chain, llm=llm),\n",
    "    original_prompt_str,\n",
    "    train_name,\n",
    "    dev_name,\n",
    "    [exact_match],\n",
    "    steps=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d3e2d-e719-4328-babf-0b230b37d49a",
   "metadata": {},
   "source": [
    "## Compare on held-out set\n",
    "\n",
    "It's easy to overfit a single benchmark if you explicitly choose your pipeline based on metrics on that benchmark.\n",
    "\n",
    "Let's compare models on an unseen test set to see whether the selected examples are reliably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e086d8c-154e-4c2d-bc6f-69f6fdb1a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score, best_examples = all_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2d5a9-00c6-47df-bc82-53a18008e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_model = create_chain()\n",
    "# # This time we will apply gpt-3.5-turbo, but use the few-shot examples + reasoning trajectories\n",
    "# # from gpt-4 to help induce better performance\n",
    "# best_performing_model = create_chain(best_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e793f7a-0406-4a41-b12b-10d63450df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, model in [\n",
    "#     (\"optimized\", best_performing_model),\n",
    "#     # (\"original\", original_model),\n",
    "# ]:\n",
    "#     client.run_on_dataset(\n",
    "#         dataset_name=test_name,\n",
    "#         llm_or_chain_factory=model,\n",
    "#         evaluation=eval_config,\n",
    "#         verbose=True,\n",
    "#         project_metadata={\n",
    "#             \"model\": model_name,\n",
    "#         },\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae02ead-d88e-4ea5-b0bd-4cb6abbfa749",
   "metadata": {},
   "source": [
    "Using the GPT-4 generated examples, we were able to boost the performance from ~0.54 to ~0.87: not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf33e1-fa45-4f45-9f39-c01ac9eb644e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
