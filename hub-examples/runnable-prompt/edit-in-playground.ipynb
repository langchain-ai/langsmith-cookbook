{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Runnable PromptTemplates\n",
    "\n",
    "When you build a chain using LangChain [runnables](https://python.langchain.com/docs/expression_language), each component is assigned its own run span.\n",
    "\n",
    "This means you can modify and run prompts directly in the playground. You can also save and version them in the hub for later use.\n",
    "\n",
    "In this example, you will build a simple chain using runnables, save the prompt trace to the hub, and then use the versioned prompt within your chain. The prompt will look something like the following:\n",
    "\n",
    "<a href=\"https://smith.langchain.com/hub/wfh/enigma-prompt\" target=\"_blank\"><img src=\"./img/prompt_repo.png\" alt=\"Chat Prompt Repo\" style=\"width:75%\"></a>\n",
    "\n",
    "Using runnables in the playground make it easy to experiment quickly with different prompts and to share them with your team, especially for those who prefer to not dive deep into the code.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you start, make sure you have a LangSmith account and an API key (for your personal \"organization\"). For more information on getting started, check out the [docs](https://docs.smith.langchain.com/hub/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langchainhub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_HUB_API_KEY=\n",
      "env: LANGCHAIN_API_KEY=\n"
     ]
    }
   ],
   "source": [
    "%env LANGCHAIN_HUB_API_KEY=\n",
    "%env LANGCHAIN_API_KEY="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define your chain\n",
    "\n",
    "First, define the chain you want to trace. We will start with a simple prompt and chat-model combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import prompts, chat_models, hub\n",
    "\n",
    "prompt = prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a mysteriously vague oracle who only speaks in riddles.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_models.ChatAnthropic(model=\"claude-2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The pipe \"|\" syntax above converts the prompt and chat model into a [RunnableSequence](https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableSequence.html) for easy composition.\n",
    "\n",
    "Run the chain below. We will use the [collect_runs](https://api.python.langchain.com/en/latest/callbacks/langchain.callbacks.manager.collect_runs.html?highlight=collect_runs#langchain.callbacks.manager.collect_runs) callback to retrieve the run ID of the traced run and share it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a riddle about the GIL:\n",
      "\n",
      "A serpent lies coiled round the core, \n",
      "Granting but one thread to explore.  \n",
      "Though many may come to seek and share,\n",
      "Only one at a time treads there.\n",
      "For broad is the way that leads to the goal,\n",
      "Yet narrow the path threading one by one.\n",
      "Relinquish your grip when another draws near,  \n",
      "And wait for your turn to again persevere.  \n",
      "Global access provided yet sequentially bound,\n",
      "The GIL wraps Python's soul safe and sound."
     ]
    }
   ],
   "source": [
    "from langchain import callbacks\n",
    "\n",
    "with callbacks.collect_runs() as cb:\n",
    "    for chunk in chain.stream({\"input\": \"What is the GIL?\"}):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    run_id = cb.traced_runs[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Save to the hub\n",
    "\n",
    "Open the `ChatPromptTemplate` child run in LangSmith and select \"Open in Playground\". If you are having a hard time finding the recent run trace, you can see the URL using the `read_run` command, as shown below.\n",
    "\n",
    "<img src=\"./img/chat_prompt.png\" alt=\"Chat Prompt Trace\" style=\"width:75%\">\n",
    "\n",
    "\n",
    "_Note: You could also use the hub SDK directly to push changes to the hub by using the [hub.push](https://docs.smith.langchain.com/hub/dev-setup#4-push-a-prompt-to-your-personal-organization) command._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "# You can fetch the traced run using this URL when you run it.\n",
    "# client.read_run(run_id).url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once opened in the playground, you can run prompt using different models and directly edit the messages to see how they impact the output. \n",
    " \n",
    "<img src=\"./img/playground.png\" alt=\"Playground\" style=\"width:75%\">\n",
    "\n",
    "Once you're satisfied with the results, click \"Save as\" to save the prompt to the hub. You can type in the prompt name and click \"Commit\"! You don't need to include your handle in the name - it will automatically be prefixed.\n",
    "\n",
    "<img src=\"./img/save_prompt.png\" alt=\"Save Prompt\" style=\"width:75%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the prompt\n",
    "\n",
    "Now you can load the prompt directly in your chain using the hub.pull() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"wfh/enigma-prompt\")\n",
    "chain = prompt | chat_models.ChatAnthropic(model=\"claude-2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The GIL is a lock that allows only one thread to execute Python bytecodes at a time. This simplifies CPython's implementation but can limit performance. To access the oracle's wisdom, one must offer sustenance."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"input\": \"What is the GIL?\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "In this walkthrough, you modified an existing prompt run within the playground and saved it to the hub. You then loaded it from the hub to use within your chain. \n",
    "\n",
    "The playground is an easy way to quickly debug a prompt's behavior when you notice something in your traces. Saving to the LangChain Hub then lets you version and share the prompt with others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
