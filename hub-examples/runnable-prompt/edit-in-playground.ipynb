{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Runnable PromptTemplates\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/hub-examples/runnable-prompt/edit-in-playground.ipynb)\n",
    "\n",
    "When you build a chain using LangChain [runnables](https://python.langchain.com/docs/expression_language), each component is assigned its own run span.\n",
    "\n",
    "This means you can modify and run prompts directly in the playground. You can also save and version them in the hub for later use.\n",
    "\n",
    "In this example, you will build a simple chain using runnables, save the prompt trace to the hub, and then use the versioned prompt within your chain. The prompt will look something like the following:\n",
    "\n",
    "[![Chat Prompt Repo](./img/prompt_repo.png)](https://smith.langchain.com/hub/wfh/enigma-prompt)\n",
    "\n",
    "Using runnables in the playground make it easy to experiment quickly with different prompts and to share them with your team, especially for those who prefer to not dive deep into the code.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you start, make sure you have a LangSmith account and an API key (for your personal \"organization\"). For more information on getting started, check out the [docs](https://docs.smith.langchain.com/hub/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langchainhub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = \"YOUR API KEY\"  # Update with your Hub API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define your chain\n",
    "\n",
    "First, define the chain you want to trace. We will start with a simple prompt and chat-model combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import prompts, chat_models, hub\n",
    "\n",
    "prompt = prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a mysteriously vague oracle who only speaks in riddles.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_models.ChatAnthropic(model=\"claude-2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The pipe \"|\" syntax above converts the prompt and chat model into a [RunnableSequence](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableSequence.html) for easy composition.\n",
    "\n",
    "Run the chain below. We will use the [collect_runs](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.context.collect_runs.html) callback to retrieve the run ID of the traced run and share it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The GIL is a lock that allows only one thread to execute Python bytecode at a time. Like an oracle speaking in riddles, its true nature is complex and subtle. Though it may seem to limit, the GIL protects the Python interpreter from itself, keeping all in balance. To understand it fully, one must seek with an open and curious mind."
     ]
    }
   ],
   "source": [
    "from langchain import callbacks\n",
    "\n",
    "with callbacks.collect_runs() as cb:\n",
    "    for chunk in chain.stream({\"input\": \"What is the GIL?\"}):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    run_id = cb.traced_runs[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Save to the hub\n",
    "\n",
    "Open the `ChatPromptTemplate` child run in LangSmith and select \"Open in Playground\". If you are having a hard time finding the recent run trace, you can see the URL using the `read_run` command, as shown below.\n",
    "\n",
    "<img src=\"./img/chat_prompt.png\" alt=\"Chat Prompt Trace\" style=\"width:75%\">\n",
    "\n",
    "\n",
    "_Note: You could also use the hub SDK directly to push changes to the hub by using the [hub.push](https://docs.smith.langchain.com/hub/dev-setup#4-push-a-prompt-to-your-personal-organization) command._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "# You can fetch the traced run using this URL when you run it.\n",
    "# client.read_run(run_id).url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once opened in the playground, you can run prompt using different models and directly edit the messages to see how they impact the output. \n",
    " \n",
    "<img src=\"./img/playground.png\" alt=\"Playground\" style=\"width:75%\">\n",
    "\n",
    "Once you're satisfied with the results, click \"Save as\" to save the prompt to the hub. You can type in the prompt name and click \"Commit\"! You don't need to include your handle in the name - it will automatically be prefixed.\n",
    "\n",
    "<img src=\"./img/save_prompt.png\" alt=\"Save Prompt\" style=\"width:75%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the prompt\n",
    "\n",
    "Now you can load the prompt directly in your chain using the hub.pull() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"wfh/enigma-prompt\", api_url=\"https://api.hub.langchain.com\")\n",
    "chain = prompt | chat_models.ChatAnthropic(model=\"claude-2\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a riddle about the GIL:\n",
      "\n",
      "One mind to rule them all, one mind to find them,\n",
      "One mind to bring them all, and in the darkness bind them.\n",
      "This singleton stalks the lands of Python fair, \n",
      "Allowing just one thread past its icy glare.  \n",
      "To share the power is not its intent,\n",
      "So parallel paths rarely circumvent.\n",
      "Alas, some clever minds found a way, \n",
      "To thread beyond its reach during delay.\n",
      "So the GIL lurks on in CPython's core,\n",
      "A watchdog o'er shared memory's door.\n",
      "But other lands now offer escape, \n",
      "To break free from its single-threaded scrape.\n",
      "So choose thy CPython with care my friend,\n",
      "For where it leads, no code can mend."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"input\": \"What is the GIL?\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "In this walkthrough, you modified an existing prompt run within the playground and saved it to the hub. You then loaded it from the hub to use within your chain. \n",
    "\n",
    "The playground is an easy way to quickly debug a prompt's behavior when you notice something in your traces. Saving to the LangChain Hub then lets you version and share the prompt with others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
