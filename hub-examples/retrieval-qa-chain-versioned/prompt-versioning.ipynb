{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Versioning\n",
    "\n",
    "Using the \"latest\" version of a prompt in production can introduce unforeseen issues. The LangChain hub offers version-specific prompt pulling to enhance deployment consistency.\n",
    "\n",
    "In this tutorial, we'll illustrate this with a RetrievalQA chain. You'll initialize it using a particular prompt version from the hub. This tutorial builds upon the [RetrievalQA Chain example](../retrieval-qa-chain/retrieval-qa.ipynb).\n",
    "\n",
    "\n",
    "![Prompt Versions](./img/prompt_versions.png)\n",
    "\n",
    "\n",
    "Here's the central takeaway: For stable production deployments, specify a prompt's commit hash instead of defaulting to the 'latest'. This is done by appending the 'version' tag to the prompt ID.\n",
    "\n",
    "```python\n",
    "from langchain import hub\n",
    "\n",
    "hub.pull(f\"{handle}/{prompt-repo}:{version}\")\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure you have a LangSmith account and an API key for your organization. If you're new, see the [docs](https://docs.smith.langchain.com/hub/quickstart) for setup guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langchainhub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env LANGCHAIN_HUB_API_KEY=ls__..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load prompt\n",
    "\n",
    "Each time you push to a given prompt \"repo\", the new version is saved with a commit hash so you can track the prompt's lineage. By default, pulling from the repo loads the latest version of the prompt into memory. However, if you want to load a specific version, you can do so by including the hash at the end of the prompt name. For instance, let's load the rag-prompt with version `c9839f14` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "handle=\"wfh\"\n",
    "version=\"c9839f14\"\n",
    "prompt = hub.pull(f\"{handle}/rag-prompt:{version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Chain\n",
    "\n",
    "With the correct version of the prompt loaded, we can define our retrieval QA chain.\n",
    "\n",
    "We will start with the retriever definition. While the specifics aren't important to this tutorial, you can learn more about Q&A in LangChain by visiting the [docs](https://python.langchain.com/docs/use_cases/question_answering/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load docs\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Store splits\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, initialize the qa_chain using the versioned prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt} # The prompt is added here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Chain\n",
    "\n",
    "Now you can use the chain directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The approaches to task decomposition include using LLM with simple prompting, task-specific instructions, and human inputs.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, you loaded a specific version of a prompt for your RetrievalQAChain. You or other contributors to your prompt repo can then continue to commit new versions without disrupting your deployment.\n",
    "\n",
    "Prompt versioning is a simple, important function to use in your workflow to let you continue to experiment and collaborate without accidentally shipping an under-validated chain component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
