{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d369cc-e1b5-4674-a61a-5cfee71f066f",
   "metadata": {},
   "source": [
    "# LangSmith Evaluation with OpenAI Assistants\n",
    "\n",
    "The OpenAI Assistants API allows you to build assistants for your application. An assistant can leverage tools, models, and knowledge to respond to the user's query. In this demo, we'll build a very simply assistant to answer questions based on knowledge in documents. Learn more about OpenAI assistants [here](https://platform.openai.com/docs/assistants/overview?context=with-streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae26854-2d3c-4266-9f54-2430d968eeea",
   "metadata": {},
   "source": [
    "First, we'll do some setup. Create a LangSmith API Key by navigating to the settings page in LangSmith, then set the following environment variables.\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR OPENAI API KEY>\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_PROJECT='oai-test'\n",
    "LANGCHAIN_API_KEY=<YOUR LANGSMITH API KEY>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f84b5be-e5f8-4229-9a21-4b36058f6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import OpenAI from \"npm:openai@4.33.1\";\n",
    "import { Client } from \"npm:langsmith\";\n",
    "import { traceable } from \"npm:langsmith/traceable\";\n",
    "import { wrapOpenAI } from \"npm:langsmith/wrappers\";\n",
    "\n",
    "// Initialize the LangSmith and OpenAI clients\n",
    "const openai = new OpenAI();\n",
    "const langsmith = new Client();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4dcef53-8588-406e-b338-7fcb1bb9bd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file with ID: file-1mLrAxdYvZyXm5eYKbGsbfPJ\n",
      "Uploaded file with ID: file-tXS5BaWZF1AeninxtN2h4L4t\n",
      "Uploaded file with ID: file-UhneqboUOJLioqtRRexrrfUh\n",
      "All file IDs: [\n",
      "  \u001b[32m\"file-1mLrAxdYvZyXm5eYKbGsbfPJ\"\u001b[39m,\n",
      "  \u001b[32m\"file-tXS5BaWZF1AeninxtN2h4L4t\"\u001b[39m,\n",
      "  \u001b[32m\"file-UhneqboUOJLioqtRRexrrfUh\"\u001b[39m\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "// Functions to upload files to OpenAI s.t. the Assistant can use them\n",
    "// Need to use the API to upload files instead of the SDK for Deno compat\n",
    "async function uploadFile(filename: string, path: string, apiKey: string) {\n",
    "    const fileData = await Deno.readFile(path);\n",
    "    const formData = new FormData();\n",
    "    formData.append(\"purpose\", \"assistants\");\n",
    "    formData.append(\"file\", new Blob([fileData]), filename);\n",
    "\n",
    "    const response = await fetch(\"https://api.openai.com/v1/files\", {\n",
    "        method: \"POST\",\n",
    "        headers: {\n",
    "            \"Authorization\": `Bearer ${apiKey}`\n",
    "        },\n",
    "        body: formData\n",
    "    });\n",
    "\n",
    "    if (response.ok) {\n",
    "        const data = await response.json();\n",
    "        console.log(`Uploaded file with ID: ${data.id}`);\n",
    "        return data.id;\n",
    "    } else {\n",
    "        console.error(\"Failed to upload file:\", await response.text());\n",
    "        throw new Error(\"Failed to upload file\");\n",
    "    }\n",
    "}\n",
    "\n",
    "const files = [\"alice.txt\", \"bob.txt\", \"sarah.txt\"];\n",
    "const apiKey = Deno.env.get(\"OPENAI_API_KEY\"); // Make sure the API key is set in the environment\n",
    "let fileIds = [];\n",
    "\n",
    "for (const filename of files) {\n",
    "    const filePath = `./files/${filename}`;\n",
    "    try {\n",
    "        const fileId = await uploadFile(filename, filePath, apiKey);\n",
    "        fileIds.push(fileId);\n",
    "    } catch (error) {\n",
    "        console.error(`Error uploading ${filename}: ${error.message}`);\n",
    "    }\n",
    "}\n",
    "\n",
    "console.log(\"All file IDs:\", fileIds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b051b62-d3c4-4959-ad27-4cf698f9adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create the Assistant\n",
    "const assistant = await openai.beta.assistants.create({\n",
    "    name: \"Investment Assistant\",\n",
    "    instructions: \"You an investment support chatbot. Answer the user's questions based SOLELY on information in the provided documents.\",\n",
    "    tools: [{ type: \"retrieval\" }],\n",
    "    model: \"gpt-4-turbo\",\n",
    "    file_ids: fileIds\n",
    "});\n",
    "\n",
    "async function generateAssistantResponse(input: { question: string }) {\n",
    "    const thread = await openai.beta.threads.create();\n",
    "    const message = await openai.beta.threads.messages.create(\n",
    "      thread.id,\n",
    "      {\n",
    "        role: \"user\",\n",
    "        content: input.question\n",
    "      }\n",
    "    );\n",
    "    const run = await openai.beta.threads.runs.createAndPoll(thread.id, {\n",
    "      assistant_id: assistant.id,\n",
    "    });\n",
    "    const messages = await openai.beta.threads.messages.list(\n",
    "        run.thread_id\n",
    "    );\n",
    "    return messages.data.reverse()[1].content[0].text.value\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2d233f2-cf3c-4620-aa7f-cd14dad04dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Wrap the function in `traceable` to trace responses to LangSmith\n",
    "const traceGenerateAssistantResponse = traceable(generateAssistantResponse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c165b3a-ba48-4716-a370-c6ad34df5777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Sarah's investment portfolio includes 6% ownership in Acme Inc and 2% ownership in XYZ Corp.\"\u001b[39m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await traceGenerateAssistantResponse({question: \"What is Sarah's investment portfolio?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9439c-dd42-4edd-9d08-c28d37942f76",
   "metadata": {},
   "source": [
    "![Screenshot of Trace View in LangSmith](assistant_trace_view.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fbd8643-e05e-4b0d-b002-a0cbc6cc5dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ \u001b[90mundefined\u001b[39m, \u001b[90mundefined\u001b[39m, \u001b[90mundefined\u001b[39m ]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a dataset to be used for testing using the LangSmith client\n",
    "const examples = [\n",
    "  [\n",
    "    \"What is Sarah's investment portfolio?\",\n",
    "    \"Sarah's investment portfolio includes 6% ownership in Acme Inc and 2% ownership in XYZ Corp.\",\n",
    "  ],\n",
    "  [\n",
    "    \"How much of Acme Inc does Bob own?\",\n",
    "    \"Bob own 5% of Acme Inc.\",\n",
    "  ],\n",
    "  [\n",
    "    \"How much of Acme Inc does Alice own?\",\n",
    "    \"Alice owns 2% of Acme Inc.\"\n",
    "  ],\n",
    "];\n",
    "\n",
    "const datasetName = \"OpenAI Assistants Pipeline\";\n",
    "const dataset = await langsmith.createDataset(datasetName);\n",
    "\n",
    "await Promise.all(\n",
    "  examples.map(async ([question, answer]) => {\n",
    "    await langsmith.createExample(\n",
    "      { question },\n",
    "      { answer },\n",
    "      { datasetId: dataset.id }\n",
    "    );\n",
    "  })\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7889ef01-d633-41db-b972-6fd344ef2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run a set of tests on the dataset and compare them in LangSmith\n",
    "// First, set up evaluators to run against the test results\n",
    "import type { RunEvalType, RunEvaluatorLike } from \"npm:langchain/smith\";\n",
    "import { runOnDataset, Criteria, LabeledCriteria } from \"npm:langchain/smith\";\n",
    "\n",
    "const evaluators: RunEvalType[] = [\n",
    "  // LangChain's built-in evaluators\n",
    "  Criteria(\"conciseness\"),\n",
    "  LabeledCriteria(\"correctness\"),\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6065cc3-3d0e-4876-b976-82431edf0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n",
      "Completed\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░ 33.33% | 1/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░ 66.67% | 2/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  projectName: \u001b[32m\"test-oai-assistants-demo\"\u001b[39m,\n",
       "  results: {\n",
       "    \u001b[32m\"7cb7f3b5-ab41-4f82-9eab-004462500286\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m6169\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"bb44f247-e6b2-45dd-affd-a9e4055011c6\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"9ba82d51-17ae-47b2-9ef3-a0955397624e\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion is conciseness. This means the submission should be brief and to the point, without un\"\u001b[39m... 352 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m,\n",
       "          feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"55e374ad-6a6e-4c30-9d50-9f2aa68897c4\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"9ba82d51-17ae-47b2-9ef3-a0955397624e\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. This involves checking if the subm\"\u001b[39m... 277 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m,\n",
       "          feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"9ba82d51-17ae-47b2-9ef3-a0955397624e\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"a5abd81d-d85d-4f03-9094-e6de2e9a9d08\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m6135\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"b0fdda12-9078-40cf-b884-282238386b69\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"dfd39bce-501b-4629-9258-4a205043a18d\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion is conciseness. This means the submission should be brief and to the point.Looking at \"\u001b[39m... 375 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m,\n",
       "          feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"edc01848-690c-4e85-b278-200d0acac323\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"dfd39bce-501b-4629-9258-4a205043a18d\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. This involves checking if the subm\"\u001b[39m... 433 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m,\n",
       "          feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"dfd39bce-501b-4629-9258-4a205043a18d\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"f59961f7-50db-45db-88e3-ba09d972f082\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m6134\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"030c05d9-ba15-493f-925b-603ca0decce1\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"e2822a3f-a21c-4fda-8472-d934e9e5ec4c\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion is conciseness. This means the submission should be brief and to the point, without un\"\u001b[39m... 343 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m,\n",
       "          feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"f2f39145-e6ec-42ea-8994-a6fa56b38bc8\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"e2822a3f-a21c-4fda-8472-d934e9e5ec4c\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. The submission states that Sarah's\"\u001b[39m... 326 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m,\n",
       "          feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"e2822a3f-a21c-4fda-8472-d934e9e5ec4c\"\u001b[39m\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use `runOnDataset` to run the pipeline against examples in the Dataset\n",
    "await runOnDataset(traceGenerateAssistantResponse, datasetName, {\n",
    "  evaluators,\n",
    "  projectName: \"test-oai-assistants-demo\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bbb3c-f971-4555-bc7c-f166c35ab4d7",
   "metadata": {},
   "source": [
    "![Screenshot of Experiment View in LangSmith](assistant_experiment.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
