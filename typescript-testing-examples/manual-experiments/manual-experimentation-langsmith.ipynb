{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b8fc9e-1346-4c7a-be2e-f88f092f5fd5",
   "metadata": {},
   "source": [
    "# Log manual experiments\n",
    "\n",
    "Here, we'll do a very basic walkthrough of how you can log individual predictions as LangSmith `experiments`. This is useful if you already have some evaluation flow set up but you still want to take advantage of LangSmith's experiment tracking functionality.\n",
    "\n",
    "First, create a LangSmith account and API Key, then create an .env file with values for the following variables in the same directory as this notebook:\n",
    "\n",
    "```\n",
    "LANGCHAIN_API_KEY=<YOUR LANGSMITH API KEY>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945ce28f-aeae-41ee-88f3-f86af63ae609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\"; // Load env vars from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f1fbf-c171-4059-9e7d-12ced72fe150",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "First, create a dataset from the inputs (and optional reference outputs) we are evaluating over. These examples let us compare predictions from different models or systems on similar data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb84a49-88d1-4920-96f4-063a04afd8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  inputs: { input: \u001b[32m\"Foo 2\"\u001b[39m },\n",
       "  outputs: { output: \u001b[32m\"Bar 2\"\u001b[39m },\n",
       "  dataset_id: \u001b[32m\"893dd7d2-8ba6-4a47-9ebb-6fc133aaeba1\"\u001b[39m,\n",
       "  source_run_id: \u001b[1mnull\u001b[22m,\n",
       "  metadata: \u001b[1mnull\u001b[22m,\n",
       "  created_at: \u001b[32m\"2024-04-17T22:56:51.675000+00:00\"\u001b[39m,\n",
       "  id: \u001b[32m\"3a3b8945-156e-4a35-8e2f-12f7619a77ce\"\u001b[39m,\n",
       "  name: \u001b[32m\"\"\u001b[39m,\n",
       "  modified_at: \u001b[32m\"2024-04-17T22:56:51.675000+00:00\"\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { Client } from \"langsmith\";\n",
    "\n",
    "const client = new Client();\n",
    "\n",
    "const datasetName = `My-Dataset-${new Date().toISOString()}`;\n",
    "const dataset = await client.createDataset(datasetName);\n",
    "await client.createExample({\"input\": \"Foo\"}, {\"output\": \"Bar\"}, {datasetId: dataset.id})\n",
    "await client.createExample({\"input\": \"Foo 2\"}, {\"output\": \"Bar 2\"}, {datasetId: dataset.id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a02fa-5aac-47e4-a683-05f2d2408cf8",
   "metadata": {},
   "source": [
    "## AddPrediction helper\n",
    "\n",
    "Next, define an `addPrediction` helper function. This does 2 things:\n",
    "1. Creates an experiment, if it doesn't already exist.\n",
    "2. Creates a `Run` to represent your prediction on this data point.\n",
    "\n",
    "It returns the runId you can use for logging feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9435fc39-727a-4bd8-bc94-690a9317b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "interface addPredictionProps {\n",
    "    experimentName: string;\n",
    "    name?: string;\n",
    "    runId?: string;\n",
    "    referenceExampleId: string;\n",
    "    inputs?: unknown;\n",
    "}\n",
    "\n",
    "async function addPrediction(client: Client, prediction: unknown, props: addPredictionProps): Promise<void> {\n",
    "    const { experimentName, runId, referenceExampleId, inputs, name } = props;\n",
    "\n",
    "    const example = await client.readExample(referenceExampleId);\n",
    "\n",
    "    await client.createProject({ projectName: experimentName, referenceDatasetId: example.dataset_id, upsert: true });\n",
    "    const runId_ = runId ?? uuidv4();\n",
    "    await client.createRun({\n",
    "        name: name ?? \"Tested\" ,\n",
    "        id: runId_,\n",
    "        inputs: inputs ? {input: inputs}: undefined,\n",
    "        outputs: {output: prediction}, \n",
    "        run_type: 'chain', \n",
    "        reference_example_id: referenceExampleId,\n",
    "        start_time: Date.now(),\n",
    "        end_time: Date.now(),\n",
    "        project_name: experimentName,\n",
    "    });\n",
    "    return runId_;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5df87a-8879-44bb-ad2c-bf4083ebaf99",
   "metadata": {},
   "source": [
    "## Add your first prediction\n",
    "\n",
    "To log a prediction, at minimum we need:\n",
    "- The predicted value\n",
    "- The dataset example we were predicting for\n",
    "- The experiment name to associate this prediction with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3357ac7-d1d7-4c70-8995-effeb19c5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \u001b[32m\"3a3b8945-156e-4a35-8e2f-12f7619a77ce\"\u001b[39m,\n",
      "  \u001b[32m\"8164992a-8db6-495e-bd23-bef7769629a9\"\u001b[39m\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const exampleIds = [];\n",
    "for await (const example of client.listExamples({datasetName})) {\n",
    "    exampleIds.push(example.id)\n",
    "}\n",
    "console.log(exampleIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7efb70-68f0-443f-8de7-15820fd82509",
   "metadata": {},
   "outputs": [],
   "source": [
    "const predictionOne = \"Foo\"; \n",
    "const experimentName = \"MyExperiment\"\n",
    "\n",
    "const runId = await addPrediction(client, predictionOne, {referenceExampleId: exampleIds[0], experimentName})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577de06f-e72a-422d-80eb-139a5c22688a",
   "metadata": {},
   "source": [
    "## Log Feedback\n",
    "\n",
    "Now you can log any type of feedback metrics for this prediction. For instance, you can score this with continuous values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b09c584-8a13-4be3-80d3-183b941088d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  id: \u001b[32m\"a619d236-a28f-444a-a127-aea3498b0808\"\u001b[39m,\n",
       "  run_id: \u001b[32m\"5abc59ee-0e42-4f5f-b600-7f8e76cb4794\"\u001b[39m,\n",
       "  key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "  score: \u001b[33m1\u001b[39m,\n",
       "  value: \u001b[90mundefined\u001b[39m,\n",
       "  correction: \u001b[90mundefined\u001b[39m,\n",
       "  comment: \u001b[32m\"This looks impeccable.\"\u001b[39m,\n",
       "  feedback_source: { type: \u001b[32m\"api\"\u001b[39m, metadata: {} },\n",
       "  feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.createFeedback(runId, \"correctness\", {score: 1, comment: \"This looks impeccable.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88feee2-a3d3-4363-b50d-c6490e172aa3",
   "metadata": {},
   "source": [
    "You can also log unstructured notes without a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc9db6e-c0d3-44fc-9ed5-833de18c8808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  id: \u001b[32m\"2acb6755-6c35-4d24-83c8-9bbb79e85674\"\u001b[39m,\n",
       "  run_id: \u001b[32m\"5abc59ee-0e42-4f5f-b600-7f8e76cb4794\"\u001b[39m,\n",
       "  key: \u001b[32m\"note\"\u001b[39m,\n",
       "  score: \u001b[90mundefined\u001b[39m,\n",
       "  value: \u001b[90mundefined\u001b[39m,\n",
       "  correction: \u001b[90mundefined\u001b[39m,\n",
       "  comment: \u001b[32m\"I think I could do better though. Not gonna leave a score here.\"\u001b[39m,\n",
       "  feedback_source: { type: \u001b[32m\"api\"\u001b[39m, metadata: {} },\n",
       "  feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.createFeedback(runId, \"note\", { comment: \"I think I could do better though. Not gonna leave a score here.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857026d-abfc-4969-8fb5-479758054283",
   "metadata": {},
   "source": [
    "## Add more predictions\n",
    "\n",
    "Continue in a similar way, logging predictions for each example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99762c7-1dbf-4e7d-bda1-830b2b6a45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "const predictionTwo = \"Bar\";\n",
    "const runId2 = await addPrediction(client, predictionTwo, {referenceExampleId: exampleIds[1], experimentName})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c57250-07ac-4c90-94af-8a0d749a8939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  id: \u001b[32m\"aa8f1d3f-ff14-47b5-b974-48da07ff2faa\"\u001b[39m,\n",
       "  run_id: \u001b[32m\"5abc59ee-0e42-4f5f-b600-7f8e76cb4794\"\u001b[39m,\n",
       "  key: \u001b[32m\"Correctness\"\u001b[39m,\n",
       "  score: \u001b[33m0\u001b[39m,\n",
       "  value: \u001b[90mundefined\u001b[39m,\n",
       "  correction: \u001b[90mundefined\u001b[39m,\n",
       "  comment: \u001b[32m\"Completely wrong.\"\u001b[39m,\n",
       "  feedback_source: { type: \u001b[32m\"api\"\u001b[39m, metadata: {} },\n",
       "  feedbackConfig: \u001b[90mundefined\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.createFeedback(runId, \"Correctness\", {score: 0, comment: \"Completely wrong.\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
