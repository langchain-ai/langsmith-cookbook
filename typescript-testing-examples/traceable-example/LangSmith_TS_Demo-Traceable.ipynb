{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b8fc9e-1346-4c7a-be2e-f88f092f5fd5",
   "metadata": {},
   "source": [
    "# LangSmith Tutorial (No LangChain)\n",
    "\n",
    "Here, we'll do a very basic walkthrough of how you can get started with LangSmith in TypeScript without LangChain.\n",
    "\n",
    "First, we'll do some setup. Create a LangSmith API Key by navigating to the settings page in LangSmith, then create an .env file with values for the following variables, in the same directory as this notebook:\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR OPENAI API KEY>\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_PROJECT='langsmith-wikirag-walkthrough'\n",
    "LANGCHAIN_API_KEY=<YOUR LANGSMITH API KEY>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945ce28f-aeae-41ee-88f3-f86af63ae609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\"; // Load env vars from .env file\n",
    "import OpenAI from \"npm:openai\";\n",
    "import wiki from \"npm:wikipedia\";\n",
    "import { Client } from \"langsmith\";\n",
    "import { traceable } from \"langsmith/traceable\";\n",
    "import { wrapOpenAI } from \"langsmith/wrappers\";\n",
    "\n",
    "// Wrap OpenAI in the LangSmith wrapper to get completions to log to the system\n",
    "const openai = wrapOpenAI(new OpenAI());\n",
    "const langsmith = new Client();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f5d23-2da7-4d6a-aeaa-cc3812188322",
   "metadata": {},
   "source": [
    "## Create a Wikipedia Rag Pipeline that doesn't use LangChain\n",
    "\n",
    "Here, we'll create a very simple RAG pipeline that:\n",
    "1. Generates a Wikpedia search query from the input question\n",
    "2. Retrieves relevant page summaries from Wikipedia based on the search query\n",
    "3. Answers the input question based on the context from the retrieval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93190a21-b0ec-45e8-b671-78bd1194f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "async function generateWikiSearch(input: { question: string }) {\n",
    "  const messages = [\n",
    "    {\n",
    "      role: \"system\" as const,\n",
    "      content:\n",
    "        \"Generate a search query to pass into wikipedia to answer the user's question. Return only the search query and nothing more. This will be passed in directly to the Wikipedia search engine.\",\n",
    "    },\n",
    "    { role: \"user\" as const, content: input.question },\n",
    "  ];\n",
    "\n",
    "  const chatCompletion = await openai.chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages: messages,\n",
    "    temperature: 0,\n",
    "  });\n",
    "  return chatCompletion.choices[0].message.content ?? \"\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcdfe83-3f57-45db-9d7d-0f5dbc797050",
   "metadata": {},
   "outputs": [],
   "source": [
    "function convertDocs(results: Array<{ summary: string; url: string }>) {\n",
    "  // Convert docs to a format that LangSmith accepts (for nicer rendering)\n",
    "  return results.map((r) => ({\n",
    "    page_content: r.summary,\n",
    "    type: \"Document\",\n",
    "    metadata: { url: r.url },\n",
    "  }));\n",
    "}\n",
    "\n",
    "async function retrieve(input: { query: string; numDocuments: number }) {\n",
    "  const { results } = await wiki.search(input.query, { limit: 10 });\n",
    "  const finalResults: Array<{ summary: string; url: string }> = [];\n",
    "\n",
    "  for (const result of results) {\n",
    "    if (finalResults.length >= input.numDocuments) {\n",
    "      // Just return the top 2 pages for now\n",
    "      break;\n",
    "    }\n",
    "    const page = await wiki.page(result.title, { autoSuggest: false });\n",
    "    const summary = await page.summary();\n",
    "    finalResults.push({\n",
    "      summary: summary.extract,\n",
    "      url: page.fullurl,\n",
    "    });\n",
    "  }\n",
    "\n",
    "  return convertDocs(finalResults);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa6f777-0149-45df-8974-7db209d10c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "async function generateAnswer(input: { question: string; context: string }) {\n",
    "  const messages = [\n",
    "    {\n",
    "      role: \"system\" as const,\n",
    "      content: `Answer the user's question based only on the content below:\\n\\n${input.context}`,\n",
    "    },\n",
    "    { role: \"user\" as const, content: input.question },\n",
    "  ];\n",
    "\n",
    "  const chatCompletion = await openai.chat.completions.create({\n",
    "    model: \"gpt-3.5-turbo\",\n",
    "    messages: messages,\n",
    "    temperature: 0,\n",
    "  });\n",
    "  return chatCompletion.choices[0].message.content ?? \"\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ec7c78-4972-4b94-adcc-b99113114918",
   "metadata": {},
   "outputs": [],
   "source": [
    "const traceGenerateWikiSearch = traceable(generateWikiSearch);\n",
    "const traceRetrieve = traceable(retrieve, {\n",
    "  name: \"Retrieve Wiki\",\n",
    "  run_type: \"retriever\",\n",
    "});\n",
    "const traceGenerateAnswer = traceable(generateAnswer);\n",
    "\n",
    "const traceRagPipeline = traceable(\n",
    "  async ({ question }, numDocuments: number = 2) => {\n",
    "    const query = await traceGenerateWikiSearch({ question });\n",
    "    const retrieverResults = await traceRetrieve({ query, numDocuments });\n",
    "    const context = retrieverResults\n",
    "      .map((result) => result.page_content)\n",
    "      .join(\"\\n\\n\");\n",
    "    const answer = await traceGenerateAnswer({ question, context });\n",
    "    return answer;\n",
    "  },\n",
    "  { name: \"Wiki RAG Pipeline\", run_type: \"chain\" }\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5814c348-d746-4efe-97aa-641769895da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"The Apple Vision Pro was released in the United States on February 2, 2024.\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await traceRagPipeline({\n",
    "  question: \"When was the Apple Vision Pro released in the US?\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b3d7d-ab4f-4267-9190-c18800edbb0d",
   "metadata": {},
   "source": [
    "![Screenshot of Trace View in LangSmith](trace_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf8e87-6b98-4d9f-bc56-8eb9004615d9",
   "metadata": {},
   "source": [
    "## Run the pipeline on some test cases\n",
    "\n",
    "Before deploying to an initial set of users, it's often helpful to create a test set of a few examples, then run your pipeline on the test set.\n",
    "\n",
    "LangSmith makes it easy to run custom evaluations (both LLM and heuristic-based) to score the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1abfef0f-6188-4566-91c9-6ba9c50e0c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ \u001b[90mundefined\u001b[39m, \u001b[90mundefined\u001b[39m, \u001b[90mundefined\u001b[39m ]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a dataset to be used for testing using the LangSmith client\n",
    "const examples = [\n",
    "  [\n",
    "    \"When was the Apple Vision Pro released in the US?\",\n",
    "    \"The Apple Vision Pro was released in the United States on February 2, 2024.\",\n",
    "  ],\n",
    "  [\n",
    "    \"What is LangChain?\",\n",
    "    \"LangChain is an open-source framework for building applications using large language models.\",\n",
    "  ],\n",
    "  [\n",
    "    \"Who is the chairman of OpenAI?\",\n",
    "    \"Bret Taylor is the chairman of OpenAI\"\n",
    "  ],\n",
    "];\n",
    "\n",
    "const datasetName = \"Wikipedia RAG Pipeline\";\n",
    "const dataset = await langsmith.createDataset(datasetName);\n",
    "\n",
    "await Promise.all(\n",
    "  examples.map(async ([question, answer]) => {\n",
    "    await langsmith.createExample(\n",
    "      { question },\n",
    "      { answer },\n",
    "      { datasetId: dataset.id }\n",
    "    );\n",
    "  })\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00e485-12e0-48d8-8028-e289db964e81",
   "metadata": {},
   "source": [
    "![Dataset](dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be9c081-c974-4e35-8fb6-a3cf398fb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run a set of tests on the dataset and compare them in LangSmith\n",
    "// First, set up evaluators to run against the test results\n",
    "import type { RunEvalType, RunEvaluatorLike } from \"langchain/smith\";\n",
    "import { runOnDataset, Criteria, LabeledCriteria } from \"langchain/smith\";\n",
    "\n",
    "// An illustrative custom evaluator example\n",
    "const containsOpenAI: RunEvaluatorLike = async ({\n",
    "  run,\n",
    "  example,\n",
    "  input,\n",
    "  prediction,\n",
    "  reference,\n",
    "}) => {\n",
    "  return {\n",
    "    key: \"contains_openai\",\n",
    "    score: prediction.output.includes(\"OpenAI\"),\n",
    "  };\n",
    "};\n",
    "\n",
    "const evaluators: RunEvalType[] = [\n",
    "  // LangChain's built-in evaluators\n",
    "  Criteria(\"conciseness\"),\n",
    "  LabeledCriteria(\"correctness\"),\n",
    "\n",
    "  // Custom evaluators can be user-defined RunEvaluator's\n",
    "  // or a compatible function\n",
    "  containsOpenAI,\n",
    "];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89feef74-28af-498f-987a-9d5064ebd493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n",
      "Completed\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░ 33.33% | 1/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░ 66.67% | 2/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  projectName: \u001b[32m\"test-2-documents-demo\"\u001b[39m,\n",
       "  results: {\n",
       "    \u001b[32m\"19c3732e-4347-4c97-bf05-57026783245e\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m1758\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"81fa0f21-9bc7-4598-8d5a-47ba2225b648\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"7e11421d-cde6-4e77-8e28-27b5c260ad66\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mtrue\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"f43dbafe-30a4-44e9-86c7-2c1eddb2989f\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"7e11421d-cde6-4e77-8e28-27b5c260ad66\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 319 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"64c28f8b-bcc0-46fd-9010-18f4b24cef81\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"7e11421d-cde6-4e77-8e28-27b5c260ad66\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m0\u001b[39m,\n",
       "          value: \u001b[32m\"N\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. This means the submitted answer sh\"\u001b[39m... 532 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"7e11421d-cde6-4e77-8e28-27b5c260ad66\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"b846e5ef-2265-42cd-9c50-4d0c7ec61376\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m1478\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"71adc217-d9a4-4c11-bc6b-91d6900befd6\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"efada762-00c9-4a4e-b003-7d86c0ccdcf7\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mfalse\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"0ece59a7-be03-4656-9d00-340dc2b1f4e3\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"efada762-00c9-4a4e-b003-7d86c0ccdcf7\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this assessment is conciseness. The submission is a single sentence that directly \"\u001b[39m... 217 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"756599ba-a798-4552-820b-dd94cdd729e0\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"efada762-00c9-4a4e-b003-7d86c0ccdcf7\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. The submission states that LangCha\"\u001b[39m... 672 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"efada762-00c9-4a4e-b003-7d86c0ccdcf7\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"aae4ce78-52fe-49f3-b945-f6f4a7c21dba\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m1844\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"8b294dd7-ba6e-4b00-8f3d-a19c6503ade9\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"b7447ce7-75c2-4623-8d90-f6fe2db6b97b\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mfalse\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"4e5442ae-5740-43e5-8f01-605cfc05a47d\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"b7447ce7-75c2-4623-8d90-f6fe2db6b97b\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 292 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"5e05a648-4733-4886-82a8-d595ce14db9a\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"b7447ce7-75c2-4623-8d90-f6fe2db6b97b\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. This involves checking if the subm\"\u001b[39m... 397 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"b7447ce7-75c2-4623-8d90-f6fe2db6b97b\"\u001b[39m\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use `runOnDataset` to run the pipeline against examples in the Dataset\n",
    "await runOnDataset(traceRagPipeline, datasetName, {\n",
    "  evaluators,\n",
    "  projectName: \"test-2-documents-demo\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0150efa-d17c-4848-8d71-4fbcafb91e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n",
      "Completed\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░ 33.33% | 1/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░ 66.67% | 2/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  projectName: \u001b[32m\"test-4-documents-demo\"\u001b[39m,\n",
       "  results: {\n",
       "    \u001b[32m\"19c3732e-4347-4c97-bf05-57026783245e\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m1927\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"6595171c-afff-4b43-ad02-47adbed7cc9a\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"7c23bee6-7333-47a0-ab45-bc46f8863eaa\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mtrue\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"8077c116-2713-456a-a82e-1ed32f48e1df\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"7c23bee6-7333-47a0-ab45-bc46f8863eaa\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this assessment is conciseness. This means the submission should be brief, to the \"\u001b[39m... 318 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"4850f2fe-a6e0-4f5c-ac2b-b94cc2c3b7d3\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"7c23bee6-7333-47a0-ab45-bc46f8863eaa\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m0\u001b[39m,\n",
       "          value: \u001b[32m\"N\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. The submission should be correct, \"\u001b[39m... 555 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"7c23bee6-7333-47a0-ab45-bc46f8863eaa\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"b846e5ef-2265-42cd-9c50-4d0c7ec61376\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m2265\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"ca3ecf2c-66c6-4b31-9329-93a9e2844009\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"d26226b2-7571-4506-8ee3-f527e3e9b9c7\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mfalse\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"7d4c3198-f17d-4f2e-b9f8-95fd83edadaa\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"d26226b2-7571-4506-8ee3-f527e3e9b9c7\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 554 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"36123b72-cc5e-44ef-9e1b-b07b953898f5\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"d26226b2-7571-4506-8ee3-f527e3e9b9c7\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. The submission states that LangCha\"\u001b[39m... 617 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"d26226b2-7571-4506-8ee3-f527e3e9b9c7\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"aae4ce78-52fe-49f3-b945-f6f4a7c21dba\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m2963\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"f4f2c883-feaa-4739-930d-4a45a8e26def\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"c7c478be-7770-4990-bccf-d3ba2e49e579\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mfalse\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"6c494270-e994-49c1-8121-3e168a6044f5\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"c7c478be-7770-4990-bccf-d3ba2e49e579\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 457 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"9af5492c-d7b5-4104-b395-f35ba2455927\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"c7c478be-7770-4990-bccf-d3ba2e49e579\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. This involves checking if the subm\"\u001b[39m... 394 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"c7c478be-7770-4990-bccf-d3ba2e49e579\"\u001b[39m\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Let's now execute a test of the pipeline where we retrieve 4 pages instead of 1\n",
    "\n",
    "// Wrapper function with numDocuments set to 4\n",
    "const traceRagPipelineFourDocuments = async ({ question }) => {\n",
    "  return traceRagPipeline({ question }, 4);\n",
    "};\n",
    "\n",
    "await runOnDataset(traceRagPipelineFourDocuments, datasetName, {\n",
    "  evaluators,\n",
    "  projectName: \"test-4-documents-demo\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97dd6f-1d83-4de0-846b-c43f19b10f9b",
   "metadata": {},
   "source": [
    "![Screenshot of Test Results in LangSmith](test_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef481578-9739-4253-87e3-3483ebe13f41",
   "metadata": {},
   "source": [
    "![Screenshot of Comparison View in LangSmith](comparison_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b3934-c865-4f8c-b686-8896f7d25877",
   "metadata": {},
   "source": [
    "## Tracing\n",
    "\n",
    "Let's say you've deployed your application to production or to an initial set of users. \n",
    "\n",
    "You can view traces of your application in LangSmith and drill down by various attributes to get statistics on a specific set of traces.\n",
    "\n",
    "You can also attach feedback to your runs (such as through a thumbs up/down button) [using the LangSmith client](https://docs.smith.langchain.com/tracing/faq/logging_feedback#collecting-feedback-programmatically), and filter on traces with a certain feedback score in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baedc0f-0578-42d7-be79-30890e7e2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Here we execute the pipeline on a number of potential user inputs, simulating how users might interact with it.\n",
    "import { pLimit } from \"https://deno.land/x/p_limit@v1.0.0/mod.ts\";\n",
    "const limit = pLimit(5); // Add a concurrency limit to avoid getting rate limited by Open\n",
    "\n",
    "async function runRagPipelines(questions: string[]): Promise<string[]> {\n",
    "  const promises = questions.map((question) =>\n",
    "    limit(() => traceRagPipeline({ question }))\n",
    "  );\n",
    "\n",
    "  try {\n",
    "    const results = await Promise.all(promises);\n",
    "    return results;\n",
    "  } catch (error) {\n",
    "    console.error(\"Error running RAG Pipelines:\", error);\n",
    "    throw error;\n",
    "  }\n",
    "}\n",
    "\n",
    "// Example usage\n",
    "const questions: string[] = [\n",
    "  \"When was Sam Altman removed from OpenAI?\",\n",
    "  \"What is the significance of the James Webb Space Telescope's first images?\",\n",
    "  \"How did the international community respond to the crisis in Ukraine?\",\n",
    "  \"What is the status of the COVID-19 vaccine distribution worldwide?\",\n",
    "  \"What are the outcomes of the recent G7 summit?\",\n",
    "  \"Who are the leading figures in climate change activism today?\",\n",
    "  \"How many majors has Jannik Sinner won?\",\n",
    "];\n",
    "await runRagPipelines(questions);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4361279-6075-4225-a918-50bae60b04e8",
   "metadata": {},
   "source": [
    "![Screenshot of Trace View in LangSmith](trace_project_view.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
