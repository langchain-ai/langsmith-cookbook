{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ae8806-be35-44e1-93c7-878eed7f14f3",
   "metadata": {},
   "source": [
    "# Loading Retriever Runs and Feedback\n",
    "\n",
    "For any Q&A application or RAG pipeline, the content and structure of the retrieved documents strongly influences your application's end-to-end quality. Filtering on user feedback is a good way to focus on cases you should improve. \n",
    "\n",
    "This walkthrough shows how to load retriever runs within traces that received negative user scores. We will read them into a pandas dataframe to easily interact with the data locally. \n",
    "\n",
    "This fetches runs in a similar fashion to that shown in the LangSmith app clip below, but it lets you programmatically handle batches of runs with your own code.\n",
    "\n",
    "![LangSmith app filter by feedback then view](./img/filter_then_view.gif)\n",
    "\n",
    "Before we start, ensure you have a LangChain project with retriever results with some logged traces. If you do not have a project prepared, you can run the first section of the  generate some using the [Testing QA Correctness](../../testing-examples/qa-correctness/qa-correctness.ipynb) notebook to get started. \n",
    "\n",
    "\n",
    "#### Setup\n",
    "\n",
    "First, install langsmith and pandas and set your langsmith API key to connect to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b4974-1f59-4517-bce3-b4836783b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langsmith pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37e579-d044-4d95-8dc5-1763f956f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env LANGCHAIN_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428695ac-6ca5-4967-8e17-7b0fc6a4df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3143554-6a2a-4f42-b6a3-0df9cb521314",
   "metadata": {},
   "source": [
    "## 1. Query Runs\n",
    "\n",
    "With the environment ready, load the run data from LangSmith. In a typical RAG application, the retriever is called within a chain or runnable sequence. However, user feedback is typically assigned to the _root run_ of a trace, since the user normally responds to end-to-end behavior.\n",
    "\n",
    "To fetch retriever runs within these traces, make two calls:\n",
    "\n",
    "1. Fetch root runs with feedback.\n",
    "2. Fetch Retriever runs for those traces.\n",
    "\n",
    "Please reference the [docs](https://docs.smith.langchain.com/tracing/use-cases/export-runs/local) for guidance on more complex filters (using metadata, tags, dates, and other attributes).\n",
    "\n",
    "Below, start by fetching root traces in your project with \"user_score\" feedback less than or equal to 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe677d3-be2d-430c-8609-6e3688d2e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "project_name = \"chat-langchain\" # Set to your project\n",
    "feedback_key = \"user_score\"\n",
    "\n",
    "root_runs_with_feedback = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    "    filter=f'and(eq(feedback_key, \"{feedback_key}\"), lte(feedback_score, 0.25))',\n",
    ")\n",
    "\n",
    "ids_with_feedback = {\n",
    "    r.id: r.feedback_stats[feedback_key]['avg']\n",
    "    for r in root_runs_with_feedback\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfe036-d524-4a5e-9cdf-c537fb7d211c",
   "metadata": {},
   "source": [
    "Next, fetch retriever runs in the same project, selecting those in the traces filtered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3150d1-1f8f-415b-aede-e519fdb8aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    run_type=\"retriever\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7744d0-9f37-4d18-877e-3a57e3e849ff",
   "metadata": {},
   "source": [
    "Finally, load into a dataframe to display locally! Here we add the feedback to the table from the `ids_with_feedback` dictionary above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa0ab4c-6902-4db7-8b21-43dcd9f3438f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "      <th>user_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What model is being used?</td>\n",
       "      <td>[{'metadata': {'title': 'Model I/O | ü¶úÔ∏èüîó Langc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can you call initialize_agent with a custo...</td>\n",
       "      <td>[{'metadata': {'title': 'Prompt pipelining | ü¶ú...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the error message encountered in the c...</td>\n",
       "      <td>[{'metadata': {'title': 'Conversation Summary ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I use a RecursiveUrlLoader to load cont...</td>\n",
       "      <td>[{'metadata': {'title': 'GitBook | ü¶úÔ∏èüîó Langcha...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I create a prompt template?</td>\n",
       "      <td>[{'metadata': {'title': 'Prompt templates | ü¶úÔ∏è...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                          What model is being used?   \n",
       "1  How can you call initialize_agent with a custo...   \n",
       "2  What is the error message encountered in the c...   \n",
       "3  How do I use a RecursiveUrlLoader to load cont...   \n",
       "4                How can I create a prompt template?   \n",
       "\n",
       "                                           documents  user_score  \n",
       "0  [{'metadata': {'title': 'Model I/O | ü¶úÔ∏èüîó Langc...         0.0  \n",
       "1  [{'metadata': {'title': 'Prompt pipelining | ü¶ú...         0.0  \n",
       "2  [{'metadata': {'title': 'Conversation Summary ...         0.0  \n",
       "3  [{'metadata': {'title': 'GitBook | ü¶úÔ∏èüîó Langcha...         0.0  \n",
       "4  [{'metadata': {'title': 'Prompt templates | ü¶úÔ∏è...         0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            **r.inputs,\n",
    "            **(r.outputs or {}),\n",
    "            feedback_key: ids_with_feedback[r.parent_run_ids[-1]],\n",
    "        } for r in retriever_runs\n",
    "        if r.parent_run_ids and r.parent_run_ids[-1] in ids_with_feedback\n",
    "    ]\n",
    ")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be3bb8-8968-458c-b2aa-13a7e0fc9878",
   "metadata": {},
   "source": [
    "With the data stored locally, you can add additional columns. Let's check to see how  many documents were retrieved for each query, and display the page source of the first retrieved document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d95a027-1651-4caf-854c-7d71ee218757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>documents</th>\n",
       "      <th>user_score</th>\n",
       "      <th>num_retrieved_docs</th>\n",
       "      <th>first_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What model is being used?</td>\n",
       "      <td>[{'metadata': {'title': 'Model I/O | ü¶úÔ∏èüîó Langc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://python.langchain.com/docs/modules/mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can you call initialize_agent with a custo...</td>\n",
       "      <td>[{'metadata': {'title': 'Prompt pipelining | ü¶ú...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://python.langchain.com/docs/modules/mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the error message encountered in the c...</td>\n",
       "      <td>[{'metadata': {'title': 'Conversation Summary ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://python.langchain.com/docs/modules/memo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I use a RecursiveUrlLoader to load cont...</td>\n",
       "      <td>[{'metadata': {'title': 'GitBook | ü¶úÔ∏èüîó Langcha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://python.langchain.com/docs/integrations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I create a prompt template?</td>\n",
       "      <td>[{'metadata': {'title': 'Prompt templates | ü¶úÔ∏è...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>https://python.langchain.com/docs/modules/mode...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                          What model is being used?   \n",
       "1  How can you call initialize_agent with a custo...   \n",
       "2  What is the error message encountered in the c...   \n",
       "3  How do I use a RecursiveUrlLoader to load cont...   \n",
       "4                How can I create a prompt template?   \n",
       "\n",
       "                                           documents  user_score  \\\n",
       "0  [{'metadata': {'title': 'Model I/O | ü¶úÔ∏èüîó Langc...         0.0   \n",
       "1  [{'metadata': {'title': 'Prompt pipelining | ü¶ú...         0.0   \n",
       "2  [{'metadata': {'title': 'Conversation Summary ...         0.0   \n",
       "3  [{'metadata': {'title': 'GitBook | ü¶úÔ∏èüîó Langcha...         0.0   \n",
       "4  [{'metadata': {'title': 'Prompt templates | ü¶úÔ∏è...         0.0   \n",
       "\n",
       "   num_retrieved_docs                                       first_source  \n",
       "0                   3  https://python.langchain.com/docs/modules/mode...  \n",
       "1                   3  https://python.langchain.com/docs/modules/mode...  \n",
       "2                   3  https://python.langchain.com/docs/modules/memo...  \n",
       "3                   3  https://python.langchain.com/docs/integrations...  \n",
       "4                   3  https://python.langchain.com/docs/modules/mode...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_retrieved_docs'] = df['documents'].apply(len)\n",
    "df['first_source'] = df['documents'].apply(lambda x: x[0]['metadata'].get('source'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633e1e9-52df-40e1-af10-dc2216625b19",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! In this walkthrough, you loaded retriever runs for traces that received negative user scores. You can use this approach to better understand common failure modes of your RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a5ee6-65d3-4655-8575-848923598ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
