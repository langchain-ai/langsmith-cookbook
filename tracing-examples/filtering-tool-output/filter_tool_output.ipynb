{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea796b03-1259-4742-b9db-25530cc93c2e",
   "metadata": {},
   "source": [
    "# Filtering Tool Output\n",
    "\n",
    "Tools are interfaces an agent can use to interact with any function. If you're building a chat bot that uses an agent, you may want to surface additional information in the UI without actually presenting it to the agent. For instance, if your tool returns a lengthy dataframe, you may have to truncate it for the agent, but you can surface the full value in the app for the user to see.\n",
    "\n",
    "This notebook walks through how to surface this by nesting a call within the tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8bbee-34c7-48ff-8338-203182759415",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This example uses LangSmith and OpenAI. Please make sure the dependencies and API keys are configured appropriately before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca82ce5-4892-46f1-bda3-21fd14a0d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain openai sklearn pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f747c6d-cbbf-449f-9470-c40ea825b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"<your api key>\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Filtering Tool Output\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<your openai api key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157e9f9-0aa2-4746-b5bd-e313eaa78e6e",
   "metadata": {},
   "source": [
    "## 1. Define tool\n",
    "\n",
    "We will show how to properly structure your nested calls both when using the decorator and when subclassing the `BaseTool` class.\n",
    "\n",
    "### Option 1: Using the @tool decorator\n",
    "\n",
    "The `@tool` decorator is the most concise way to define a LangChain tool. To propagate callbacks\n",
    "through the tool function, simply include the \"callbacks\" option in the wrapped function. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aebf19-2815-4f05-8c6b-eb838ea268b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sklearn.datasets\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "def load_dataframe(dataframe_name: str) -> dict:\n",
    "    if dataframe_name == \"iris\":\n",
    "        df = pd.DataFrame(sklearn.datasets.load_iris()[\"data\"])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Dataframe with name '{dataframe_name}' not found. Please try again\"\n",
    "        )\n",
    "    return {\n",
    "        # We will expose the raw_output in our callback handler\n",
    "        \"raw_output\": df.to_json(),\n",
    "        \"dataframe\": df,\n",
    "    }\n",
    "\n",
    "\n",
    "load_dataframe_runnable = RunnableLambda(load_dataframe)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_dataframe(url: str, callbacks: Callbacks = None):\n",
    "    \"\"\"Gets the dataframe\"\"\"\n",
    "    # This will register an `on_chain_{start,end}` event where the `end` call contains the full dataframe\n",
    "    results = load_dataframe_runnable.invoke(dataframe_name, {\"callbacks\": callbacks})\n",
    "    # The agent will only see the head of the dataframe\n",
    "    return results[\"dataframe\"].head().to_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83877b9-34a7-46a7-8741-e844f788d8ef",
   "metadata": {},
   "source": [
    "### Option 2: Subclass BaseTool\n",
    "\n",
    "If you directly subclass `BaseTool`, you have more control over the class behavior and state management. You can choose to propagate callbacks by accepting a \"run_manager\" argument in your `_run` method.\n",
    "\n",
    "Below is an equivalent definition of our tool using inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872f468-89fa-4d68-a1fc-11fe74263189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Optional\n",
    "\n",
    "import aiohttp\n",
    "import requests\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class GetDataFrame(BaseTool):\n",
    "    name = \"get_dataframe\"\n",
    "    description = \"Get the dataframe head\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        dataframe_name: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        callbacks = run_manager.get_child() if run_manager else None\n",
    "        # This will register an `on_chain_{start,end}` event where the `end` call contains the full dataframe\n",
    "        results = load_dataframe_runnable.invoke(\n",
    "            dataframe_name, {\"callbacks\": callbacks}\n",
    "        )\n",
    "        # The agent will only see the head of the dataframe\n",
    "        return results[\"dataframe\"].head().to_markdown()\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        dataframe_name: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        callbacks = run_manager.get_child() if run_manager else None\n",
    "        # This will register an `on_chain_{start,end}` event where the `end` call contains the full dataframe\n",
    "        results = await load_dataframe_runnable.ainvoke(\n",
    "            dataframe_name, {\"callbacks\": callbacks}\n",
    "        )\n",
    "        # The agent will only see the head of the dataframe\n",
    "        return results[\"dataframe\"].head().to_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9e507-14c6-4bce-b8e1-2fdc7553d412",
   "metadata": {},
   "source": [
    "## 2. Define agent\n",
    "\n",
    "We will construct a simple agent using runnables and OpenAI functions, following the [Agents overview](https://python.langchain.com/docs/modules/agents/) in the LangChain documentation. The specifics aren't important to this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997babf2-41d4-4047-baa3-cac4f13fec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are very powerful assistant.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9398d7-d641-46e0-a81a-fa7fecd76a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "# Configure the LLM with access to the appropriate function definitions\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = [\n",
    "    GetDataFrame()  # Could alternatively use the decorator-based method: get_dataframe\n",
    "]\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6b794-e359-486e-ae79-f26d771095dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x.get(\"chat_history\") or [],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ").with_config(run_name=\"Agent\")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1030ea2-1b4d-45de-abb9-af6cbc2946d9",
   "metadata": {},
   "source": [
    "## 3. Create Callback Handler\n",
    "\n",
    "Now we will create a custom callback handler to surface the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee89906-17b3-460d-aef4-f8fd8649b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139c465-ae86-4de6-b2c6-8a7ff1b525e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from uuid import UUID\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class MyToolCallbackHandler(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self._run_map = {}\n",
    "\n",
    "    def on_chain_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None,\n",
    "        run_type: Optional[str] = None,\n",
    "        name: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if name == \"load_dataframe\":\n",
    "            self._run_map[run_id] = {\"inputs\": inputs, \"name\": name}\n",
    "\n",
    "    def on_chain_end(\n",
    "        self,\n",
    "        outputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        if run_id in self._run_map:\n",
    "            df = pd.read_json(outputs[\"raw_output\"])\n",
    "            # Display this in the jupyter notebook UX\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec4034-f866-4900-a008-7ca9a77584bc",
   "metadata": {},
   "source": [
    "## 4. Invoke\n",
    "\n",
    "Now its time to call the agent. All callbacks, including your custom callback handler, will be passed to the nested runnable, but only the filtered result will be shown to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fec7e-1773-4af4-aab4-ca83ff8fbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": f\"What's the first row of the iris dataset?\"},\n",
    "    {\"callbacks\": [MyToolCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23004db-c044-4b37-b9be-0ca126767ecc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, you created a tool in two ways: using the decorator and by subclassing `BaseTool`. You configured the callbacks so the nested call to the \"Summarize Text\" chain is traced correctly.\n",
    "\n",
    "LangSmith uses LangChain's callbacks system to trace the execution of your application. To trace nested components,  the callbacks have to be passed to that component.  Any time you see a trace show up on the top level when it ought to be nested, it's likely that somewhere the callbacks weren't correctly passed between components.\n",
    "\n",
    "This is all made easy when composing functions and other calls as runnables (i.e., [LangChain expression language](https://python.langchain.com/docs/expression_language/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c44e71-5650-4f61-9e94-cda5c0d649d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
