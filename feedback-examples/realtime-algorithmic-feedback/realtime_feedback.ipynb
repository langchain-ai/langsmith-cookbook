{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bddb3342-ae12-4d6b-98d1-21cb0674c1d0",
      "metadata": {},
      "source": [
        "# Real-time Automated Feedback\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/feedback-examples/realtime-algorithmic-feedback/realtime_feedback.ipynb)\n",
        "\n",
        "This tutorial shows how to attach a reference-free evaluator as a callback to your chain to automatically generate feedback for each trace. LangSmith can use this information to help you monitor the quality of your deployment.\n",
        "\n",
        "![model-based feedback monitoring charts](./img/feedback_charts.png)\n",
        "\n",
        "If the metrics reveal issues, you can isolate problematic runs for debugging or fine-tuning.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. **Define Feedback Logic**: Create a RunEvaluator to calculate the feedback metrics. We will use LangChain's \"CriteriaEvaluator\" as an example.\n",
        "  \n",
        "2. **Include in callbacks**: Using the [EvaluatorCallbackHandler](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.evaluation.EvaluatorCallbackHandler.html#langchain.callbacks.tracers.evaluation.EvaluatorCallbackHandler), we can make sure the evaluators are applied any time a trace is completed.\n",
        "  \n",
        "We'll be using LangSmith, so make sure you have the necessary API keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba38d55-6551-49c2-a3cb-72a56e3c3953",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U langchain openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f496bc2-d9c6-46ea-be19-98ff2dd2800c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"YOUR PROJECT NAME\"  # Change to your project name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec884ee8-c4af-4bc5-b9e7-e87fa4c4f97f",
      "metadata": {},
      "source": [
        "Once you've decided the runs you want to evaluate, it's time to define the feedback pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 1. Define feedback logic\n",
        "\n",
        "All feedback needs a key and should have a (nullable) numeric score. You can apply any algorithm to generate these\n",
        "scores, but you'll want to choose the one that makes the most sense for your use case.\n",
        "\n",
        "The following example selects the \"input\" and \"output\" keys of the trace and returns 1 if the algorithm believes the response to be \"helpful\", 0 otherwise.\n",
        "\n",
        "LangChain has a number of reference-free evaluators you can use off-the-shelf or configure to your needs. You can apply these directly to your runs to log the evaluation results as feedback. For more information on available LangChain evaluators, check out the [open source documentation](https://python.langchain.com/docs/guides/productionization/evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from langchain.evaluation import load_evaluator\n",
        "from langsmith.evaluation import RunEvaluator, EvaluationResult\n",
        "from langsmith.schemas import Run, Example\n",
        "\n",
        "\n",
        "class HelpfulnessEvaluator(RunEvaluator):\n",
        "    def __init__(self):\n",
        "        self.evaluator = load_evaluator(\n",
        "            \"score_string\", criteria=\"helpfulness\", normalize_by=10\n",
        "        )\n",
        "\n",
        "    def evaluate_run(\n",
        "        self, run: Run, example: Optional[Example] = None\n",
        "    ) -> EvaluationResult:\n",
        "        if (\n",
        "            not run.inputs\n",
        "            or not run.inputs.get(\"input\")\n",
        "            or not run.outputs\n",
        "            or not run.outputs.get(\"output\")\n",
        "        ):\n",
        "            return EvaluationResult(key=\"helpfulness\", score=None)\n",
        "        result = self.evaluator.evaluate_strings(\n",
        "            input=run.inputs[\"input\"], prediction=run.outputs[\"output\"]\n",
        "        )\n",
        "        return EvaluationResult(\n",
        "            **{\"key\": \"helpfulness\", \"comment\": result.get(\"reasoning\"), **result}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd061c7-039c-4799-8eeb-2acffc082545",
      "metadata": {},
      "source": [
        "Here, we are using the `collect_runs` callback handler to easily fetch the run ID from the evaluation run. By adding it to the evaluator_info, the feedback will retain a link from the evaluated run to the source run so you can see why the tag was generated. Below, we will log feedback to all the traces in our project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c73207-5ba2-40e9-85c3-c9b0401b5702",
      "metadata": {},
      "source": [
        "## 2. Include in callbacks\n",
        "\n",
        "We can use the [EvaluatorCallbackHandler](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.evaluation.EvaluatorCallbackHandler.html#langchain.callbacks.tracers.evaluation.EvaluatorCallbackHandler) to automatically call the evaluator in a separate thread any time a trace is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "35a465ce-7e59-411b-b53a-f60d23bfaa94",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "chain = (\n",
        "    ChatPromptTemplate.from_messages([(\"user\", \"{input}\")])\n",
        "    | ChatOpenAI()\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "907cd516-b175-46fe-b056-f8967be37930",
      "metadata": {},
      "source": [
        "Then create the callback. This callback runs the evaluators in a separate thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "801fc59a-0575-49c9-be5a-81ce6ae0b39c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.callbacks.tracers.evaluation import EvaluatorCallbackHandler\n",
        "\n",
        "evaluator = HelpfulnessEvaluator()\n",
        "\n",
        "feedback_callback = EvaluatorCallbackHandler(evaluators=[evaluator])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "37390889-b2f6-43db-a8c6-1d600b3ef24c",
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"Where is Antioch?\",\n",
        "    \"What was the US's inflation rate in 2018?\",\n",
        "    \"Who were the stars in the show Friends?\",\n",
        "    \"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\",\n",
        "    \"Why is the sky blue?\",\n",
        "    \"When is Rosh hashanah in 2023?\",\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    chain.invoke({\"input\": query}, {\"callbacks\": [feedback_callback]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e66e144-141a-4d23-b7c3-3627d8ae6265",
      "metadata": {},
      "source": [
        "Check out the target project to see the feedback appear as the runs are evaluated.\n",
        "\n",
        "![feedback_result](./img/feedback_result.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congrats! You've configured an evaluator to run any time your chain is called. This will generate relevant metrics you can use to track your deployment."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
