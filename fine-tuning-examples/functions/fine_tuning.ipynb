{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e228915-ec4b-4877-aecd-2497e2623cc0",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Function-calling chain\n",
    "\n",
    "Steps:\n",
    "- Define agent and tools that you want to learn from\n",
    "- Define the inputs you'd like to feed into the agent\n",
    "- Capture traces\n",
    "- Convert to fine-tuning format\n",
    "- Fine-tune\n",
    "- Use in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223d3011-9345-464f-8daf-ed8f16825bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07b8ac6-cf13-4296-b34a-e626ed019437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import chains, agents, chat_models, prompts, tools\n",
    "# Local chat loader for some Kapa questions\n",
    "from discord_loader import DiscordChatLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207d99da-dfc5-4f98-9efd-454e3876f421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "import uuid\n",
    "from typing import Optional\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    model: str = \"openai\"\n",
    "    history: Optional[list] = None\n",
    "    conversation_id: Optional[str] = None\n",
    "    \n",
    "    \n",
    "class ChatLangchainRequest(BaseModel):\n",
    "    message: str = Field(..., description=\"\"\"The question to ask the documentation agent.\"\"\")\n",
    "    conversation_uuid: Optional[uuid.UUID] = Field(default=None, description=\"\"\"The UUID for the conversation. If none, a UUID will be generated.\"\"\")\n",
    "    \n",
    "chat_histories = {}\n",
    "    \n",
    "@tools.tool(args_schema=ChatLangchainRequest)\n",
    "def chat_langchain(message: str,  conversation_uuid: Optional[uuid.UUID] = None) -> dict:\n",
    "    \"\"\"Query the langchain documentation to answer questions about LangChain, the\n",
    "    LLM application framework.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The question to ask the documentation agent.\n",
    "        conversation_uuid (Optional[uuid]): The optional UUID for the conversation. If none, a UUID will be generated for you.\n",
    "    \n",
    "    \"\"\"\n",
    "    global chat_histories\n",
    "    conversation_id = conversation_uuid\n",
    "    if conversation_id is not None:\n",
    "        try:\n",
    "            conversation_id = uuid.UUID(conversation_id)\n",
    "        except:\n",
    "            conversation_id = None\n",
    "    conversation_id = conversation_id or uuid.uuid4()\n",
    "    history = chat_histories.get(conversation_id) or []\n",
    "    history.append({\"question\": message})\n",
    "    request = ChatRequest(message=message, model=\"openai\", history=history, conversation_id=str(conversation_id))\n",
    "    response = requests.post(\"http://localhost:8080/chat\", json=request.dict())\n",
    "    chat_histories[conversation_id] = history + [{\"result\": response.text}]\n",
    "    return {\n",
    "        \"conversation_uuid\": conversation_id,\n",
    "        \"response\": response.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66ed72d-4b61-4d1f-99b4-950eebc7d367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_tools = [chat_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95055c84-2b29-4ec0-b4b1-3e0b92559527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = agents.initialize_agent(\n",
    "    tools=selected_tools,\n",
    "    llm=chat_models.ChatOpenAI(model=\"gpt-4\"),\n",
    "    agent=agents.AgentType.OPENAI_FUNCTIONS,\n",
    ")\n",
    "\n",
    "# We will reframe the question to better direct the agent.\n",
    "ask_q = lambda x: f\"A discord user asked this question:\\n \\\"\\\"\\\"\\n{x}\\n\\\"\\\"\\\" What's the verified answer?? Do not stop until you have the correct answer.\"\n",
    "chain = ask_q | agent_executor | (lambda x: x['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b008bd7-ed45-4250-9648-84f83c96a6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = DiscordChatLoader(\"discord.txt\")\n",
    "sessions = docs.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da72d67-82df-4dd2-83b4-a1fb27431fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import callbacks\n",
    "\n",
    "trace_project_name = \"chat-langchain-user-agent\"\n",
    "\n",
    "# We will be processing each of these questions with some level of parallelism\n",
    "# questions = [m.content for m in sessions[0]['messages']]\n",
    "# with callbacks.tracing_v2_enabled(project_name=trace_project_name):\n",
    "#     results = chain.batch(questions, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129d6f22-83f8-437b-bc6a-29bd8347b8c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "import json\n",
    "import subprocess\n",
    "import functools\n",
    "from typing import List, Dict\n",
    "\n",
    "def convert_to_typescript(functions: dict) -> str:\n",
    "    # Depends on !npm install -g json-schema-to-typescript\n",
    "    return subprocess.run(\n",
    "        \"json2ts --bannerComment=''\",\n",
    "        input=json.dumps(functions).encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        check=True,\n",
    "        shell=True\n",
    "    ).stdout.decode(\"utf-8\")\n",
    "    \n",
    "\n",
    "def convert_from_oai(f: dict) -> str:\n",
    "    schema = copy.deepcopy(f)\n",
    "    parameters = schema.pop('parameters')\n",
    "    parameters['title'] = schema['name']\n",
    "    parameters['description'] = schema['description']\n",
    "    return parameters\n",
    "\n",
    "_cache = {}\n",
    "def convert_functions_to_typescript(functions: List[dict]) -> str:\n",
    "    key = tuple([str(f) for f in functions])\n",
    "    if key in _cache:\n",
    "        return _cache[key]\n",
    "    result = \"\\n\\n\".join([convert_to_typescript(convert_from_oai(f)) for f in functions])\n",
    "    _cache[key] = result\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77056629-5999-4f10-a246-2cdc211f85c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Optional, List, Type\n",
    "from langchain.load import load\n",
    "from langchain import schema\n",
    "import langsmith\n",
    "\n",
    "def get_ultimate_llm_runs(client: langsmith.Client, project_name: str, tags: Optional[List[str]] = None, run_name: Optional[str] = None) -> List[langsmith.schemas.Run]:\n",
    "    runs = client.list_runs(project_name=project_name, run_type=\"llm\", tags=tags, run_name=run_name)\n",
    "    by_parent = collections.defaultdict(list)\n",
    "    for run in runs:\n",
    "        by_parent[run.parent_run_id].append(run)\n",
    "\n",
    "    all_parents = list(by_parent.keys())\n",
    "    last_llm = []\n",
    "    for parent in all_parents:\n",
    "        last_llm.append(sorted(by_parent[parent], key=lambda x: x.start_time, reverse=True)[0])\n",
    "    return last_llm\n",
    "\n",
    "def extract_fine_tuning_message(run: langsmith.schemas.Run) -> List[schema.BaseMessage]:\n",
    "    # TODO: Incorporate the output. Probably not necessary though.\n",
    "    messages = load.load(run.inputs['messages'])\n",
    "    functions = run.extra['invocation_params']['functions']\n",
    "    functions_ts = convert_functions_to_typescript(functions)\n",
    "    # Assumes the first message is a system message\n",
    "    messages[0] = schema.SystemMessage(content=functions_ts)\n",
    "    return messages\n",
    "\n",
    "def convert_function_messages(messages: List[schema.BaseMessage], to_type: Type[schema.BaseMessage] = schema.HumanMessage) -> List[schema.BaseMessage]:\n",
    "    results = []\n",
    "    for m in messages:\n",
    "        if m.type == \"function\":\n",
    "            results.append(to_type(content=m.content))\n",
    "        else:\n",
    "            results.append(m)\n",
    "    return results\n",
    "\n",
    "def move_function_call(messages: List[schema.BaseMessage]) -> List[schema.BaseMessage]:\n",
    "    results = []\n",
    "    for m in messages:\n",
    "        if m.type == \"ai\" and m.additional_kwargs:\n",
    "            fc = m.additional_kwargs['function_call']\n",
    "            results.append(schema.AIMessage(content=m.content or \"\" + json.dumps(fc)))\n",
    "        else:\n",
    "            results.append(m)\n",
    "    return results\n",
    "\n",
    "# TODO: Filter out non-error but unhelpful messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a91976c-14d2-40bd-9e7a-417a1002e249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = langsmith.Client()\n",
    "project_name = \"chat-langchain-user-agent\"\n",
    "runs = get_ultimate_llm_runs(client, project_name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cdacec3-d7f5-4fbc-b963-c4eb49675e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converted_messages = [\n",
    "    {\"messages\": convert_function_messages(move_function_call(extract_fine_tuning_message(run)), to_type=schema.HumanMessage)}\n",
    "    for run in runs\n",
    "    if 'functions' in run.extra.get('invocation_params', {})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36549182-f386-42c5-bb6a-ac40711a3ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.adapters import openai as openai_adapter\n",
    "\n",
    "training_data = openai_adapter.convert_messages_for_finetuning(converted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc60c62-d385-43df-9d44-4b7cd5ff5497",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tune\n",
    "\n",
    "#### Upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed5f784b-5628-4e6f-9fc3-b5b9049f9d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from io import BytesIO\n",
    "# import time\n",
    "\n",
    "# import openai\n",
    "\n",
    "# # We will write the jsonl file in memory\n",
    "# my_file = BytesIO()\n",
    "# for m in training_data:\n",
    "#     my_file.write((json.dumps({\"messages\": m}) + \"\\n\").encode('utf-8'))\n",
    "\n",
    "# my_file.seek(0)\n",
    "# training_file = openai.File.create(\n",
    "#   file=my_file,\n",
    "#   purpose='fine-tune'\n",
    "# )\n",
    "\n",
    "# # OpenAI audits each training file for compliance reasons.\n",
    "# # This make take a few minutes\n",
    "# status = openai.File.retrieve(training_file.id).status\n",
    "# start_time = time.time()\n",
    "# while status != \"processed\":\n",
    "#     print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n",
    "#     time.sleep(5)\n",
    "#     status = openai.File.retrieve(training_file.id).status\n",
    "# print(f\"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0893b86-457b-484f-8235-836a5ac59a27",
   "metadata": {},
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66effe0d-be83-4d68-9ccd-1f69c7616e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdf62b1-f40b-4178-8bbe-1fe1beac4ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# job = openai.FineTuningJob.create(\n",
    "#     training_file=training_file.id,\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd670a9e-be30-4ed7-8172-a2234f7575a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status=[running]... 1105.64s\r"
     ]
    }
   ],
   "source": [
    "# status = openai.FineTuningJob.retrieve(job.id).status\n",
    "# start_time = time.time()\n",
    "# while status != \"succeeded\":\n",
    "#     print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n",
    "#     time.sleep(5)\n",
    "#     job = openai.FineTuningJob.retrieve(job.id)\n",
    "#     status = job.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1fe12a9-b7c4-411b-8ef7-12e6d63c4caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(job.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139ca15-e0f5-4a52-bf11-8d3f343f0645",
   "metadata": {},
   "source": [
    "## Use in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec3b3ec6-3d51-4691-b833-7207cab2222a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"ft:gpt-3.5-turbo-0613:personal::7sgF8sHD\", # job.fine_tuned_model,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56b0fec5-e58e-4ca4-b5b1-a69923852af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_tool_to_ts(my_tool):\n",
    "    schema = {\"name\": my_tool.name, \"description\": my_tool.description, \"parameters\": chat_langchain.args_schema.schema()}\n",
    "    return convert_functions_to_typescript([schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffc4a07c-03cf-4246-9feb-f45eac02096e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import openai_functions\n",
    "\n",
    "prompt = prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"{function_ts_def}\"),\n",
    "        (\"human\", \"A discord user asked this question:\\n\\n\\\"\\\"\\\"\\n{input}\\n\\\"\\\"\\\" What's the verified answer?\"),\n",
    "        prompts.MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tool_schema = convert_tool_to_ts(chat_langchain)\n",
    "\n",
    "def split_up(message: schema.AIMessage) -> schema.AIMessage:\n",
    "    if message.content.startswith('{'):\n",
    "        try:\n",
    "            return schema.AIMessage(content='', additional_kwargs={\"function_call\": json.loads(message.content)})\n",
    "        except:\n",
    "            pass\n",
    "    return message\n",
    "\n",
    "runnable_chain = (\n",
    "    prompt.partial(function_ts_def=tool_schema) \n",
    "    | model \n",
    "    | split_up \n",
    "    | openai_functions.JsonOutputFunctionsParser(args_only=False).with_fallbacks([schema.output_parser.StrOutputParser()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c305081-af38-4868-b9a6-c59a49f0b097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import AgentExecutor, BaseSingleActionAgent\n",
    "\n",
    "\n",
    "class MyOAIAgent(BaseSingleActionAgent):\n",
    "    \"\"\"Fake Custom Agent.\"\"\"\n",
    "    \n",
    "    runnable_chain: schema.runnable.Runnable\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        \n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]\n",
    "    \n",
    "\n",
    "    def plan(\n",
    "        self,\n",
    "        intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "        callbacks = None,\n",
    "        **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        # print(intermediate_steps)\n",
    "        chat_history = [message for step in intermediate_steps for message in [schema.AIMessage(content=json.dumps(step[0].log)), schema.HumanMessage(content=str(step[1]))]]\n",
    "        res = self.runnable_chain.invoke({\"input\": kwargs[\"input\"], \"chat_history\": chat_history}, {\"callbacks\": callbacks})\n",
    "        if isinstance(res, dict) and \"name\" in res:\n",
    "            return AgentAction(tool=res[\"name\"], tool_input=res[\"arguments\"], log=res)\n",
    "        return AgentFinish(return_values={\"output\": res}, log=res)\n",
    "\n",
    "    async def aplan(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "77ca00d5-a5f3-4b1a-b089-250bec084e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = MyOAIAgent(runnable_chain=runnable_chain)\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=[chat_langchain], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bec4c4ce-e522-455c-8e36-7686adcfaefc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{'name': 'chat_langchain', 'arguments': {'message': 'How do I use the ConversationRetrieverChain?'}}\u001b[0m\u001b[36;1m\u001b[1;3m{'conversation_uuid': UUID('a46bcc3c-7047-4ffa-872a-3ef1e97e3c08'), 'response': 'To use the ConversationalRetrievalChain in Langchain, you need to follow these steps:\\n\\n1. Import the necessary modules:\\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\nfrom langchain.document_loaders import TextLoader\\n```\\n\\n2. Load in your documents. You can replace this with a loader for whatever type of data you want:\\n```python\\nloader = TextLoader(...)\\n```\\n\\n3. Create a vector store from embeddings:\\n```python\\nvectorstore = Chroma(...)\\n```\\n\\n4. Create a retriever from the vector store:\\n```python\\nretriever = vectorstore.as_retriever()\\n```\\n\\n5. Create an instance of the ConversationalRetrievalChain:\\n```python\\nllm = OpenAI(...)\\nchat = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\\n```\\n\\n6. Use the chain to answer a question:\\n```python\\nresult = chat({\"question\": \"Your question here\"})\\nprint(result[\\'answer\\'])\\n```\\n\\nRemember to replace the placeholders with your actual data or parameters. The ConversationalRetrievalChain takes in chat history and new questions, and then returns an answer to that question. It uses the chat history and the new question to create a \"standalone question\", retrieves relevant documents based on this question, and then passes these documents and the question to a question answering chain to return a response.'}\u001b[0m\u001b[32;1m\u001b[1;3mTo use the ConversationalRetrievalChain in Langchain, you need to follow these steps:\n",
      "\n",
      "1. Import the necessary modules:\n",
      "```python\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import ConversationalRetrievalChain\n",
      "from langchain.document_loaders import TextLoader\n",
      "```\n",
      "\n",
      "2. Load in your documents. You can replace this with a loader for whatever type of data you want:\n",
      "```python\n",
      "loader = TextLoader(...)\n",
      "```\n",
      "\n",
      "3. Create a vector store from embeddings:\n",
      "```python\n",
      "vectorstore = Chroma(...)\n",
      "```\n",
      "\n",
      "4. Create a retriever from the vector store:\n",
      "```python\n",
      "retriever = vectorstore.as_retriever()\n",
      "```\n",
      "\n",
      "5. Create an instance of the ConversationalRetrievalChain:\n",
      "```python\n",
      "llm = OpenAI(...)\n",
      "chat = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
      "```\n",
      "\n",
      "6. Use the chain to answer a question:\n",
      "```python\n",
      "result = chat({\"question\": \"Your question here\"})\n",
      "print(result['answer'])\n",
      "```\n",
      "\n",
      "Remember to replace the placeholders with your actual data or parameters. The ConversationalRetrievalChain takes in chat history and new questions, and then returns an answer to that question. It uses the chat history and the new question to create a \"standalone question\", retrieves relevant documents based on this question, and then passes these documents and the question to a question answering chain to return a response.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How do I use the ConversationRetrieverChain?',\n",
       " 'output': 'To use the ConversationalRetrievalChain in Langchain, you need to follow these steps:\\n\\n1. Import the necessary modules:\\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\nfrom langchain.document_loaders import TextLoader\\n```\\n\\n2. Load in your documents. You can replace this with a loader for whatever type of data you want:\\n```python\\nloader = TextLoader(...)\\n```\\n\\n3. Create a vector store from embeddings:\\n```python\\nvectorstore = Chroma(...)\\n```\\n\\n4. Create a retriever from the vector store:\\n```python\\nretriever = vectorstore.as_retriever()\\n```\\n\\n5. Create an instance of the ConversationalRetrievalChain:\\n```python\\nllm = OpenAI(...)\\nchat = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\\n```\\n\\n6. Use the chain to answer a question:\\n```python\\nresult = chat({\"question\": \"Your question here\"})\\nprint(result[\\'answer\\'])\\n```\\n\\nRemember to replace the placeholders with your actual data or parameters. The ConversationalRetrievalChain takes in chat history and new questions, and then returns an answer to that question. It uses the chat history and the new question to create a \"standalone question\", retrieves relevant documents based on this question, and then passes these documents and the question to a question answering chain to return a response.'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor(\"How do I use the ConversationRetrieverChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bcfb8-d07f-4486-ad87-6ddf135eb77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
